{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pathlib\n",
    "import dspy\n",
    "from dspy import Example\n",
    "from dspy.evaluate.evaluate import Evaluate\n",
    "from dspy.evaluate.metrics import answer_exact_match\n",
    "from functools import partial\n",
    "from dspy import Signature, InputField, OutputField\n",
    "from pydantic import BaseModel, Field\n",
    "from datetime import datetime\n",
    "from dspy.teleprompt import MIPROv2\n",
    "from dspy.teleprompt.random_search import BootstrapFewShotWithRandomSearch\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "class Problem(BaseModel):\n",
    "    problem_dir: pathlib.Path = Field(\n",
    "        ..., description=\"The path to the problem directory\"\n",
    "    )\n",
    "    problem_name: str = Field(..., description=\"The name of the problem\")\n",
    "    problem_description: str = Field(..., description=\"The description of the problem\")\n",
    "    sample_input: str = Field(..., description=\"The sample input of the problem\")\n",
    "    sample_output: str = Field(..., description=\"The sample output of the problem\")\n",
    "    problem_input: pathlib.Path = Field(..., description=\"The path to the input file\")\n",
    "    problem_output: pathlib.Path = Field(..., description=\"The path to the output file\")\n",
    "    solution: str = Field(..., description=\"The solution to the problem\")\n",
    "\n",
    "    @property\n",
    "    def as_xml(self) -> str:\n",
    "        return f\"\"\"\n",
    "<problem>\n",
    "<problem_statement>\n",
    "{self.problem_description}\n",
    "</problem_statement>\n",
    "<sample_test_cases>\n",
    "<sample_input>\n",
    "{self.sample_input}\n",
    "</sample_input>\n",
    "<sample_output>\n",
    "{self.sample_output}\n",
    "</sample_output>\n",
    "</sample_test_cases>\n",
    "</problem>\n",
    "\"\"\"\n",
    "\n",
    "\n",
    "def load_problem(problem_name: str, problem_dir: pathlib.Path) -> Problem:\n",
    "    problem_input = problem_dir / f\"{problem_name}.in\"\n",
    "    problem_output = problem_dir / f\"{problem_name}.out\"\n",
    "    sample_input = problem_dir / f\"{problem_name}_sample_input.txt\"\n",
    "    sample_output = problem_dir / f\"{problem_name}_sample_output.txt\"\n",
    "    problem_description = problem_dir / f\"{problem_name}.md\"\n",
    "    solution = problem_dir / f\"{problem_name}_sol.md\"\n",
    "    return Problem(\n",
    "        problem_dir=problem_dir,\n",
    "        problem_name=problem_name,\n",
    "        problem_description=problem_description.read_text(),\n",
    "        sample_input=sample_input.read_text(),\n",
    "        sample_output=sample_output.read_text(),\n",
    "        problem_input=problem_input,\n",
    "        problem_output=problem_output,\n",
    "        solution=solution.read_text(),\n",
    "    )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "problems = list(pathlib.Path(\"data/dataset/\").rglob(\"*.in\"))\n",
    "eval_problems = list(filter(lambda x: \"2022/final\" in str(x), problems))\n",
    "test_problems = list(filter(lambda x: \"2023/practice\" in str(x), problems))\n",
    "train_problems = list(filter(lambda x: x not in eval_problems + test_problems, problems))\n",
    "\n",
    "\n",
    "def load_problem_from_path(problem_path: pathlib.Path):\n",
    "    problem_name = problem_path.stem\n",
    "    problem_dir = problem_path.parent\n",
    "    problem = load_problem(problem_name, problem_dir)\n",
    "    return problem\n",
    "\n",
    "\n",
    "\n",
    "    \n",
    "train_problems = list(map(load_problem_from_path, train_problems))\n",
    "eval_problems = list(map(load_problem_from_path, eval_problems))\n",
    "test_problems = list(map(load_problem_from_path, test_problems))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "(len(train_problems), len(eval_problems), len(test_problems))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "trainset = [Example(\n",
    "    problem_statement=problem.problem_description,\n",
    "    sample_input=problem.sample_input,\n",
    "    sample_output=problem.sample_output,\n",
    "    solution=problem.solution).with_inputs(\"problem_statement\", \"sample_input\", \"sample_output\") for problem in train_problems]\n",
    "\n",
    "devset = [Example(\n",
    "    problem_statement=problem.problem_description,\n",
    "    sample_input=problem.sample_input,\n",
    "    sample_output=problem.sample_output,\n",
    "    solution=problem.solution).with_inputs(\"problem_statement\", \"sample_input\", \"sample_output\") for problem in eval_problems]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "lm = dspy.OpenAI(\n",
    "        model=\"gpt-4o-mini\", # Note: didn't find much a difference btwn mini & full gpt-4o\n",
    "        max_tokens=4000,\n",
    "        temperature=0.1,\n",
    "    )\n",
    "\n",
    "dspy.settings.configure(lm=lm)\n",
    "dspy.configure(experimental=True)\n",
    "\n",
    "\n",
    "def validate_solution(example, prediction, trace=None,frac=0.8):\n",
    "    \n",
    "    result = answer_exact_match(\n",
    "        example=Example(answer=example.solution),\n",
    "        pred=Example(answer=prediction.solution),\n",
    "        trace=trace, \n",
    "        frac=frac)\n",
    "    return result\n",
    "    \n",
    "\n",
    "# Setup evaluation function\n",
    "evaluate = Evaluate(\n",
    "    devset=devset,\n",
    "    num_threads=6, # Note: Set this to 1 for debugging purposes \n",
    "    display_progress=True,\n",
    "    display_table=5,\n",
    "    metric=validate_solution\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "class GenerateSolution(Signature):\n",
    "    \"\"\"You are an expert problem solver. Your task is to solve the problem at hand.\n",
    "\n",
    "    When solving a competitive programming problem, start by thoroughly reading and understanding the problem statement, including all constraints and input/output formats.\n",
    "    Identify the key elements, analyze sample inputs/outputs, and consider edge cases.\n",
    "    Look for patterns or mathematical relationships, then develop a logical approach, breaking the problem into subproblems if needed. \n",
    "    Explain the core insight in simple terms, followed by a step-by-step explanation of the solution.\n",
    "    Use clear language and visual aids if helpful. Discuss optimization techniques, alternative approaches, and special cases. \n",
    "    Address time and space complexity, and provide pseudocode if appropriate. \n",
    "    Finally, proofread the solution for clarity and completeness, ensuring it's accessible to readers with varying levels of programming experience.\n",
    "    Note: You are not expected to write the code for the solution, just provide a solution explanation so that an experienced developer can write the code for the solution.\n",
    "    \"\"\"\n",
    "\n",
    "    problem_statement: str = InputField(format=str)\n",
    "    sample_input: str = InputField(format=str, desc=\"The sample input provided with the problem statement.\")\n",
    "    sample_output: str = InputField(format=str, desc=\"The sample output provided with the problem statement.\")\n",
    "    solution: str = OutputField(\n",
    "        format=str, desc=\"a solution explanation for how we should go about solving this problem.\"\n",
    "    )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dspy.ChainOfThought(GenerateSolution)(\n",
    "    problem_statement=trainset[0].problem_statement, \n",
    "    sample_input=trainset[0].sample_input, \n",
    "    sample_output=trainset[0].sample_output\n",
    "    ).solution\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class SimpleGenerateSolution(dspy.Module):\n",
    "    def __init__(self):\n",
    "        super().__init__()\n",
    "        self.generate_code = dspy.ChainOfThought(GenerateSolution)\n",
    "\n",
    "    def forward(self, problem_statement, sample_input, sample_output):\n",
    "        solution = self.generate_code(\n",
    "                problem_statement=problem_statement,\n",
    "                sample_input=sample_input,\n",
    "                sample_output=sample_output\n",
    "            ).solution\n",
    "\n",
    "        return dspy.Prediction(solution=solution)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# prediction = SimpleGenerateSolution()(problem_statement=trainset[0].problem_statement, \n",
    "#     sample_input=trainset[0].sample_input, \n",
    "#     sample_output=trainset[0].sample_output)\n",
    "\n",
    "# print(prediction.solution)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "simple_program = SimpleGenerateSolution()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "evaluate(program=simple_program, devset=devset)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "def optimize_with_mipro(program, prompt_model, task_model, metric, trainset):\n",
    "    teleprompter = MIPROv2(\n",
    "        prompt_model=prompt_model,\n",
    "        task_model=task_model,\n",
    "        metric=metric,\n",
    "        num_candidates=5,\n",
    "        init_temperature=0.5,\n",
    "        verbose=False,\n",
    "        log_dir=\"./logs\",\n",
    "    )\n",
    "\n",
    "    optimized_program = teleprompter.compile(\n",
    "        program.deepcopy(),\n",
    "        trainset=trainset,\n",
    "        eval_kwargs=dict(num_threads=16),\n",
    "        max_bootstrapped_demos=0, # 0-shot optimization\n",
    "        max_labeled_demos=0,\n",
    "        num_batches=20,\n",
    "        minibatch=False, # turning this off bc we have a small trainset already\n",
    "        seed=9\n",
    "    )\n",
    "    now = datetime.now()\n",
    "    date_time = now.strftime(\"%Y%m%d_%H%M%S\")\n",
    "\n",
    "    optimized_program.save(f\"mipro_optimized_{date_time}\")\n",
    "\n",
    "    return optimized_program\n",
    "\n",
    "\n",
    "def optimize_with_bootstrap_fewshot(program, task_model, teacher_model, metric, trainset):\n",
    "    rs_optimizer = BootstrapFewShotWithRandomSearch(\n",
    "        metric=metric,\n",
    "        num_threads=8,\n",
    "        num_candidate_programs=5,\n",
    "        max_labeled_demos=0,\n",
    "        max_bootstrapped_demos=2,\n",
    "        max_errors=10000,\n",
    "        teacher_settings=dict(lm=teacher_model)\n",
    "    )\n",
    "    \n",
    "    optimized_program = rs_optimizer.compile(\n",
    "        program,\n",
    "        trainset=trainset,\n",
    "    )\n",
    "\n",
    "    now = datetime.now()\n",
    "    date_time = now.strftime(\"%Y%m%d_%H%M%S\")\n",
    "\n",
    "    optimized_program.save(f\"fewshot_optimized_{date_time}\")\n",
    "\n",
    "\n",
    "    return optimized_program"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "optimized_program = optimize_with_bootstrap_fewshot(simple_program, lm, lm, validate_solution, trainset)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "evaluate(program=optimized_program, devset=devset)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "mipro_optimized_program = optimize_with_mipro(simple_program, lm, lm, validate_solution, trainset)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "evaluate(program=mipro_optimized_program, devset=devset)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
