{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a href=\"https://colab.research.google.com/github/tcapelle/hackercup_rag/blob/main/demo.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>\n",
    "<!--- @wandbcode{rag-hackercup} -->"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<img src=\"http://wandb.me/logo-im-png\" width=\"400\" alt=\"Weights & Biases\" />\n",
    "\n",
    "\n",
    "# Introduction\n",
    "\n",
    "\n",
    "</a>\n",
    "\n",
    "\n",
    "In this notebook, we will build a few Code Generation agents for the [HackerCup AI](https://hackercupai.github.io/) challenge.\n",
    "\n",
    "We will build three different agents using different techniques and evaluate them using [W&B Weave](https://weave-docs.wandb.ai/).\n",
    "\n",
    "\n",
    "<img src=\"https://raw.githubusercontent.com/wandb/weave/master/docs/static/img/evals-hero.png\" width=\"800\" height=\"450\">\n",
    "\n",
    "A more detailed walkthough of the approach we will use in this notebook can be found in the following Youtube video:\n",
    "Hint: Click on the image to watch the video üòé\n",
    "\n",
    "<a target=\"_blank\" href=\"https://www.youtube.com/watch?v=cObBj2UpWK8\">\n",
    "<img src=\"https://img.youtube.com/vi/cObBj2UpWK8/0.jpg\" width=\"600\" height=\"450\">\n",
    "</a>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Weave\n",
    "\n",
    "\n",
    "Weave is a lightweight toolkit for tracking and evaluating LLM applications, built by Weights & Biases. We will use the following weave to trace and evaluate the various agents we build.\n",
    "\n",
    "We will use Weave to keep track and evaluate the different agents we build.\n",
    "\n",
    "Our goal is to bring rigor, best-practices, and composability to the inherently experimental process of developing AI applications, without introducing cognitive overhead.\n",
    "\n",
    "If you want to learn more about Weave, you can [get started](https://weave-docs.wandb.ai/quickstart) by decorating Python functions with `@weave.op`."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Setup "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Note: You need to run this cell only once**\n",
    "We will clone the starter-kits repo\n",
    "Set the rag folder as our working directory\n",
    "and install the dependencies for the project.\n",
    "\n",
    "**You can comment out the cell after you have run it once.**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "fatal: destination path 'hackercup_rag' already exists and is not an empty directory.\n",
      "/Users/tcapelle/work/hackercup_rag/hackercup_rag\n",
      "Requirement already satisfied: bm25s==0.1.10 in /Users/tcapelle/miniforge3/envs/weave/lib/python3.11/site-packages (from -r requirements.txt (line 1)) (0.1.10)\n",
      "Requirement already satisfied: datasets==2.21.0 in /Users/tcapelle/miniforge3/envs/weave/lib/python3.11/site-packages (from -r requirements.txt (line 2)) (2.21.0)\n",
      "Requirement already satisfied: instructor==1.4.0 in /Users/tcapelle/miniforge3/envs/weave/lib/python3.11/site-packages (from -r requirements.txt (line 3)) (1.4.0)\n",
      "Requirement already satisfied: joblib==1.4.2 in /Users/tcapelle/miniforge3/envs/weave/lib/python3.11/site-packages (from -r requirements.txt (line 4)) (1.4.2)\n",
      "Requirement already satisfied: openai==1.43.0 in /Users/tcapelle/miniforge3/envs/weave/lib/python3.11/site-packages (from -r requirements.txt (line 5)) (1.43.0)\n",
      "Requirement already satisfied: pandas==2.2.2 in /Users/tcapelle/miniforge3/envs/weave/lib/python3.11/site-packages (from -r requirements.txt (line 6)) (2.2.2)\n",
      "Requirement already satisfied: pydantic==2.8.2 in /Users/tcapelle/miniforge3/envs/weave/lib/python3.11/site-packages (from -r requirements.txt (line 7)) (2.8.2)\n",
      "Requirement already satisfied: scikit_learn==1.5.1 in /Users/tcapelle/miniforge3/envs/weave/lib/python3.11/site-packages (from -r requirements.txt (line 8)) (1.5.1)\n",
      "Requirement already satisfied: simple_parsing==0.1.5 in /Users/tcapelle/miniforge3/envs/weave/lib/python3.11/site-packages (from -r requirements.txt (line 9)) (0.1.5)\n",
      "Requirement already satisfied: tree_sitter_languages==1.10.2 in /Users/tcapelle/miniforge3/envs/weave/lib/python3.11/site-packages (from -r requirements.txt (line 10)) (1.10.2)\n",
      "Requirement already satisfied: tree-sitter==0.21.3 in /Users/tcapelle/miniforge3/envs/weave/lib/python3.11/site-packages (from -r requirements.txt (line 11)) (0.21.3)\n",
      "Requirement already satisfied: weave==0.50.14 in /Users/tcapelle/miniforge3/envs/weave/lib/python3.11/site-packages (from -r requirements.txt (line 12)) (0.50.14)\n",
      "Requirement already satisfied: sentence-transformers==3.0.1 in /Users/tcapelle/miniforge3/envs/weave/lib/python3.11/site-packages (from -r requirements.txt (line 13)) (3.0.1)\n",
      "Requirement already satisfied: litellm in /Users/tcapelle/miniforge3/envs/weave/lib/python3.11/site-packages (from -r requirements.txt (line 14)) (1.44.18)\n",
      "Requirement already satisfied: scipy in /Users/tcapelle/miniforge3/envs/weave/lib/python3.11/site-packages (from bm25s==0.1.10->-r requirements.txt (line 1)) (1.14.0)\n",
      "Requirement already satisfied: numpy in /Users/tcapelle/miniforge3/envs/weave/lib/python3.11/site-packages (from bm25s==0.1.10->-r requirements.txt (line 1)) (1.26.4)\n",
      "Requirement already satisfied: filelock in /Users/tcapelle/miniforge3/envs/weave/lib/python3.11/site-packages (from datasets==2.21.0->-r requirements.txt (line 2)) (3.14.0)\n",
      "Requirement already satisfied: pyarrow>=15.0.0 in /Users/tcapelle/miniforge3/envs/weave/lib/python3.11/site-packages (from datasets==2.21.0->-r requirements.txt (line 2)) (16.1.0)\n",
      "Requirement already satisfied: dill<0.3.9,>=0.3.0 in /Users/tcapelle/miniforge3/envs/weave/lib/python3.11/site-packages (from datasets==2.21.0->-r requirements.txt (line 2)) (0.3.8)\n",
      "Requirement already satisfied: requests>=2.32.2 in /Users/tcapelle/miniforge3/envs/weave/lib/python3.11/site-packages (from datasets==2.21.0->-r requirements.txt (line 2)) (2.32.3)\n",
      "Requirement already satisfied: tqdm>=4.66.3 in /Users/tcapelle/miniforge3/envs/weave/lib/python3.11/site-packages (from datasets==2.21.0->-r requirements.txt (line 2)) (4.66.5)\n",
      "Requirement already satisfied: xxhash in /Users/tcapelle/miniforge3/envs/weave/lib/python3.11/site-packages (from datasets==2.21.0->-r requirements.txt (line 2)) (3.4.1)\n",
      "Requirement already satisfied: multiprocess in /Users/tcapelle/miniforge3/envs/weave/lib/python3.11/site-packages (from datasets==2.21.0->-r requirements.txt (line 2)) (0.70.16)\n",
      "Requirement already satisfied: fsspec<=2024.6.1,>=2023.1.0 in /Users/tcapelle/miniforge3/envs/weave/lib/python3.11/site-packages (from fsspec[http]<=2024.6.1,>=2023.1.0->datasets==2.21.0->-r requirements.txt (line 2)) (2024.3.1)\n",
      "Requirement already satisfied: aiohttp in /Users/tcapelle/miniforge3/envs/weave/lib/python3.11/site-packages (from datasets==2.21.0->-r requirements.txt (line 2)) (3.10.5)\n",
      "Requirement already satisfied: huggingface-hub>=0.21.2 in /Users/tcapelle/miniforge3/envs/weave/lib/python3.11/site-packages (from datasets==2.21.0->-r requirements.txt (line 2)) (0.23.3)\n",
      "Requirement already satisfied: packaging in /Users/tcapelle/miniforge3/envs/weave/lib/python3.11/site-packages (from datasets==2.21.0->-r requirements.txt (line 2)) (24.1)\n",
      "Requirement already satisfied: pyyaml>=5.1 in /Users/tcapelle/miniforge3/envs/weave/lib/python3.11/site-packages (from datasets==2.21.0->-r requirements.txt (line 2)) (6.0.2)\n",
      "Requirement already satisfied: docstring-parser<0.17,>=0.16 in /Users/tcapelle/miniforge3/envs/weave/lib/python3.11/site-packages (from instructor==1.4.0->-r requirements.txt (line 3)) (0.16)\n",
      "Requirement already satisfied: jiter<0.5.0,>=0.4.1 in /Users/tcapelle/miniforge3/envs/weave/lib/python3.11/site-packages (from instructor==1.4.0->-r requirements.txt (line 3)) (0.4.2)\n",
      "Requirement already satisfied: pydantic-core<3.0.0,>=2.18.0 in /Users/tcapelle/miniforge3/envs/weave/lib/python3.11/site-packages (from instructor==1.4.0->-r requirements.txt (line 3)) (2.20.1)\n",
      "Requirement already satisfied: rich<14.0.0,>=13.7.0 in /Users/tcapelle/miniforge3/envs/weave/lib/python3.11/site-packages (from instructor==1.4.0->-r requirements.txt (line 3)) (13.7.1)\n",
      "Requirement already satisfied: tenacity<9.0.0,>=8.4.1 in /Users/tcapelle/miniforge3/envs/weave/lib/python3.11/site-packages (from instructor==1.4.0->-r requirements.txt (line 3)) (8.5.0)\n",
      "Requirement already satisfied: typer<1.0.0,>=0.9.0 in /Users/tcapelle/miniforge3/envs/weave/lib/python3.11/site-packages (from instructor==1.4.0->-r requirements.txt (line 3)) (0.12.3)\n",
      "Requirement already satisfied: anyio<5,>=3.5.0 in /Users/tcapelle/miniforge3/envs/weave/lib/python3.11/site-packages (from openai==1.43.0->-r requirements.txt (line 5)) (4.4.0)\n",
      "Requirement already satisfied: distro<2,>=1.7.0 in /Users/tcapelle/miniforge3/envs/weave/lib/python3.11/site-packages (from openai==1.43.0->-r requirements.txt (line 5)) (1.9.0)\n",
      "Requirement already satisfied: httpx<1,>=0.23.0 in /Users/tcapelle/miniforge3/envs/weave/lib/python3.11/site-packages (from openai==1.43.0->-r requirements.txt (line 5)) (0.27.2)\n",
      "Requirement already satisfied: sniffio in /Users/tcapelle/miniforge3/envs/weave/lib/python3.11/site-packages (from openai==1.43.0->-r requirements.txt (line 5)) (1.3.1)\n",
      "Requirement already satisfied: typing-extensions<5,>=4.11 in /Users/tcapelle/miniforge3/envs/weave/lib/python3.11/site-packages (from openai==1.43.0->-r requirements.txt (line 5)) (4.12.2)\n",
      "Requirement already satisfied: python-dateutil>=2.8.2 in /Users/tcapelle/miniforge3/envs/weave/lib/python3.11/site-packages (from pandas==2.2.2->-r requirements.txt (line 6)) (2.9.0.post0)\n",
      "Requirement already satisfied: pytz>=2020.1 in /Users/tcapelle/miniforge3/envs/weave/lib/python3.11/site-packages (from pandas==2.2.2->-r requirements.txt (line 6)) (2024.1)\n",
      "Requirement already satisfied: tzdata>=2022.7 in /Users/tcapelle/miniforge3/envs/weave/lib/python3.11/site-packages (from pandas==2.2.2->-r requirements.txt (line 6)) (2024.1)\n",
      "Requirement already satisfied: annotated-types>=0.4.0 in /Users/tcapelle/miniforge3/envs/weave/lib/python3.11/site-packages (from pydantic==2.8.2->-r requirements.txt (line 7)) (0.7.0)\n",
      "Requirement already satisfied: threadpoolctl>=3.1.0 in /Users/tcapelle/miniforge3/envs/weave/lib/python3.11/site-packages (from scikit_learn==1.5.1->-r requirements.txt (line 8)) (3.5.0)\n",
      "Requirement already satisfied: tiktoken>=0.4.0 in /Users/tcapelle/miniforge3/envs/weave/lib/python3.11/site-packages (from weave==0.50.14->-r requirements.txt (line 12)) (0.7.0)\n",
      "Requirement already satisfied: aiofiles>=22.1.0 in /Users/tcapelle/miniforge3/envs/weave/lib/python3.11/site-packages (from weave==0.50.14->-r requirements.txt (line 12)) (24.1.0)\n",
      "Requirement already satisfied: aioprocessing>=2.0.1 in /Users/tcapelle/miniforge3/envs/weave/lib/python3.11/site-packages (from weave==0.50.14->-r requirements.txt (line 12)) (2.0.1)\n",
      "Requirement already satisfied: Werkzeug>=3.0.3 in /Users/tcapelle/miniforge3/envs/weave/lib/python3.11/site-packages (from weave==0.50.14->-r requirements.txt (line 12)) (3.0.4)\n",
      "Requirement already satisfied: janus>=1.0.0 in /Users/tcapelle/miniforge3/envs/weave/lib/python3.11/site-packages (from weave==0.50.14->-r requirements.txt (line 12)) (1.0.0)\n",
      "Requirement already satisfied: python-json-logger>=2.0.4 in /Users/tcapelle/miniforge3/envs/weave/lib/python3.11/site-packages (from weave==0.50.14->-r requirements.txt (line 12)) (2.0.7)\n",
      "Requirement already satisfied: wandb>=0.16.4 in /Users/tcapelle/miniforge3/envs/weave/lib/python3.11/site-packages (from weave==0.50.14->-r requirements.txt (line 12)) (0.17.7)\n",
      "Requirement already satisfied: graphql-core>3 in /Users/tcapelle/miniforge3/envs/weave/lib/python3.11/site-packages (from weave==0.50.14->-r requirements.txt (line 12)) (3.2.3)\n",
      "Requirement already satisfied: gql>=3.4.1 in /Users/tcapelle/miniforge3/envs/weave/lib/python3.11/site-packages (from gql[requests]>=3.4.1->weave==0.50.14->-r requirements.txt (line 12)) (3.5.0)\n",
      "Requirement already satisfied: analytics-python>=1.2.9 in /Users/tcapelle/miniforge3/envs/weave/lib/python3.11/site-packages (from weave==0.50.14->-r requirements.txt (line 12)) (1.2.9)\n",
      "Requirement already satisfied: emoji>=2.12.1 in /Users/tcapelle/miniforge3/envs/weave/lib/python3.11/site-packages (from weave==0.50.14->-r requirements.txt (line 12)) (2.12.1)\n",
      "Requirement already satisfied: uuid-utils>=0.9.0 in /Users/tcapelle/miniforge3/envs/weave/lib/python3.11/site-packages (from weave==0.50.14->-r requirements.txt (line 12)) (0.9.0)\n",
      "Requirement already satisfied: transformers<5.0.0,>=4.34.0 in /Users/tcapelle/miniforge3/envs/weave/lib/python3.11/site-packages (from sentence-transformers==3.0.1->-r requirements.txt (line 13)) (4.43.2)\n",
      "Requirement already satisfied: torch>=1.11.0 in /Users/tcapelle/miniforge3/envs/weave/lib/python3.11/site-packages (from sentence-transformers==3.0.1->-r requirements.txt (line 13)) (2.4.0)\n",
      "Requirement already satisfied: Pillow in /Users/tcapelle/miniforge3/envs/weave/lib/python3.11/site-packages (from sentence-transformers==3.0.1->-r requirements.txt (line 13)) (10.4.0)\n",
      "Requirement already satisfied: click in /Users/tcapelle/miniforge3/envs/weave/lib/python3.11/site-packages (from litellm->-r requirements.txt (line 14)) (8.1.7)\n",
      "Requirement already satisfied: importlib-metadata>=6.8.0 in /Users/tcapelle/miniforge3/envs/weave/lib/python3.11/site-packages (from litellm->-r requirements.txt (line 14)) (8.4.0)\n",
      "Requirement already satisfied: jinja2<4.0.0,>=3.1.2 in /Users/tcapelle/miniforge3/envs/weave/lib/python3.11/site-packages (from litellm->-r requirements.txt (line 14)) (3.1.4)\n",
      "Requirement already satisfied: jsonschema<5.0.0,>=4.22.0 in /Users/tcapelle/miniforge3/envs/weave/lib/python3.11/site-packages (from litellm->-r requirements.txt (line 14)) (4.22.0)\n",
      "Requirement already satisfied: python-dotenv>=0.2.0 in /Users/tcapelle/miniforge3/envs/weave/lib/python3.11/site-packages (from litellm->-r requirements.txt (line 14)) (1.0.1)\n",
      "Requirement already satisfied: tokenizers in /Users/tcapelle/miniforge3/envs/weave/lib/python3.11/site-packages (from litellm->-r requirements.txt (line 14)) (0.19.1)\n",
      "Requirement already satisfied: aiohappyeyeballs>=2.3.0 in /Users/tcapelle/miniforge3/envs/weave/lib/python3.11/site-packages (from aiohttp->datasets==2.21.0->-r requirements.txt (line 2)) (2.4.0)\n",
      "Requirement already satisfied: aiosignal>=1.1.2 in /Users/tcapelle/miniforge3/envs/weave/lib/python3.11/site-packages (from aiohttp->datasets==2.21.0->-r requirements.txt (line 2)) (1.3.1)\n",
      "Requirement already satisfied: attrs>=17.3.0 in /Users/tcapelle/miniforge3/envs/weave/lib/python3.11/site-packages (from aiohttp->datasets==2.21.0->-r requirements.txt (line 2)) (24.2.0)\n",
      "Requirement already satisfied: frozenlist>=1.1.1 in /Users/tcapelle/miniforge3/envs/weave/lib/python3.11/site-packages (from aiohttp->datasets==2.21.0->-r requirements.txt (line 2)) (1.4.1)\n",
      "Requirement already satisfied: multidict<7.0,>=4.5 in /Users/tcapelle/miniforge3/envs/weave/lib/python3.11/site-packages (from aiohttp->datasets==2.21.0->-r requirements.txt (line 2)) (6.0.5)\n",
      "Requirement already satisfied: yarl<2.0,>=1.0 in /Users/tcapelle/miniforge3/envs/weave/lib/python3.11/site-packages (from aiohttp->datasets==2.21.0->-r requirements.txt (line 2)) (1.9.4)\n",
      "Requirement already satisfied: six>=1.5 in /Users/tcapelle/miniforge3/envs/weave/lib/python3.11/site-packages (from analytics-python>=1.2.9->weave==0.50.14->-r requirements.txt (line 12)) (1.16.0)\n",
      "Requirement already satisfied: idna>=2.8 in /Users/tcapelle/miniforge3/envs/weave/lib/python3.11/site-packages (from anyio<5,>=3.5.0->openai==1.43.0->-r requirements.txt (line 5)) (3.8)\n",
      "Requirement already satisfied: backoff<3.0,>=1.11.1 in /Users/tcapelle/miniforge3/envs/weave/lib/python3.11/site-packages (from gql>=3.4.1->gql[requests]>=3.4.1->weave==0.50.14->-r requirements.txt (line 12)) (2.2.1)\n",
      "Requirement already satisfied: requests-toolbelt<2,>=1.0.0 in /Users/tcapelle/miniforge3/envs/weave/lib/python3.11/site-packages (from gql[requests]>=3.4.1->weave==0.50.14->-r requirements.txt (line 12)) (1.0.0)\n",
      "Requirement already satisfied: certifi in /Users/tcapelle/miniforge3/envs/weave/lib/python3.11/site-packages (from httpx<1,>=0.23.0->openai==1.43.0->-r requirements.txt (line 5)) (2024.8.30)\n",
      "Requirement already satisfied: httpcore==1.* in /Users/tcapelle/miniforge3/envs/weave/lib/python3.11/site-packages (from httpx<1,>=0.23.0->openai==1.43.0->-r requirements.txt (line 5)) (1.0.5)\n",
      "Requirement already satisfied: h11<0.15,>=0.13 in /Users/tcapelle/miniforge3/envs/weave/lib/python3.11/site-packages (from httpcore==1.*->httpx<1,>=0.23.0->openai==1.43.0->-r requirements.txt (line 5)) (0.14.0)\n",
      "Requirement already satisfied: zipp>=0.5 in /Users/tcapelle/miniforge3/envs/weave/lib/python3.11/site-packages (from importlib-metadata>=6.8.0->litellm->-r requirements.txt (line 14)) (3.20.1)\n",
      "Requirement already satisfied: MarkupSafe>=2.0 in /Users/tcapelle/miniforge3/envs/weave/lib/python3.11/site-packages (from jinja2<4.0.0,>=3.1.2->litellm->-r requirements.txt (line 14)) (2.1.5)\n",
      "Requirement already satisfied: jsonschema-specifications>=2023.03.6 in /Users/tcapelle/miniforge3/envs/weave/lib/python3.11/site-packages (from jsonschema<5.0.0,>=4.22.0->litellm->-r requirements.txt (line 14)) (2023.12.1)\n",
      "Requirement already satisfied: referencing>=0.28.4 in /Users/tcapelle/miniforge3/envs/weave/lib/python3.11/site-packages (from jsonschema<5.0.0,>=4.22.0->litellm->-r requirements.txt (line 14)) (0.35.1)\n",
      "Requirement already satisfied: rpds-py>=0.7.1 in /Users/tcapelle/miniforge3/envs/weave/lib/python3.11/site-packages (from jsonschema<5.0.0,>=4.22.0->litellm->-r requirements.txt (line 14)) (0.18.1)\n",
      "Requirement already satisfied: charset-normalizer<4,>=2 in /Users/tcapelle/miniforge3/envs/weave/lib/python3.11/site-packages (from requests>=2.32.2->datasets==2.21.0->-r requirements.txt (line 2)) (3.3.2)\n",
      "Requirement already satisfied: urllib3<3,>=1.21.1 in /Users/tcapelle/miniforge3/envs/weave/lib/python3.11/site-packages (from requests>=2.32.2->datasets==2.21.0->-r requirements.txt (line 2)) (2.2.2)\n",
      "Requirement already satisfied: markdown-it-py>=2.2.0 in /Users/tcapelle/miniforge3/envs/weave/lib/python3.11/site-packages (from rich<14.0.0,>=13.7.0->instructor==1.4.0->-r requirements.txt (line 3)) (3.0.0)\n",
      "Requirement already satisfied: pygments<3.0.0,>=2.13.0 in /Users/tcapelle/miniforge3/envs/weave/lib/python3.11/site-packages (from rich<14.0.0,>=13.7.0->instructor==1.4.0->-r requirements.txt (line 3)) (2.18.0)\n",
      "Requirement already satisfied: regex>=2022.1.18 in /Users/tcapelle/miniforge3/envs/weave/lib/python3.11/site-packages (from tiktoken>=0.4.0->weave==0.50.14->-r requirements.txt (line 12)) (2024.7.24)\n",
      "Requirement already satisfied: sympy in /Users/tcapelle/miniforge3/envs/weave/lib/python3.11/site-packages (from torch>=1.11.0->sentence-transformers==3.0.1->-r requirements.txt (line 13)) (1.13.1)\n",
      "Requirement already satisfied: networkx in /Users/tcapelle/miniforge3/envs/weave/lib/python3.11/site-packages (from torch>=1.11.0->sentence-transformers==3.0.1->-r requirements.txt (line 13)) (3.3)\n",
      "Requirement already satisfied: safetensors>=0.4.1 in /Users/tcapelle/miniforge3/envs/weave/lib/python3.11/site-packages (from transformers<5.0.0,>=4.34.0->sentence-transformers==3.0.1->-r requirements.txt (line 13)) (0.4.3)\n",
      "Requirement already satisfied: shellingham>=1.3.0 in /Users/tcapelle/miniforge3/envs/weave/lib/python3.11/site-packages (from typer<1.0.0,>=0.9.0->instructor==1.4.0->-r requirements.txt (line 3)) (1.5.4)\n",
      "Requirement already satisfied: docker-pycreds>=0.4.0 in /Users/tcapelle/miniforge3/envs/weave/lib/python3.11/site-packages (from wandb>=0.16.4->weave==0.50.14->-r requirements.txt (line 12)) (0.4.0)\n",
      "Requirement already satisfied: gitpython!=3.1.29,>=1.0.0 in /Users/tcapelle/miniforge3/envs/weave/lib/python3.11/site-packages (from wandb>=0.16.4->weave==0.50.14->-r requirements.txt (line 12)) (3.1.43)\n",
      "Requirement already satisfied: platformdirs in /Users/tcapelle/miniforge3/envs/weave/lib/python3.11/site-packages (from wandb>=0.16.4->weave==0.50.14->-r requirements.txt (line 12)) (4.2.2)\n",
      "Requirement already satisfied: protobuf!=4.21.0,<6,>=3.19.0 in /Users/tcapelle/miniforge3/envs/weave/lib/python3.11/site-packages (from wandb>=0.16.4->weave==0.50.14->-r requirements.txt (line 12)) (5.27.3)\n",
      "Requirement already satisfied: psutil>=5.0.0 in /Users/tcapelle/miniforge3/envs/weave/lib/python3.11/site-packages (from wandb>=0.16.4->weave==0.50.14->-r requirements.txt (line 12)) (6.0.0)\n",
      "Requirement already satisfied: sentry-sdk>=1.0.0 in /Users/tcapelle/miniforge3/envs/weave/lib/python3.11/site-packages (from wandb>=0.16.4->weave==0.50.14->-r requirements.txt (line 12)) (2.13.0)\n",
      "Requirement already satisfied: setproctitle in /Users/tcapelle/miniforge3/envs/weave/lib/python3.11/site-packages (from wandb>=0.16.4->weave==0.50.14->-r requirements.txt (line 12)) (1.3.3)\n",
      "Requirement already satisfied: setuptools in /Users/tcapelle/miniforge3/envs/weave/lib/python3.11/site-packages (from wandb>=0.16.4->weave==0.50.14->-r requirements.txt (line 12)) (73.0.1)\n"
     ]
    }
   ],
   "source": [
    "# Clone the starter-kits repo\n",
    "!git clone https://github.com/tcapelle/hackercup_rag\n",
    "# Change directory to the rag folder. Running the next line twice in the same session will raise an error.\n",
    "%cd hackercup_rag\n",
    "# Install dependencies\n",
    "!pip install -r requirements.txt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Logged in as Weights & Biases user: capecape.\n",
      "View Weave data at https://wandb.ai/capecape/hackercup/weave\n"
     ]
    }
   ],
   "source": [
    "import weave\n",
    "\n",
    "WEAVE_PROJECT = \"hackercup\"\n",
    "weave_client = weave.init(WEAVE_PROJECT)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Dataset\n",
    "We will use [HackerCup dataset](https://huggingface.co/datasets/hackercupai/hackercup) in this notebook.\n",
    "\n",
    "Specifically, the **practice** dataset from the **2023** season.\n",
    "\n",
    "We have already processed the dataset and saved it as a [`weave.Dataset`](https://weave-docs.wandb.ai/guides/core-types/datasets/). You can either use the Dataset by running the next cell or download the dataset using the instructions below.\n",
    "\n",
    "We will use the dataset to load some practice problems and solutions from the HackerCup dataset and evaluate our agents on it."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Sample Problem:\n",
      "\n",
      " {\n",
      "  \"problem_dir\": \"data/2023/practice\",\n",
      "  \"problem_name\": \"two_apples_a_day\",\n",
      "  \"problem_description\": \"‚ÄúAn apple a day keeps the doctor away‚Äù is Steve‚Äôs motto. His other motto, ‚ÄúYou can never have too much of a good thing,‚Äù holds true for both apples and mottos. Steve would like to eat two apples per day for the next \\\\(N\\\\) days, but with strict adherence to his third motto ‚ÄúConsistency is key.‚Äù Specifically, he‚Äôd like the sum of the two apple weights he eats over the next \\\\(N\\\\) days to be the same for each day.\\n\\nSteve has already purchased \\\\(2*N-1\\\\) apples, the \\\\(i\\\\)th of which weighs \\\\(A_i\\\\) ounces. He'd like to buy one more apple that's as light as possible to fulfill his goal. Steve can buy an apple of any positive integer weight in ounces from the store. Is it possible for him to reach his goal, and if so, what weight apple should he buy?\\n\\n{{PHOTO_ID:1563872647765708|WIDTH:600}}\\n\\n\\n*The above image depicts the solution to the first sample. Each day, Steve will eat two apples totalling \\\\(7\\\\) oz. Steve must buy a \\\\(4\\\\) oz apple to make this happen.*\\n\\n# Constraints\\n\\\\(1 \\\\leq T \\\\leq 70\\\\)\\n\\\\(1 \\\\leq N \\\\leq 3*10^5\\\\)\\nThe sum of \\\\(N\\\\) over all cases is at most \\\\(600{,}000\\\\)\\n\\\\(1 \\\\leq A_i \\\\leq  10^9\\\\)\\n\\n# Input Format\\nInput begins with an integer \\\\(T\\\\), the number of test cases. Each test case starts with a single integer \\\\(N\\\\). The next line contains \\\\(2*N-1\\\\) space-separated integers \\\\(A_1, ..., A_{2*N - 1}\\\\).\\n\\n# Output Format\\nFor the \\\\(i\\\\)th test case, print \\\"`Case #i:` \\\" followed a single integer, the smallest possible apple weight in ounces that Steve can buy so that he can eat two apples for the next \\\\(N\\\\) days and have the sum of apple weights be the same every day, or \\\\(-1\\\\) if doing so is impossible.\\n\\n# Sample Explanation\\n\\nIn the first case, if Steve buys a \\\\(4\\\\) oz apple, he can group his apples as shown above. For this input, there's no way to succeed by buying any apple below \\\\(4\\\\) oz.\\n\\nIn the second case, Steve can buy a \\\\(7\\\\) oz apple, and eat two apples totaling \\\\(14\\\\) oz each day.\\n\\nIn the third case, any apple weight will suffice, so Steve will buy the lightest one possible.\\n\\nIn the fourth case, no matter what weight apple Steve attempts to buy, it is impossible for him to achieve his goal.\\n\\nPlease note, as demonstrated in the seventh case, that it's possible for the answer to exceed \\\\(10^9\\\\).\\n\\n\\n\",\n",
      "  \"sample_input\": \"7\\n3\\n6 3 1 2 5\\n2\\n7 7 7\\n1\\n1\\n3\\n1 9 1 1 4\\n4\\n1 9 1 1 4 9 9\\n4\\n1 9 10 1 4 6 9\\n3\\n1000000000 2 10 4 999999994\\n\",\n",
      "  \"sample_output\": \"Case #1: 4\\nCase #2: 7\\nCase #3: 1\\nCase #4: -1\\nCase #5: 6\\nCase #6: -1\\nCase #7: 1000000002\\n\",\n",
      "  \"problem_input\": \"data/2023/practice/two_apples_a_day.in\",\n",
      "  \"problem_output\": \"data/2023/practice/two_apples_a_day.out\"\n",
      "}\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "\n",
    "os.environ[\"BASE_URL\"] = \"http://195.242.24.252:8000/v1\"\n",
    "os.environ[\"MAX_TOKENS\"] = \"2048\"\n",
    "os.environ[\"WEAVE_PARALLELISM\"] = 1\n",
    "\n",
    "from utils import (\n",
    "    Problem,\n",
    "    FAST_LLM,\n",
    "    STRONG_LLM,\n",
    ")\n",
    "\n",
    "\n",
    "practice_dataset_uri = \"weave:///parambharat/hackercup/object/practice_dataset:R35fXf9N3FE2IOesg7bRPaPAxiE9YbpirhXO9HcHs8w\"\n",
    "problems_dataset = weave.ref(practice_dataset_uri).get().rows[:]\n",
    "problems = list(map(lambda x: Problem(**x), problems_dataset))\n",
    "problem = problems[0]\n",
    "print(\"Sample Problem:\\n\\n\", problem.model_dump_json(indent=2))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Alternatively, you can download the dataset by running the download script from the [submit-first-solution](https://github.com/HackerCupAI/starter-kits/tree/main/submit_first_solution). Specifically, you can run the following command to download the dataset:\n",
    "\n",
    "```bash\n",
    "python download.py --year 2023 --dataset_folder data\n",
    "```\n",
    "\n",
    "\n",
    "This should create a `dataset` folder with the problems and solutions.\n",
    "\n",
    "Here's an example of what the data looks like for the `dim_sum_delivery` problem from the `2023` season:\n",
    "\n",
    "```\n",
    "data/dataset/2023/practice\n",
    "...\n",
    "‚îú‚îÄ‚îÄ dim_sum_delivery.cpp\n",
    "‚îú‚îÄ‚îÄ dim_sum_delivery.in\n",
    "‚îú‚îÄ‚îÄ dim_sum_delivery.md\n",
    "‚îú‚îÄ‚îÄ dim_sum_delivery.out\n",
    "‚îú‚îÄ‚îÄ dim_sum_delivery_sample_input.txt\n",
    "‚îú‚îÄ‚îÄ dim_sum_delivery_sample_output.txt\n",
    "‚îú‚îÄ‚îÄ dim_sum_delivery_sol.md\n",
    "...\n",
    "```\n",
    "\n",
    "Each problem has a `in`, `out`, `md`, `cpp`, and `sol` file.\n",
    "\n",
    "The `in` file contains the input data for the problem.\n",
    "The `out` file contains the expected output for the problem.\n",
    "The `md` file contains the problem statement.\n",
    "The `cpp` file contains the source code to the solution.\n",
    "The `sol` file contains the detailed solution to the problem.\n",
    "The `sample_input.txt` and `sample_output.txt` files contain the sample input and output for the problem. These are the test cases that will be available to the agent during development and evaluation."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import asyncio\n",
    "import logging\n",
    "\n",
    "from nest_asyncio import apply\n",
    "\n",
    "apply()\n",
    "\n",
    "# Some logging to see the progress\n",
    "logging.basicConfig(\n",
    "    format=\"%(asctime)s : %(levelname)s : %(message)s\", level=logging.INFO\n",
    ")\n",
    "\n",
    "logger = logging.getLogger(__name__)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now that we have a way to load a problem and evaluate a solution, let's define a prompt to solve the problem and create a simple agent to solve the problem. \n",
    "\n",
    "Here'e one such prompt we will use to solve the problem, it contains instructions for the model on how to solve the problem and the format of the response we expect from the model. Feel free to tweak the prompt if you like but this should work decently well for our use case."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Reflection Agent\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "While the RAG agent is an improvement over the zero-shot agent, it's still not perfect.\n",
    "It's still susceptible to hallucinations and incorrect solutions. \n",
    "One way to mitigate this is to use reflection.\n",
    "We can use another LLM call to reflect on the solution and test results and improve it.\n",
    "We can then use the improved solution to generate new few-shot examples and repeat the process in a loop until we converge to a solution or the iteration limit is reached.\n",
    "\n",
    "Again, this is not the best approach to solve the problem and has a lot of room for improvement, but it should help us get towards a working solution.\n",
    "\n",
    "Here are the reflection instructions we will provide to the LLM to reflect on the solution and test results, feel free to change the instructions to improve the agent's performance."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2024-09-05 22:56:39,984 : INFO : PyTorch version 2.4.0 available.\n",
      "2024-09-05 22:56:42,080 : INFO : Use pytorch device_name: mps\n",
      "2024-09-05 22:56:42,080 : INFO : Load pretrained SentenceTransformer: jinaai/jina-embeddings-v2-base-code\n",
      "2024-09-05 22:56:49,314 : DEBUG : Building index from IDs objects      \n",
      "                                                                               "
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "You are a world-class competitive programmer with a keen eye for detail and problem solving. \n",
      "Your expertise is in algorithms and data structures. \n",
      "You have incorrectly answered the following programming problem. \n",
      "Your task is to reflect on the problem, your solution, and the correct answer.\n",
      "You will then use this information help you answer the same question in the future. \n",
      "First, explain why you answered the question incorrectly.\n",
      "Second, list the keywords that describe the type of your errors from most general to most specific.\n",
      "Third, solve the problem again, step-by-step, based on your knowledge of the correct answer.\n",
      "Fourth, create a list of detailed instructions to help you correctly solve this problem in the future.\n",
      "Finally, create a list of general advice to help you solve similar types of problems in the future.\n",
      "Be concise in your response; however, capture all of the essential information.\n",
      "\n",
      "{problem}\n",
      "<incorrect_solution>\n",
      "{incorrect_solution}\n",
      "</incorrect_solution>\n",
      "<test_report>\n",
      "{test_report}\n",
      "</test_report>\n",
      "\n",
      "**Format Instructions: Your response must follow the following xml format** -\n",
      "\n",
      "<root>\n",
      "<reflection>\n",
      "[Reflect on the problem, your solution, and the correct answer.]\n",
      "</reflection>\n",
      "<keywords>\n",
      "[List the keywords that describe the type of your errors from most general to most specific.]\n",
      "</keywords>\n",
      "<step_by_step_solution>\n",
      "[Solve the problem again, step-by-step, based on your knowledge of the correct answer.]\n",
      "</step_by_step_solution>\n",
      "<instructions>\n",
      "[Create a list of detailed instructions to help you correctly solve this problem in the future.]\n",
      "</instructions>\n",
      "<general_advice>\n",
      "[Create a list of general advice to help you solve similar types of problems in the future.]\n",
      "</general_advice>\n",
      "</root>\n",
      "---\n",
      "Let's think step by step to reflect on the problem:\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r"
     ]
    }
   ],
   "source": [
    "from agent import rag_solver\n",
    "from retriever import Retriever\n",
    "\n",
    "retriever = Retriever()\n",
    "\n",
    "from agent import REFLECTION_INSTRUCTIONS, rework_solution\n",
    "\n",
    "print(REFLECTION_INSTRUCTIONS)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "@weave.op\n",
    "async def rag_solver_with_reflection(\n",
    "        retriever: Retriever,\n",
    "        problem: Problem,\n",
    "        model: str = FAST_LLM,\n",
    "        temperature: float = 0.1,\n",
    "        max_iterations: int = 2,\n",
    "        timeout: int = 10,\n",
    "):\n",
    "    num_iterations = 0\n",
    "    test_report = \"failed\"\n",
    "    solution = None\n",
    "    while not test_report == \"passed\" and num_iterations < max_iterations:\n",
    "        rag_result = await rag_solver(\n",
    "            retriever=retriever,\n",
    "            problem=problem,\n",
    "            timeout=timeout,\n",
    "            model=model,\n",
    "            temperature=temperature,\n",
    "        )\n",
    "        solution = rag_result[\"solution\"]\n",
    "        test_report = rag_result[\"test_report\"]\n",
    "        if test_report == \"passed\":\n",
    "            return rag_result\n",
    "        rework_result = await rework_solution(\n",
    "            problem=problem,\n",
    "            incorrect_solution=solution,\n",
    "            test_report=test_report,\n",
    "            model=model,\n",
    "            temperature=temperature,\n",
    "            timeout=timeout,\n",
    "        )\n",
    "        solution = rework_result[\"solution\"]\n",
    "        test_report = rework_result[\"test_report\"]\n",
    "        if test_report == \"passed\":\n",
    "            return {\n",
    "                \"solution\": solution,\n",
    "                \"stage\": \"reflection\",\n",
    "                \"test_report\": test_report,\n",
    "            }\n",
    "        num_iterations += 1\n",
    "    logger.info(\"Failed to generate a solution\")\n",
    "    return {\"solution\": solution, \"stage\": \"failed\", \"test_report\": test_report}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2024-09-05 22:56:52,762 : INFO : Drafting intial zero-shot solution\n",
      "\u001b[92m22:57:10 - LiteLLM:INFO\u001b[0m: utils.py:2975 - \n",
      "LiteLLM completion() model= open-mistral-nemo-2407; provider = mistral\n",
      "2024-09-05 22:57:10,333 : INFO : \n",
      "LiteLLM completion() model= open-mistral-nemo-2407; provider = mistral\n",
      "2024-09-05 22:57:16,959 : INFO : HTTP Request: POST http://195.242.24.252:8000/v1/chat/completions \"HTTP/1.1 200 OK\"\n",
      "\u001b[92m22:57:17 - LiteLLM:INFO\u001b[0m: utils.py:2975 - \n",
      "LiteLLM completion() model= open-mistral-nemo-2407; provider = mistral\n",
      "2024-09-05 22:57:17,598 : INFO : \n",
      "LiteLLM completion() model= open-mistral-nemo-2407; provider = mistral\n",
      "2024-09-05 22:57:22,799 : INFO : HTTP Request: POST http://195.242.24.252:8000/v1/chat/completions \"HTTP/1.1 200 OK\"\n",
      "2024-09-05 22:57:25,034 : INFO : Draft solution result: \"WRONG ANSWER!!\\n\\n<expected>\\n'Case #1: 4\\nCase #2: 7\\nCase #3: 1\\nCase #4: -1\\nCase #5: 6\\nCase #6: -1\\nCase #7: 1000000002\\n'\\n</expected>\\n---\\n<got>\\n'Case #1: 1\\nCase #2: 3\\nCase #3: 1\\nCase #4: 2\\nCase #5: 6\\nCase #6: 0\\nCase #7: 0\\n'\\n</got>\"\n",
      "2024-09-05 22:57:25,035 : INFO : Iterating on a RAG solution\n",
      "2024-09-05 22:57:25,624 : INFO : Generating RAG solution:\n",
      "2024-09-05 22:57:26,557 : INFO : Generating examplars:\n",
      "Batches: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 1/1 [00:00<00:00,  1.67it/s]\n",
      "Batches: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 2/2 [00:06<00:00,  3.48s/it]\n",
      "\u001b[92m22:57:37 - LiteLLM:INFO\u001b[0m: utils.py:2975 - \n",
      "LiteLLM completion() model= open-mistral-nemo-2407; provider = mistral\n",
      "2024-09-05 22:57:37,745 : INFO : \n",
      "LiteLLM completion() model= open-mistral-nemo-2407; provider = mistral\n",
      "\u001b[92m22:57:37 - LiteLLM:INFO\u001b[0m: utils.py:2975 - \n",
      "LiteLLM completion() model= open-mistral-nemo-2407; provider = mistral\n",
      "2024-09-05 22:57:37,746 : INFO : \n",
      "LiteLLM completion() model= open-mistral-nemo-2407; provider = mistral\n",
      "\u001b[92m22:57:37 - LiteLLM:INFO\u001b[0m: utils.py:2975 - \n",
      "LiteLLM completion() model= open-mistral-nemo-2407; provider = mistral\n",
      "2024-09-05 22:57:37,749 : INFO : \n",
      "LiteLLM completion() model= open-mistral-nemo-2407; provider = mistral\n",
      "\u001b[92m22:57:37 - LiteLLM:INFO\u001b[0m: utils.py:2975 - \n",
      "LiteLLM completion() model= open-mistral-nemo-2407; provider = mistral\n",
      "\u001b[92m22:57:37 - LiteLLM:INFO\u001b[0m: utils.py:2975 - \n",
      "LiteLLM completion() model= open-mistral-nemo-2407; provider = mistral\n",
      "2024-09-05 22:57:37,750 : INFO : \n",
      "LiteLLM completion() model= open-mistral-nemo-2407; provider = mistral\n",
      "2024-09-05 22:57:37,752 : INFO : \n",
      "LiteLLM completion() model= open-mistral-nemo-2407; provider = mistral\n",
      "2024-09-05 22:57:45,056 : INFO : HTTP Request: POST http://195.242.24.252:8000/v1/chat/completions \"HTTP/1.1 200 OK\"\n",
      "\u001b[92m22:57:45 - LiteLLM:INFO\u001b[0m: utils.py:2975 - \n",
      "LiteLLM completion() model= open-mistral-nemo-2407; provider = mistral\n",
      "2024-09-05 22:57:45,073 : INFO : \n",
      "LiteLLM completion() model= open-mistral-nemo-2407; provider = mistral\n",
      "2024-09-05 22:57:45,750 : INFO : HTTP Request: POST http://195.242.24.252:8000/v1/chat/completions \"HTTP/1.1 200 OK\"\n",
      "\u001b[92m22:57:45 - LiteLLM:INFO\u001b[0m: utils.py:2975 - \n",
      "LiteLLM completion() model= open-mistral-nemo-2407; provider = mistral\n",
      "2024-09-05 22:57:45,757 : INFO : \n",
      "LiteLLM completion() model= open-mistral-nemo-2407; provider = mistral\n",
      "2024-09-05 22:57:52,214 : INFO : HTTP Request: POST http://195.242.24.252:8000/v1/chat/completions \"HTTP/1.1 200 OK\"\n",
      "2024-09-05 22:57:52,768 : INFO : HTTP Request: POST http://195.242.24.252:8000/v1/chat/completions \"HTTP/1.1 200 OK\"\n",
      "\u001b[92m22:57:52 - LiteLLM:INFO\u001b[0m: utils.py:2975 - \n",
      "LiteLLM completion() model= open-mistral-nemo-2407; provider = mistral\n",
      "2024-09-05 22:57:52,779 : INFO : \n",
      "LiteLLM completion() model= open-mistral-nemo-2407; provider = mistral\n",
      "2024-09-05 22:57:54,632 : INFO : HTTP Request: POST http://195.242.24.252:8000/v1/chat/completions \"HTTP/1.1 200 OK\"\n",
      "\u001b[92m22:57:54 - LiteLLM:INFO\u001b[0m: utils.py:2975 - \n",
      "LiteLLM completion() model= open-mistral-nemo-2407; provider = mistral\n",
      "2024-09-05 22:57:54,638 : INFO : \n",
      "LiteLLM completion() model= open-mistral-nemo-2407; provider = mistral\n",
      "2024-09-05 22:57:57,985 : INFO : HTTP Request: POST http://195.242.24.252:8000/v1/chat/completions \"HTTP/1.1 500 Internal Server Error\"\n",
      "2024-09-05 22:57:57,991 : INFO : Retrying request to /chat/completions in 0.975597 seconds\n",
      "2024-09-05 22:57:59,622 : INFO : HTTP Request: POST http://195.242.24.252:8000/v1/chat/completions \"HTTP/1.1 200 OK\"\n",
      "2024-09-05 22:58:02,488 : INFO : HTTP Request: POST http://195.242.24.252:8000/v1/chat/completions \"HTTP/1.1 200 OK\"\n",
      "2024-09-05 22:58:05,876 : INFO : HTTP Request: POST http://195.242.24.252:8000/v1/chat/completions \"HTTP/1.1 500 Internal Server Error\"\n",
      "2024-09-05 22:58:05,878 : INFO : Retrying request to /chat/completions in 0.952649 seconds\n",
      "2024-09-05 22:58:19,067 : INFO : HTTP Request: POST http://195.242.24.252:8000/v1/chat/completions \"HTTP/1.1 500 Internal Server Error\"\n",
      "2024-09-05 22:58:19,072 : INFO : Retrying request to /chat/completions in 1.966158 seconds\n",
      "2024-09-05 22:58:21,765 : INFO : HTTP Request: POST http://195.242.24.252:8000/v1/chat/completions \"HTTP/1.1 200 OK\"\n",
      "2024-09-05 22:58:41,072 : INFO : HTTP Request: POST http://195.242.24.252:8000/v1/chat/completions \"HTTP/1.1 200 OK\"\n",
      "\u001b[92m22:58:41 - LiteLLM:INFO\u001b[0m: utils.py:2975 - \n",
      "LiteLLM completion() model= open-mistral-nemo-2407; provider = mistral\n",
      "2024-09-05 22:58:41,088 : INFO : \n",
      "LiteLLM completion() model= open-mistral-nemo-2407; provider = mistral\n",
      "2024-09-05 22:58:54,216 : INFO : HTTP Request: POST http://195.242.24.252:8000/v1/chat/completions \"HTTP/1.1 200 OK\"\n",
      "\u001b[92m22:58:55 - LiteLLM:INFO\u001b[0m: utils.py:2975 - \n",
      "LiteLLM completion() model= open-mistral-nemo-2407; provider = mistral\n",
      "2024-09-05 22:58:55,428 : INFO : \n",
      "LiteLLM completion() model= open-mistral-nemo-2407; provider = mistral\n",
      "2024-09-05 22:59:08,463 : INFO : HTTP Request: POST http://195.242.24.252:8000/v1/chat/completions \"HTTP/1.1 200 OK\"\n",
      "\u001b[92m22:59:08 - LiteLLM:INFO\u001b[0m: utils.py:2975 - \n",
      "LiteLLM completion() model= open-mistral-nemo-2407; provider = mistral\n",
      "2024-09-05 22:59:08,479 : INFO : \n",
      "LiteLLM completion() model= open-mistral-nemo-2407; provider = mistral\n",
      "2024-09-05 22:59:28,568 : INFO : HTTP Request: POST http://195.242.24.252:8000/v1/chat/completions \"HTTP/1.1 500 Internal Server Error\"\n",
      "2024-09-05 22:59:28,576 : INFO : Retrying request to /chat/completions in 0.897552 seconds\n",
      "2024-09-05 22:59:39,118 : INFO : HTTP Request: POST http://195.242.24.252:8000/v1/chat/completions \"HTTP/1.1 200 OK\"\n",
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
      "2024-09-05 22:59:39,767 : INFO : RAG Solution Result: \"WRONG ANSWER!!\\n\\n<expected>\\n'Case #1: 4\\nCase #2: 7\\nCase #3: 1\\nCase #4: -1\\nCase #5: 6\\nCase #6: -1\\nCase #7: 1000000002\\n'\\n</expected>\\n---\\n<got>\\n'Case #1: -1\\nCase #2: -1\\nCase #3: 0\\nCase #4: -1\\nCase #5: -1\\nCase #6: 0\\nCase #7: 0\\n'\\n</got>\"\n",
      "2024-09-05 22:59:40,597 : INFO : Reflecting and improving solution\n",
      "\u001b[92m22:59:41 - LiteLLM:INFO\u001b[0m: utils.py:2975 - \n",
      "LiteLLM completion() model= open-mistral-nemo-2407; provider = mistral\n",
      "2024-09-05 22:59:41,320 : INFO : \n",
      "LiteLLM completion() model= open-mistral-nemo-2407; provider = mistral\n",
      "2024-09-05 22:59:47,737 : INFO : HTTP Request: POST http://195.242.24.252:8000/v1/chat/completions \"HTTP/1.1 200 OK\"\n",
      "2024-09-05 22:59:47,743 : INFO : Completion parameters:\n",
      "2024-09-05 22:59:47,743 : INFO :   model: mistral/open-mistral-nemo-2407\n",
      "2024-09-05 22:59:47,743 : INFO :   messages: [{'role': 'system', 'content': 'You are a world-class competitive programmer with a keen eye for detail and problem solving. \\nYour expertise is in algorithms and data structures. \\nYou have incorrectly answered the following programming problem. \\nYour task is to reflect on the problem, your solution, and the correct answer.\\nYou will then use this information help you answer the same question in the future. \\nFirst, explain why you answered the question incorrectly.\\nSecond, list the keywords that describe the type of your errors from most general to most specific.\\nThird, solve the problem again, step-by-step, based on your knowledge of the correct answer.\\nFourth, create a list of detailed instructions to help you correctly solve this problem in the future.\\nFinally, create a list of general advice to help you solve similar types of problems in the future.\\nBe concise in your response; however, capture all of the essential information.\\n\\n\\n<problem>\\n<problem_statement>\\n‚ÄúAn apple a day keeps the doctor away‚Äù is Steve‚Äôs motto. His other motto, ‚ÄúYou can never have too much of a good thing,‚Äù holds true for both apples and mottos. Steve would like to eat two apples per day for the next \\\\(N\\\\) days, but with strict adherence to his third motto ‚ÄúConsistency is key.‚Äù Specifically, he‚Äôd like the sum of the two apple weights he eats over the next \\\\(N\\\\) days to be the same for each day.\\nSteve has already purchased \\\\(2*N-1\\\\) apples, the \\\\(i\\\\)th of which weighs \\\\(A_i\\\\) ounces. He\\'d like to buy one more apple that\\'s as light as possible to fulfill his goal. Steve can buy an apple of any positive integer weight in ounces from the store. Is it possible for him to reach his goal, and if so, what weight apple should he buy?\\n{{PHOTO_ID:1563872647765708|WIDTH:600}}\\n*The above image depicts the solution to the first sample. Each day, Steve will eat two apples totalling \\\\(7\\\\) oz. Steve must buy a \\\\(4\\\\) oz apple to make this happen.*\\n# Constraints\\n\\\\(1 \\\\leq T \\\\leq 70\\\\)\\n\\\\(1 \\\\leq N \\\\leq 3*10^5\\\\)\\nThe sum of \\\\(N\\\\) over all cases is at most \\\\(600{,}000\\\\)\\n\\\\(1 \\\\leq A_i \\\\leq  10^9\\\\)\\n# Input Format\\nInput begins with an integer \\\\(T\\\\), the number of test cases. Each test case starts with a single integer \\\\(N\\\\). The next line contains \\\\(2*N-1\\\\) space-separated integers \\\\(A_1, ..., A_{2*N - 1}\\\\).\\n# Output Format\\nFor the \\\\(i\\\\)th test case, print \"`Case #i:` \" followed a single integer, the smallest possible apple weight in ounces that Steve can buy so that he can eat two apples for the next \\\\(N\\\\) days and have the sum of apple weights be the same every day, or \\\\(-1\\\\) if doing so is impossible.\\n# Sample Explanation\\nIn the first case, if Steve buys a \\\\(4\\\\) oz apple, he can group his apples as shown above. For this input, there\\'s no way to succeed by buying any apple below \\\\(4\\\\) oz.\\nIn the second case, Steve can buy a \\\\(7\\\\) oz apple, and eat two apples totaling \\\\(14\\\\) oz each day.\\nIn the third case, any apple weight will suffice, so Steve will buy the lightest one possible.\\nIn the fourth case, no matter what weight apple Steve attempts to buy, it is impossible for him to achieve his goal.\\nPlease note, as demonstrated in the seventh case, that it\\'s possible for the answer to exceed \\\\(10^9\\\\).\\n\\n</problem_statement>\\n<sample_test_cases>\\n<sample_input>\\n7\\n3\\n6 3 1 2 5\\n2\\n7 7 7\\n1\\n1\\n3\\n1 9 1 1 4\\n4\\n1 9 1 1 4 9 9\\n4\\n1 9 10 1 4 6 9\\n3\\n1000000000 2 10 4 999999994\\n\\n</sample_input>\\n<sample_output>\\nCase #1: 4\\nCase #2: 7\\nCase #3: 1\\nCase #4: -1\\nCase #5: 6\\nCase #6: -1\\nCase #7: 1000000002\\n\\n</sample_output>\\n</sample_test_cases>\\n</problem>\\n\\n<incorrect_solution>\\n\\n<root>\\n\\n<core_question>\\nGiven a list of apple weights and a number of days (N), determine if it\\'s possible for Steve to eat two apples per day for the next N days such that the sum of the two apple weights he eats over the next N days is the same for each day. If it\\'s possible, find the smallest possible weight of the additional apple Steve needs to buy.\\n</core_question>\\n<problem_solving_info>\\n[\"To solve this problem, we can first calculate the total weight of the apples Steve already has. Then, we can calculate the average weight of the apples he needs to eat each day to meet his goal. If the average weight is not an integer, it\\'s impossible for Steve to meet his goal. Otherwise, we can calculate the weight of the additional apple Steve needs to buy by subtracting the total weight of the apples he already has from the total weight he needs to have after N days. If the result is negative, it\\'s impossible for Steve to meet his goal. Otherwise, the result is the weight of the additional apple Steve needs to buy.\"]\\n</problem_solving_info>\\n<algorithm>\\n1. Read the number of test cases (T).\\n2. For each test case:\\n   a. Read the number of days (N).\\n   b. Read the list of apple weights (A).\\n   c. Calculate the total weight of the apples Steve already has (total_weight).\\n   d. Calculate the average weight of the apples Steve needs to eat each day (avg_weight).\\n   e. If avg_weight is not an integer, print \\'-1\\' and continue to the next test case.\\n   f. Calculate the total weight Steve needs to have after N days (total_weight_needed).\\n   g. Calculate the weight of the additional apple Steve needs to buy (additional_weight).\\n   h. If additional_weight is negative, print \\'-1\\'. Otherwise, print the result.\\n</algorithm>\\n<tutorial>\\nThe algorithm used here is a simple mathematical approach to solve the problem. The key insight is to calculate the average weight of the apples Steve needs to eat each day and use that to determine if it\\'s possible for him to meet his goal. If it\\'s possible, the algorithm then calculates the weight of the additional apple Steve needs to buy to reach his goal.\\n</tutorial>\\n<plan>\\n1. Read the number of test cases (T).\\n2. For each test case:\\n   a. Read the number of days (N).\\n   b. Read the list of apple weights (A).\\n   c. Calculate the total weight of the apples Steve already has (total_weight).\\n   d. Calculate the average weight of the apples Steve needs to eat each day (avg_weight).\\n   e. If avg_weight is not an integer, print \\'-1\\' and continue to the next test case.\\n   f. Calculate the total weight Steve needs to have after N days (total_weight_needed).\\n   g. Calculate the weight of the additional apple Steve needs to buy (additional_weight).\\n   h. If additional_weight is negative, print \\'-1\\'. Otherwise, print the result.\\n</plan>\\n<pseudocode>\\n1. PROCEDURE SolveProblem()\\n2.     READ numberOfTestCases\\n3.     FOR testCase FROM 1 TO numberOfTestCases\\n4.         READ numberOfDays\\n5.         READ appleWeights\\n6.         LET totalWeight = 0\\n7.         FOR weight IN appleWeights\\n8.             totalWeight += weight\\n9.         END FOR\\n10.        LET avgWeight = totalWeight / numberOfDays\\n11.        IF avgWeight % 1 != 0 THEN\\n12.            PRINT \\'-1\\'\\n13.            CONTINUE\\n14.        END IF\\n15.        LET totalWeightNeeded = avgWeight * numberOfDays\\n16.        LET additionalWeight = totalWeightNeeded - totalWeight\\n17.        IF additionalWeight < 0 THEN\\n18.            PRINT \\'-1\\'\\n19.        ELSE\\n20.            PRINT additionalWeight\\n21.        END IF\\n22.     END FOR\\n23. END PROCEDURE\\n</pseudocode>\\n\\n<source_code>\\nt = int(input())\\nfor i in range(t):\\n    n = int(input())\\n    a = list(map(int, input().split()))\\n    total = sum(a)\\n    avg = total // n\\n    if total % n != 0:\\n        print(\\'Case #\\' + str(i + 1) + \\': -1\\')\\n        continue\\n    total_needed = avg * n\\n    additional = total_needed - total\\n    print(\\'Case #\\' + str(i + 1) + \\': \\' + str(additional))\\n</source_code>\\n</root>\\n\\n</incorrect_solution>\\n<test_report>\\nWRONG ANSWER!!\\n\\n<expected>\\n\\'Case #1: 4\\nCase #2: 7\\nCase #3: 1\\nCase #4: -1\\nCase #5: 6\\nCase #6: -1\\nCase #7: 1000000002\\n\\'\\n</expected>\\n---\\n<got>\\n\\'Case #1: -1\\nCase #2: -1\\nCase #3: 0\\nCase #4: -1\\nCase #5: -1\\nCase #6: 0\\nCase #7: 0\\n\\'\\n</got>\\n</test_report>\\n\\n**Format Instructions: Your response must follow the following xml format** -\\n\\n<root>\\n<reflection>\\n[Reflect on the problem, your solution, and the correct answer.]\\n</reflection>\\n<keywords>\\n[List the keywords that describe the type of your errors from most general to most specific.]\\n</keywords>\\n<step_by_step_solution>\\n[Solve the problem again, step-by-step, based on your knowledge of the correct answer.]\\n</step_by_step_solution>\\n<instructions>\\n[Create a list of detailed instructions to help you correctly solve this problem in the future.]\\n</instructions>\\n<general_advice>\\n[Create a list of general advice to help you solve similar types of problems in the future.]\\n</general_advice>\\n</root>\\n---\\nLet\\'s think step by step to reflect on the problem:\\n'}]\n",
      "2024-09-05 22:59:47,744 : INFO :   response_model: None\n",
      "2024-09-05 22:59:47,744 : INFO :   temperature: 0.1\n",
      "2024-09-05 22:59:47,744 : INFO :   base_url: http://195.242.24.252:8000/v1\n",
      "\u001b[92m22:59:47 - LiteLLM:INFO\u001b[0m: utils.py:2975 - \n",
      "LiteLLM completion() model= open-mistral-nemo-2407; provider = mistral\n",
      "2024-09-05 22:59:47,749 : INFO : \n",
      "LiteLLM completion() model= open-mistral-nemo-2407; provider = mistral\n",
      "2024-09-05 23:00:07,488 : INFO : HTTP Request: POST http://195.242.24.252:8000/v1/chat/completions \"HTTP/1.1 200 OK\"\n",
      "\u001b[92m23:00:08 - LiteLLM:INFO\u001b[0m: utils.py:2975 - \n",
      "LiteLLM completion() model= open-mistral-nemo-2407; provider = mistral\n",
      "2024-09-05 23:00:08,660 : INFO : \n",
      "LiteLLM completion() model= open-mistral-nemo-2407; provider = mistral\n",
      "2024-09-05 23:00:18,883 : INFO : HTTP Request: POST http://195.242.24.252:8000/v1/chat/completions \"HTTP/1.1 200 OK\"\n",
      "\u001b[92m23:00:18 - LiteLLM:INFO\u001b[0m: utils.py:2975 - \n",
      "LiteLLM completion() model= open-mistral-nemo-2407; provider = mistral\n",
      "2024-09-05 23:00:18,897 : INFO : \n",
      "LiteLLM completion() model= open-mistral-nemo-2407; provider = mistral\n",
      "2024-09-05 23:00:38,977 : INFO : HTTP Request: POST http://195.242.24.252:8000/v1/chat/completions \"HTTP/1.1 500 Internal Server Error\"\n",
      "2024-09-05 23:00:38,979 : INFO : Retrying request to /chat/completions in 0.914831 seconds\n",
      "2024-09-05 23:00:59,978 : INFO : HTTP Request: POST http://195.242.24.252:8000/v1/chat/completions \"HTTP/1.1 500 Internal Server Error\"\n",
      "2024-09-05 23:00:59,981 : INFO : Retrying request to /chat/completions in 1.964403 seconds\n",
      "2024-09-05 23:01:10,995 : INFO : HTTP Request: POST http://195.242.24.252:8000/v1/chat/completions \"HTTP/1.1 200 OK\"\n",
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
      "2024-09-05 23:01:11,569 : INFO : Reworked solution result: \"WRONG ANSWER!!\\n\\n<expected>\\n'Case #1: 4\\nCase #2: 7\\nCase #3: 1\\nCase #4: -1\\nCase #5: 6\\nCase #6: -1\\nCase #7: 1000000002\\n'\\n</expected>\\n---\\n<got>\\n'Case #1: -1\\nCase #2: -1\\nCase #3: 0\\nCase #4: -1\\nCase #5: -1\\nCase #6: 0\\nCase #7: 0\\n'\\n</got>\"\n",
      "2024-09-05 23:01:11,570 : INFO : Drafting intial zero-shot solution\n",
      "\u001b[92m23:01:11 - LiteLLM:INFO\u001b[0m: utils.py:2975 - \n",
      "LiteLLM completion() model= open-mistral-nemo-2407; provider = mistral\n",
      "2024-09-05 23:01:11,574 : INFO : \n",
      "LiteLLM completion() model= open-mistral-nemo-2407; provider = mistral\n",
      "2024-09-05 23:01:18,229 : INFO : HTTP Request: POST http://195.242.24.252:8000/v1/chat/completions \"HTTP/1.1 200 OK\"\n",
      "\u001b[92m23:01:18 - LiteLLM:INFO\u001b[0m: utils.py:2975 - \n",
      "LiteLLM completion() model= open-mistral-nemo-2407; provider = mistral\n",
      "2024-09-05 23:01:18,233 : INFO : \n",
      "LiteLLM completion() model= open-mistral-nemo-2407; provider = mistral\n",
      "2024-09-05 23:01:31,603 : INFO : HTTP Request: POST http://195.242.24.252:8000/v1/chat/completions \"HTTP/1.1 200 OK\"\n",
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
      "2024-09-05 23:01:32,162 : INFO : Draft solution result: \"WRONG ANSWER!!\\n\\n<expected>\\n'Case #1: 4\\nCase #2: 7\\nCase #3: 1\\nCase #4: -1\\nCase #5: 6\\nCase #6: -1\\nCase #7: 1000000002\\n'\\n</expected>\\n---\\n<got>\\n'Case #1: -1\\nCase #2: 0.0\\nCase #3: 0\\nCase #4: 0\\nCase #5: 0.0\\nCase #6: -1.5\\nCase #7: -2\\n'\\n</got>\"\n",
      "2024-09-05 23:01:32,163 : INFO : Iterating on a RAG solution\n",
      "2024-09-05 23:01:32,951 : INFO : Generating RAG solution:\n",
      "2024-09-05 23:01:33,746 : INFO : Generating examplars:\n",
      "Batches: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 1/1 [00:01<00:00,  1.74s/it]\n",
      "Batches: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 2/2 [00:10<00:00,  5.10s/it]\n",
      "\u001b[92m23:01:45 - LiteLLM:INFO\u001b[0m: utils.py:2975 - \n",
      "LiteLLM completion() model= open-mistral-nemo-2407; provider = mistral\n",
      "2024-09-05 23:01:45,970 : INFO : \n",
      "LiteLLM completion() model= open-mistral-nemo-2407; provider = mistral\n",
      "\u001b[92m23:01:45 - LiteLLM:INFO\u001b[0m: utils.py:2975 - \n",
      "LiteLLM completion() model= open-mistral-nemo-2407; provider = mistral\n",
      "2024-09-05 23:01:45,972 : INFO : \n",
      "LiteLLM completion() model= open-mistral-nemo-2407; provider = mistral\n",
      "\u001b[92m23:01:45 - LiteLLM:INFO\u001b[0m: utils.py:2975 - \n",
      "LiteLLM completion() model= open-mistral-nemo-2407; provider = mistral\n",
      "2024-09-05 23:01:45,974 : INFO : \n",
      "LiteLLM completion() model= open-mistral-nemo-2407; provider = mistral\n",
      "\u001b[92m23:01:45 - LiteLLM:INFO\u001b[0m: utils.py:2975 - \n",
      "LiteLLM completion() model= open-mistral-nemo-2407; provider = mistral\n",
      "\u001b[92m23:01:45 - LiteLLM:INFO\u001b[0m: utils.py:2975 - \n",
      "LiteLLM completion() model= open-mistral-nemo-2407; provider = mistral\n",
      "2024-09-05 23:01:45,975 : INFO : \n",
      "LiteLLM completion() model= open-mistral-nemo-2407; provider = mistral\n",
      "2024-09-05 23:01:45,975 : INFO : \n",
      "LiteLLM completion() model= open-mistral-nemo-2407; provider = mistral\n",
      "2024-09-05 23:01:56,690 : INFO : HTTP Request: POST http://195.242.24.252:8000/v1/chat/completions \"HTTP/1.1 200 OK\"\n",
      "\u001b[92m23:01:56 - LiteLLM:INFO\u001b[0m: utils.py:2975 - \n",
      "LiteLLM completion() model= open-mistral-nemo-2407; provider = mistral\n",
      "2024-09-05 23:01:56,701 : INFO : \n",
      "LiteLLM completion() model= open-mistral-nemo-2407; provider = mistral\n",
      "2024-09-05 23:01:59,562 : INFO : HTTP Request: POST http://195.242.24.252:8000/v1/chat/completions \"HTTP/1.1 200 OK\"\n",
      "\u001b[92m23:01:59 - LiteLLM:INFO\u001b[0m: utils.py:2975 - \n",
      "LiteLLM completion() model= open-mistral-nemo-2407; provider = mistral\n",
      "2024-09-05 23:01:59,574 : INFO : \n",
      "LiteLLM completion() model= open-mistral-nemo-2407; provider = mistral\n",
      "2024-09-05 23:01:59,853 : INFO : HTTP Request: POST http://195.242.24.252:8000/v1/chat/completions \"HTTP/1.1 200 OK\"\n",
      "\u001b[92m23:01:59 - LiteLLM:INFO\u001b[0m: utils.py:2975 - \n",
      "LiteLLM completion() model= open-mistral-nemo-2407; provider = mistral\n",
      "2024-09-05 23:01:59,861 : INFO : \n",
      "LiteLLM completion() model= open-mistral-nemo-2407; provider = mistral\n",
      "2024-09-05 23:02:01,112 : INFO : HTTP Request: POST http://195.242.24.252:8000/v1/chat/completions \"HTTP/1.1 500 Internal Server Error\"\n",
      "2024-09-05 23:02:01,116 : INFO : HTTP Request: POST http://195.242.24.252:8000/v1/chat/completions \"HTTP/1.1 500 Internal Server Error\"\n",
      "2024-09-05 23:02:01,119 : INFO : HTTP Request: POST http://195.242.24.252:8000/v1/chat/completions \"HTTP/1.1 500 Internal Server Error\"\n",
      "2024-09-05 23:02:01,121 : INFO : HTTP Request: POST http://195.242.24.252:8000/v1/chat/completions \"HTTP/1.1 500 Internal Server Error\"\n",
      "2024-09-05 23:02:01,121 : INFO : HTTP Request: POST http://195.242.24.252:8000/v1/chat/completions \"HTTP/1.1 500 Internal Server Error\"\n",
      "2024-09-05 23:02:01,122 : INFO : Retrying request to /chat/completions in 0.860215 seconds\n",
      "2024-09-05 23:02:01,123 : INFO : Retrying request to /chat/completions in 0.989535 seconds\n",
      "2024-09-05 23:02:01,123 : INFO : Retrying request to /chat/completions in 0.816989 seconds\n",
      "2024-09-05 23:02:01,124 : INFO : Retrying request to /chat/completions in 0.801719 seconds\n",
      "2024-09-05 23:02:01,125 : INFO : Retrying request to /chat/completions in 0.977158 seconds\n",
      "2024-09-05 23:02:01,986 : INFO : Retrying request to /chat/completions in 1.666986 seconds\n",
      "2024-09-05 23:02:01,999 : INFO : Retrying request to /chat/completions in 1.678728 seconds\n",
      "2024-09-05 23:02:02,041 : INFO : Retrying request to /chat/completions in 1.640060 seconds\n",
      "2024-09-05 23:02:02,155 : INFO : Retrying request to /chat/completions in 1.933926 seconds\n",
      "2024-09-05 23:02:02,169 : INFO : Retrying request to /chat/completions in 1.764167 seconds\n",
      "\u001b[92m23:02:03 - LiteLLM:INFO\u001b[0m: utils.py:2975 - \n",
      "LiteLLM completion() model= open-mistral-nemo-2407; provider = mistral\n",
      "2024-09-05 23:02:03,763 : INFO : \n",
      "LiteLLM completion() model= open-mistral-nemo-2407; provider = mistral\n",
      "\u001b[92m23:02:03 - LiteLLM:INFO\u001b[0m: utils.py:2975 - \n",
      "LiteLLM completion() model= open-mistral-nemo-2407; provider = mistral\n",
      "2024-09-05 23:02:03,777 : INFO : \n",
      "LiteLLM completion() model= open-mistral-nemo-2407; provider = mistral\n",
      "\u001b[92m23:02:03 - LiteLLM:INFO\u001b[0m: utils.py:2975 - \n",
      "LiteLLM completion() model= open-mistral-nemo-2407; provider = mistral\n",
      "2024-09-05 23:02:03,777 : INFO : \n",
      "LiteLLM completion() model= open-mistral-nemo-2407; provider = mistral\n",
      "2024-09-05 23:02:03,833 : INFO : Retrying request to /chat/completions in 0.827259 seconds\n",
      "2024-09-05 23:02:03,837 : INFO : Retrying request to /chat/completions in 0.958903 seconds\n",
      "2024-09-05 23:02:03,838 : INFO : Retrying request to /chat/completions in 0.766889 seconds\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "\u001b[1;31mGive Feedback / Get Help: https://github.com/BerriAI/litellm/issues/new\u001b[0m\n",
      "LiteLLM.Info: If you need to debug this error, use `litellm.set_verbose=True'.\n",
      "\n",
      "\n",
      "\u001b[1;31mGive Feedback / Get Help: https://github.com/BerriAI/litellm/issues/new\u001b[0m\n",
      "LiteLLM.Info: If you need to debug this error, use `litellm.set_verbose=True'.\n",
      "\n",
      "\n",
      "\u001b[1;31mGive Feedback / Get Help: https://github.com/BerriAI/litellm/issues/new\u001b[0m\n",
      "LiteLLM.Info: If you need to debug this error, use `litellm.set_verbose=True'.\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[92m23:02:03 - LiteLLM:INFO\u001b[0m: utils.py:2975 - \n",
      "LiteLLM completion() model= open-mistral-nemo-2407; provider = mistral\n",
      "2024-09-05 23:02:03,997 : INFO : \n",
      "LiteLLM completion() model= open-mistral-nemo-2407; provider = mistral\n",
      "2024-09-05 23:02:04,056 : INFO : Retrying request to /chat/completions in 0.797548 seconds\n",
      "\u001b[92m23:02:04 - LiteLLM:INFO\u001b[0m: utils.py:2975 - \n",
      "LiteLLM completion() model= open-mistral-nemo-2407; provider = mistral\n",
      "2024-09-05 23:02:04,154 : INFO : \n",
      "LiteLLM completion() model= open-mistral-nemo-2407; provider = mistral\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "\u001b[1;31mGive Feedback / Get Help: https://github.com/BerriAI/litellm/issues/new\u001b[0m\n",
      "LiteLLM.Info: If you need to debug this error, use `litellm.set_verbose=True'.\n",
      "\n",
      "\n",
      "\u001b[1;31mGive Feedback / Get Help: https://github.com/BerriAI/litellm/issues/new\u001b[0m\n",
      "LiteLLM.Info: If you need to debug this error, use `litellm.set_verbose=True'.\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2024-09-05 23:02:04,214 : INFO : Retrying request to /chat/completions in 0.783185 seconds\n",
      "2024-09-05 23:02:04,673 : INFO : Retrying request to /chat/completions in 1.674434 seconds\n",
      "2024-09-05 23:02:04,760 : INFO : Retrying request to /chat/completions in 1.651550 seconds\n",
      "2024-09-05 23:02:04,864 : INFO : Retrying request to /chat/completions in 1.774962 seconds\n",
      "2024-09-05 23:02:04,910 : INFO : Retrying request to /chat/completions in 1.911112 seconds\n",
      "2024-09-05 23:02:05,052 : INFO : Retrying request to /chat/completions in 1.948356 seconds\n",
      "\u001b[92m23:02:06 - LiteLLM:INFO\u001b[0m: utils.py:2975 - \n",
      "LiteLLM completion() model= open-mistral-nemo-2407; provider = mistral\n",
      "2024-09-05 23:02:06,426 : INFO : \n",
      "LiteLLM completion() model= open-mistral-nemo-2407; provider = mistral\n",
      "\u001b[92m23:02:06 - LiteLLM:INFO\u001b[0m: utils.py:2975 - \n",
      "LiteLLM completion() model= open-mistral-nemo-2407; provider = mistral\n",
      "2024-09-05 23:02:06,490 : INFO : Retrying request to /chat/completions in 0.894631 seconds\n",
      "2024-09-05 23:02:06,489 : INFO : \n",
      "LiteLLM completion() model= open-mistral-nemo-2407; provider = mistral\n",
      "2024-09-05 23:02:06,545 : INFO : Retrying request to /chat/completions in 0.895823 seconds\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "\u001b[1;31mGive Feedback / Get Help: https://github.com/BerriAI/litellm/issues/new\u001b[0m\n",
      "LiteLLM.Info: If you need to debug this error, use `litellm.set_verbose=True'.\n",
      "\n",
      "\n",
      "\u001b[1;31mGive Feedback / Get Help: https://github.com/BerriAI/litellm/issues/new\u001b[0m\n",
      "LiteLLM.Info: If you need to debug this error, use `litellm.set_verbose=True'.\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[92m23:02:06 - LiteLLM:INFO\u001b[0m: utils.py:2975 - \n",
      "LiteLLM completion() model= open-mistral-nemo-2407; provider = mistral\n",
      "2024-09-05 23:02:06,711 : INFO : \n",
      "LiteLLM completion() model= open-mistral-nemo-2407; provider = mistral\n",
      "2024-09-05 23:02:06,768 : INFO : Retrying request to /chat/completions in 0.903880 seconds\n",
      "\u001b[92m23:02:06 - LiteLLM:INFO\u001b[0m: utils.py:2975 - \n",
      "LiteLLM completion() model= open-mistral-nemo-2407; provider = mistral\n",
      "2024-09-05 23:02:06,893 : INFO : \n",
      "LiteLLM completion() model= open-mistral-nemo-2407; provider = mistral\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "\u001b[1;31mGive Feedback / Get Help: https://github.com/BerriAI/litellm/issues/new\u001b[0m\n",
      "LiteLLM.Info: If you need to debug this error, use `litellm.set_verbose=True'.\n",
      "\n",
      "\n",
      "\u001b[1;31mGive Feedback / Get Help: https://github.com/BerriAI/litellm/issues/new\u001b[0m\n",
      "LiteLLM.Info: If you need to debug this error, use `litellm.set_verbose=True'.\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2024-09-05 23:02:06,955 : INFO : Retrying request to /chat/completions in 0.967968 seconds\n",
      "\u001b[92m23:02:07 - LiteLLM:INFO\u001b[0m: utils.py:2975 - \n",
      "LiteLLM completion() model= open-mistral-nemo-2407; provider = mistral\n",
      "2024-09-05 23:02:07,070 : INFO : \n",
      "LiteLLM completion() model= open-mistral-nemo-2407; provider = mistral\n",
      "2024-09-05 23:02:07,129 : INFO : Retrying request to /chat/completions in 0.787977 seconds\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "\u001b[1;31mGive Feedback / Get Help: https://github.com/BerriAI/litellm/issues/new\u001b[0m\n",
      "LiteLLM.Info: If you need to debug this error, use `litellm.set_verbose=True'.\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2024-09-05 23:02:07,456 : INFO : Retrying request to /chat/completions in 1.806692 seconds\n",
      "2024-09-05 23:02:07,509 : INFO : Retrying request to /chat/completions in 1.858282 seconds\n",
      "2024-09-05 23:02:07,727 : INFO : Retrying request to /chat/completions in 1.787996 seconds\n",
      "2024-09-05 23:02:07,978 : INFO : Retrying request to /chat/completions in 1.731372 seconds\n",
      "2024-09-05 23:02:07,980 : INFO : Retrying request to /chat/completions in 1.618331 seconds\n",
      "2024-09-05 23:02:17,239 : INFO : HTTP Request: POST http://195.242.24.252:8000/v1/chat/completions \"HTTP/1.1 200 OK\"\n",
      "2024-09-05 23:02:18,603 : INFO : HTTP Request: POST http://195.242.24.252:8000/v1/chat/completions \"HTTP/1.1 200 OK\"\n",
      "2024-09-05 23:02:22,523 : INFO : HTTP Request: POST http://195.242.24.252:8000/v1/chat/completions \"HTTP/1.1 200 OK\"\n",
      "\u001b[92m23:02:22 - LiteLLM:INFO\u001b[0m: utils.py:2975 - \n",
      "LiteLLM completion() model= open-mistral-nemo-2407; provider = mistral\n",
      "2024-09-05 23:02:22,532 : INFO : \n",
      "LiteLLM completion() model= open-mistral-nemo-2407; provider = mistral\n",
      "2024-09-05 23:02:24,243 : INFO : HTTP Request: POST http://195.242.24.252:8000/v1/chat/completions \"HTTP/1.1 200 OK\"\n",
      "2024-09-05 23:02:29,699 : INFO : HTTP Request: POST http://195.242.24.252:8000/v1/chat/completions \"HTTP/1.1 500 Internal Server Error\"\n",
      "2024-09-05 23:02:29,711 : ERROR : Failed after retries: <bound method Future.exception of <Future at 0x3388790d0 state=finished raised APIError>>\n",
      "Traceback (most recent call last):\n",
      "  File \"/Users/tcapelle/miniforge3/envs/weave/lib/python3.11/site-packages/litellm/llms/openai.py\", line 1089, in acompletion\n",
      "    headers, response = await self.make_openai_chat_completion_request(\n",
      "                        ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/Users/tcapelle/miniforge3/envs/weave/lib/python3.11/site-packages/litellm/llms/openai.py\", line 793, in make_openai_chat_completion_request\n",
      "    raise e\n",
      "  File \"/Users/tcapelle/miniforge3/envs/weave/lib/python3.11/site-packages/litellm/llms/openai.py\", line 781, in make_openai_chat_completion_request\n",
      "    await openai_aclient.chat.completions.with_raw_response.create(\n",
      "  File \"/Users/tcapelle/miniforge3/envs/weave/lib/python3.11/site-packages/openai/_legacy_response.py\", line 367, in wrapped\n",
      "    return cast(LegacyAPIResponse[R], await func(*args, **kwargs))\n",
      "                                      ^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/Users/tcapelle/miniforge3/envs/weave/lib/python3.11/site-packages/weave/trace/op.py\", line 333, in wrapper\n",
      "    res, _ = await _execute_call(wrapper, call, *args, **kwargs)  # type: ignore\n",
      "             ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/Users/tcapelle/miniforge3/envs/weave/lib/python3.11/site-packages/weave/trace/op.py\", line 213, in _call_async\n",
      "    return handle_exception(e)\n",
      "           ^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/Users/tcapelle/miniforge3/envs/weave/lib/python3.11/site-packages/weave/trace/op.py\", line 211, in _call_async\n",
      "    res = await func(*args, **kwargs)\n",
      "          ^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/Users/tcapelle/miniforge3/envs/weave/lib/python3.11/site-packages/weave/integrations/openai/openai_sdk.py\", line 288, in _wrapper\n",
      "    return await fn(*args, **kwargs)\n",
      "           ^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/Users/tcapelle/miniforge3/envs/weave/lib/python3.11/site-packages/openai/resources/chat/completions.py\", line 1339, in create\n",
      "    return await self._post(\n",
      "           ^^^^^^^^^^^^^^^^^\n",
      "  File \"/Users/tcapelle/miniforge3/envs/weave/lib/python3.11/site-packages/openai/_base_client.py\", line 1816, in post\n",
      "    return await self.request(cast_to, opts, stream=stream, stream_cls=stream_cls)\n",
      "           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/Users/tcapelle/miniforge3/envs/weave/lib/python3.11/site-packages/openai/_base_client.py\", line 1510, in request\n",
      "    return await self._request(\n",
      "           ^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/Users/tcapelle/miniforge3/envs/weave/lib/python3.11/site-packages/openai/_base_client.py\", line 1573, in _request\n",
      "    return await self._retry_request(\n",
      "           ^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/Users/tcapelle/miniforge3/envs/weave/lib/python3.11/site-packages/openai/_base_client.py\", line 1643, in _retry_request\n",
      "    return await self._request(\n",
      "           ^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/Users/tcapelle/miniforge3/envs/weave/lib/python3.11/site-packages/openai/_base_client.py\", line 1573, in _request\n",
      "    return await self._retry_request(\n",
      "           ^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/Users/tcapelle/miniforge3/envs/weave/lib/python3.11/site-packages/openai/_base_client.py\", line 1643, in _retry_request\n",
      "    return await self._request(\n",
      "           ^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/Users/tcapelle/miniforge3/envs/weave/lib/python3.11/site-packages/openai/_base_client.py\", line 1611, in _request\n",
      "    raise self._make_status_error_from_response(err.response) from None\n",
      "openai.InternalServerError: Error code: 500 - {'error': 'Internal server error'}\n",
      "\n",
      "During handling of the above exception, another exception occurred:\n",
      "\n",
      "Traceback (most recent call last):\n",
      "  File \"/Users/tcapelle/miniforge3/envs/weave/lib/python3.11/site-packages/litellm/main.py\", line 427, in acompletion\n",
      "    response = await init_response\n",
      "               ^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/Users/tcapelle/miniforge3/envs/weave/lib/python3.11/site-packages/litellm/llms/openai.py\", line 1136, in acompletion\n",
      "    raise OpenAIError(\n",
      "litellm.llms.openai.OpenAIError: Error code: 500 - {'error': 'Internal server error'}\n",
      "\n",
      "During handling of the above exception, another exception occurred:\n",
      "\n",
      "Traceback (most recent call last):\n",
      "  File \"/Users/tcapelle/miniforge3/envs/weave/lib/python3.11/site-packages/instructor/retry.py\", line 239, in retry_async\n",
      "    response: ChatCompletion = await func(*args, **kwargs)\n",
      "                               ^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/Users/tcapelle/miniforge3/envs/weave/lib/python3.11/site-packages/weave/trace/op.py\", line 333, in wrapper\n",
      "    res, _ = await _execute_call(wrapper, call, *args, **kwargs)  # type: ignore\n",
      "             ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/Users/tcapelle/miniforge3/envs/weave/lib/python3.11/site-packages/weave/trace/op.py\", line 213, in _call_async\n",
      "    return handle_exception(e)\n",
      "           ^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/Users/tcapelle/miniforge3/envs/weave/lib/python3.11/site-packages/weave/trace/op.py\", line 211, in _call_async\n",
      "    res = await func(*args, **kwargs)\n",
      "          ^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/Users/tcapelle/miniforge3/envs/weave/lib/python3.11/site-packages/litellm/utils.py\", line 1593, in wrapper_async\n",
      "    raise e\n",
      "  File \"/Users/tcapelle/miniforge3/envs/weave/lib/python3.11/site-packages/litellm/utils.py\", line 1413, in wrapper_async\n",
      "    result = await original_function(*args, **kwargs)\n",
      "             ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/Users/tcapelle/miniforge3/envs/weave/lib/python3.11/site-packages/litellm/main.py\", line 449, in acompletion\n",
      "    raise exception_type(\n",
      "          ^^^^^^^^^^^^^^^\n",
      "  File \"/Users/tcapelle/miniforge3/envs/weave/lib/python3.11/site-packages/litellm/utils.py\", line 8187, in exception_type\n",
      "    raise e\n",
      "  File \"/Users/tcapelle/miniforge3/envs/weave/lib/python3.11/site-packages/litellm/utils.py\", line 6581, in exception_type\n",
      "    raise APIError(\n",
      "litellm.exceptions.APIError: litellm.APIError: APIError: MistralException - Error code: 500 - {'error': 'Internal server error'}\n",
      "\n",
      "The above exception was the direct cause of the following exception:\n",
      "\n",
      "Traceback (most recent call last):\n",
      "  File \"/Users/tcapelle/miniforge3/envs/weave/lib/python3.11/site-packages/instructor/retry.py\", line 235, in retry_async\n",
      "    async for attempt in max_retries:\n",
      "  File \"/Users/tcapelle/miniforge3/envs/weave/lib/python3.11/site-packages/tenacity/asyncio/__init__.py\", line 166, in __anext__\n",
      "    do = await self.iter(retry_state=self._retry_state)\n",
      "         ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/Users/tcapelle/miniforge3/envs/weave/lib/python3.11/site-packages/tenacity/asyncio/__init__.py\", line 153, in iter\n",
      "    result = await action(retry_state)\n",
      "             ^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/Users/tcapelle/miniforge3/envs/weave/lib/python3.11/site-packages/tenacity/_utils.py\", line 99, in inner\n",
      "    return call(*args, **kwargs)\n",
      "           ^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/Users/tcapelle/miniforge3/envs/weave/lib/python3.11/site-packages/tenacity/__init__.py\", line 419, in exc_check\n",
      "    raise retry_exc from fut.exception()\n",
      "tenacity.RetryError: RetryError[<Future at 0x3388790d0 state=finished raised APIError>]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "\u001b[1;31mGive Feedback / Get Help: https://github.com/BerriAI/litellm/issues/new\u001b[0m\n",
      "LiteLLM.Info: If you need to debug this error, use `litellm.set_verbose=True'.\n",
      "\n"
     ]
    },
    {
     "ename": "InstructorRetryException",
     "evalue": "RetryError[<Future at 0x3388790d0 state=finished raised APIError>]",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mInternalServerError\u001b[0m                       Traceback (most recent call last)",
      "File \u001b[0;32m~/miniforge3/envs/weave/lib/python3.11/site-packages/litellm/llms/openai.py:1089\u001b[0m, in \u001b[0;36mOpenAIChatCompletion.acompletion\u001b[0;34m(self, data, model_response, logging_obj, timeout, api_key, api_base, organization, client, max_retries, headers, drop_params)\u001b[0m\n\u001b[1;32m   1076\u001b[0m logging_obj\u001b[38;5;241m.\u001b[39mpre_call(\n\u001b[1;32m   1077\u001b[0m     \u001b[38;5;28minput\u001b[39m\u001b[38;5;241m=\u001b[39mdata[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mmessages\u001b[39m\u001b[38;5;124m\"\u001b[39m],\n\u001b[1;32m   1078\u001b[0m     api_key\u001b[38;5;241m=\u001b[39mopenai_aclient\u001b[38;5;241m.\u001b[39mapi_key,\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m   1086\u001b[0m     },\n\u001b[1;32m   1087\u001b[0m )\n\u001b[0;32m-> 1089\u001b[0m headers, response \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mawait\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mmake_openai_chat_completion_request(\n\u001b[1;32m   1090\u001b[0m     openai_aclient\u001b[38;5;241m=\u001b[39mopenai_aclient, data\u001b[38;5;241m=\u001b[39mdata, timeout\u001b[38;5;241m=\u001b[39mtimeout\n\u001b[1;32m   1091\u001b[0m )\n\u001b[1;32m   1092\u001b[0m stringified_response \u001b[38;5;241m=\u001b[39m response\u001b[38;5;241m.\u001b[39mmodel_dump()\n",
      "File \u001b[0;32m~/miniforge3/envs/weave/lib/python3.11/site-packages/litellm/llms/openai.py:793\u001b[0m, in \u001b[0;36mOpenAIChatCompletion.make_openai_chat_completion_request\u001b[0;34m(self, openai_aclient, data, timeout)\u001b[0m\n\u001b[1;32m    792\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mException\u001b[39;00m \u001b[38;5;28;01mas\u001b[39;00m e:\n\u001b[0;32m--> 793\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m e\n",
      "File \u001b[0;32m~/miniforge3/envs/weave/lib/python3.11/site-packages/litellm/llms/openai.py:781\u001b[0m, in \u001b[0;36mOpenAIChatCompletion.make_openai_chat_completion_request\u001b[0;34m(self, openai_aclient, data, timeout)\u001b[0m\n\u001b[1;32m    779\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[1;32m    780\u001b[0m     raw_response \u001b[38;5;241m=\u001b[39m (\n\u001b[0;32m--> 781\u001b[0m         \u001b[38;5;28;01mawait\u001b[39;00m openai_aclient\u001b[38;5;241m.\u001b[39mchat\u001b[38;5;241m.\u001b[39mcompletions\u001b[38;5;241m.\u001b[39mwith_raw_response\u001b[38;5;241m.\u001b[39mcreate(\n\u001b[1;32m    782\u001b[0m             \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mdata, timeout\u001b[38;5;241m=\u001b[39mtimeout\n\u001b[1;32m    783\u001b[0m         )\n\u001b[1;32m    784\u001b[0m     )\n\u001b[1;32m    786\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mhasattr\u001b[39m(raw_response, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mheaders\u001b[39m\u001b[38;5;124m\"\u001b[39m):\n",
      "File \u001b[0;32m~/miniforge3/envs/weave/lib/python3.11/site-packages/openai/_legacy_response.py:367\u001b[0m, in \u001b[0;36masync_to_raw_response_wrapper.<locals>.wrapped\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m    365\u001b[0m kwargs[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mextra_headers\u001b[39m\u001b[38;5;124m\"\u001b[39m] \u001b[38;5;241m=\u001b[39m extra_headers\n\u001b[0;32m--> 367\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m cast(LegacyAPIResponse[R], \u001b[38;5;28;01mawait\u001b[39;00m func(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs))\n",
      "File \u001b[0;32m~/miniforge3/envs/weave/lib/python3.11/site-packages/weave/trace/op.py:333\u001b[0m, in \u001b[0;36mop.<locals>.op_deco.<locals>.create_wrapper.<locals>.wrapper\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m    332\u001b[0m call \u001b[38;5;241m=\u001b[39m _create_call(wrapper, \u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)  \u001b[38;5;66;03m# type: ignore\u001b[39;00m\n\u001b[0;32m--> 333\u001b[0m res, _ \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mawait\u001b[39;00m _execute_call(wrapper, call, \u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)  \u001b[38;5;66;03m# type: ignore\u001b[39;00m\n\u001b[1;32m    334\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m res\n",
      "File \u001b[0;32m~/miniforge3/envs/weave/lib/python3.11/site-packages/weave/trace/op.py:213\u001b[0m, in \u001b[0;36m_execute_call.<locals>._call_async\u001b[0;34m()\u001b[0m\n\u001b[1;32m    212\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mException\u001b[39;00m \u001b[38;5;28;01mas\u001b[39;00m e:\n\u001b[0;32m--> 213\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mhandle_exception\u001b[49m\u001b[43m(\u001b[49m\u001b[43me\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    214\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n",
      "File \u001b[0;32m~/miniforge3/envs/weave/lib/python3.11/site-packages/weave/trace/op.py:211\u001b[0m, in \u001b[0;36m_execute_call.<locals>._call_async\u001b[0;34m()\u001b[0m\n\u001b[1;32m    210\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[0;32m--> 211\u001b[0m     res \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mawait\u001b[39;00m func(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n\u001b[1;32m    212\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mException\u001b[39;00m \u001b[38;5;28;01mas\u001b[39;00m e:\n",
      "File \u001b[0;32m~/miniforge3/envs/weave/lib/python3.11/site-packages/weave/integrations/openai/openai_sdk.py:288\u001b[0m, in \u001b[0;36mcreate_wrapper_async.<locals>.wrapper.<locals>._add_stream_options.<locals>._wrapper\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m    287\u001b[0m     kwargs[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mstream_options\u001b[39m\u001b[38;5;124m\"\u001b[39m] \u001b[38;5;241m=\u001b[39m {\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124minclude_usage\u001b[39m\u001b[38;5;124m\"\u001b[39m: \u001b[38;5;28;01mTrue\u001b[39;00m}\n\u001b[0;32m--> 288\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;01mawait\u001b[39;00m fn(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n",
      "File \u001b[0;32m~/miniforge3/envs/weave/lib/python3.11/site-packages/openai/resources/chat/completions.py:1339\u001b[0m, in \u001b[0;36mAsyncCompletions.create\u001b[0;34m(self, messages, model, frequency_penalty, function_call, functions, logit_bias, logprobs, max_tokens, n, parallel_tool_calls, presence_penalty, response_format, seed, service_tier, stop, stream, stream_options, temperature, tool_choice, tools, top_logprobs, top_p, user, extra_headers, extra_query, extra_body, timeout)\u001b[0m\n\u001b[1;32m   1338\u001b[0m validate_response_format(response_format)\n\u001b[0;32m-> 1339\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;01mawait\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_post(\n\u001b[1;32m   1340\u001b[0m     \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m/chat/completions\u001b[39m\u001b[38;5;124m\"\u001b[39m,\n\u001b[1;32m   1341\u001b[0m     body\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mawait\u001b[39;00m async_maybe_transform(\n\u001b[1;32m   1342\u001b[0m         {\n\u001b[1;32m   1343\u001b[0m             \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mmessages\u001b[39m\u001b[38;5;124m\"\u001b[39m: messages,\n\u001b[1;32m   1344\u001b[0m             \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mmodel\u001b[39m\u001b[38;5;124m\"\u001b[39m: model,\n\u001b[1;32m   1345\u001b[0m             \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mfrequency_penalty\u001b[39m\u001b[38;5;124m\"\u001b[39m: frequency_penalty,\n\u001b[1;32m   1346\u001b[0m             \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mfunction_call\u001b[39m\u001b[38;5;124m\"\u001b[39m: function_call,\n\u001b[1;32m   1347\u001b[0m             \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mfunctions\u001b[39m\u001b[38;5;124m\"\u001b[39m: functions,\n\u001b[1;32m   1348\u001b[0m             \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mlogit_bias\u001b[39m\u001b[38;5;124m\"\u001b[39m: logit_bias,\n\u001b[1;32m   1349\u001b[0m             \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mlogprobs\u001b[39m\u001b[38;5;124m\"\u001b[39m: logprobs,\n\u001b[1;32m   1350\u001b[0m             \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mmax_tokens\u001b[39m\u001b[38;5;124m\"\u001b[39m: max_tokens,\n\u001b[1;32m   1351\u001b[0m             \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mn\u001b[39m\u001b[38;5;124m\"\u001b[39m: n,\n\u001b[1;32m   1352\u001b[0m             \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mparallel_tool_calls\u001b[39m\u001b[38;5;124m\"\u001b[39m: parallel_tool_calls,\n\u001b[1;32m   1353\u001b[0m             \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mpresence_penalty\u001b[39m\u001b[38;5;124m\"\u001b[39m: presence_penalty,\n\u001b[1;32m   1354\u001b[0m             \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mresponse_format\u001b[39m\u001b[38;5;124m\"\u001b[39m: response_format,\n\u001b[1;32m   1355\u001b[0m             \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mseed\u001b[39m\u001b[38;5;124m\"\u001b[39m: seed,\n\u001b[1;32m   1356\u001b[0m             \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mservice_tier\u001b[39m\u001b[38;5;124m\"\u001b[39m: service_tier,\n\u001b[1;32m   1357\u001b[0m             \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mstop\u001b[39m\u001b[38;5;124m\"\u001b[39m: stop,\n\u001b[1;32m   1358\u001b[0m             \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mstream\u001b[39m\u001b[38;5;124m\"\u001b[39m: stream,\n\u001b[1;32m   1359\u001b[0m             \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mstream_options\u001b[39m\u001b[38;5;124m\"\u001b[39m: stream_options,\n\u001b[1;32m   1360\u001b[0m             \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mtemperature\u001b[39m\u001b[38;5;124m\"\u001b[39m: temperature,\n\u001b[1;32m   1361\u001b[0m             \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mtool_choice\u001b[39m\u001b[38;5;124m\"\u001b[39m: tool_choice,\n\u001b[1;32m   1362\u001b[0m             \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mtools\u001b[39m\u001b[38;5;124m\"\u001b[39m: tools,\n\u001b[1;32m   1363\u001b[0m             \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mtop_logprobs\u001b[39m\u001b[38;5;124m\"\u001b[39m: top_logprobs,\n\u001b[1;32m   1364\u001b[0m             \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mtop_p\u001b[39m\u001b[38;5;124m\"\u001b[39m: top_p,\n\u001b[1;32m   1365\u001b[0m             \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124muser\u001b[39m\u001b[38;5;124m\"\u001b[39m: user,\n\u001b[1;32m   1366\u001b[0m         },\n\u001b[1;32m   1367\u001b[0m         completion_create_params\u001b[38;5;241m.\u001b[39mCompletionCreateParams,\n\u001b[1;32m   1368\u001b[0m     ),\n\u001b[1;32m   1369\u001b[0m     options\u001b[38;5;241m=\u001b[39mmake_request_options(\n\u001b[1;32m   1370\u001b[0m         extra_headers\u001b[38;5;241m=\u001b[39mextra_headers, extra_query\u001b[38;5;241m=\u001b[39mextra_query, extra_body\u001b[38;5;241m=\u001b[39mextra_body, timeout\u001b[38;5;241m=\u001b[39mtimeout\n\u001b[1;32m   1371\u001b[0m     ),\n\u001b[1;32m   1372\u001b[0m     cast_to\u001b[38;5;241m=\u001b[39mChatCompletion,\n\u001b[1;32m   1373\u001b[0m     stream\u001b[38;5;241m=\u001b[39mstream \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28;01mFalse\u001b[39;00m,\n\u001b[1;32m   1374\u001b[0m     stream_cls\u001b[38;5;241m=\u001b[39mAsyncStream[ChatCompletionChunk],\n\u001b[1;32m   1375\u001b[0m )\n",
      "File \u001b[0;32m~/miniforge3/envs/weave/lib/python3.11/site-packages/openai/_base_client.py:1816\u001b[0m, in \u001b[0;36mAsyncAPIClient.post\u001b[0;34m(self, path, cast_to, body, files, options, stream, stream_cls)\u001b[0m\n\u001b[1;32m   1813\u001b[0m opts \u001b[38;5;241m=\u001b[39m FinalRequestOptions\u001b[38;5;241m.\u001b[39mconstruct(\n\u001b[1;32m   1814\u001b[0m     method\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mpost\u001b[39m\u001b[38;5;124m\"\u001b[39m, url\u001b[38;5;241m=\u001b[39mpath, json_data\u001b[38;5;241m=\u001b[39mbody, files\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mawait\u001b[39;00m async_to_httpx_files(files), \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39moptions\n\u001b[1;32m   1815\u001b[0m )\n\u001b[0;32m-> 1816\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;01mawait\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mrequest(cast_to, opts, stream\u001b[38;5;241m=\u001b[39mstream, stream_cls\u001b[38;5;241m=\u001b[39mstream_cls)\n",
      "File \u001b[0;32m~/miniforge3/envs/weave/lib/python3.11/site-packages/openai/_base_client.py:1510\u001b[0m, in \u001b[0;36mAsyncAPIClient.request\u001b[0;34m(self, cast_to, options, stream, stream_cls, remaining_retries)\u001b[0m\n\u001b[1;32m   1501\u001b[0m \u001b[38;5;28;01masync\u001b[39;00m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mrequest\u001b[39m(\n\u001b[1;32m   1502\u001b[0m     \u001b[38;5;28mself\u001b[39m,\n\u001b[1;32m   1503\u001b[0m     cast_to: Type[ResponseT],\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m   1508\u001b[0m     remaining_retries: Optional[\u001b[38;5;28mint\u001b[39m] \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m,\n\u001b[1;32m   1509\u001b[0m ) \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m>\u001b[39m ResponseT \u001b[38;5;241m|\u001b[39m _AsyncStreamT:\n\u001b[0;32m-> 1510\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;01mawait\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_request(\n\u001b[1;32m   1511\u001b[0m         cast_to\u001b[38;5;241m=\u001b[39mcast_to,\n\u001b[1;32m   1512\u001b[0m         options\u001b[38;5;241m=\u001b[39moptions,\n\u001b[1;32m   1513\u001b[0m         stream\u001b[38;5;241m=\u001b[39mstream,\n\u001b[1;32m   1514\u001b[0m         stream_cls\u001b[38;5;241m=\u001b[39mstream_cls,\n\u001b[1;32m   1515\u001b[0m         remaining_retries\u001b[38;5;241m=\u001b[39mremaining_retries,\n\u001b[1;32m   1516\u001b[0m     )\n",
      "File \u001b[0;32m~/miniforge3/envs/weave/lib/python3.11/site-packages/openai/_base_client.py:1573\u001b[0m, in \u001b[0;36mAsyncAPIClient._request\u001b[0;34m(self, cast_to, options, stream, stream_cls, remaining_retries)\u001b[0m\n\u001b[1;32m   1572\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m retries \u001b[38;5;241m>\u001b[39m \u001b[38;5;241m0\u001b[39m:\n\u001b[0;32m-> 1573\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;01mawait\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_retry_request(\n\u001b[1;32m   1574\u001b[0m         input_options,\n\u001b[1;32m   1575\u001b[0m         cast_to,\n\u001b[1;32m   1576\u001b[0m         retries,\n\u001b[1;32m   1577\u001b[0m         stream\u001b[38;5;241m=\u001b[39mstream,\n\u001b[1;32m   1578\u001b[0m         stream_cls\u001b[38;5;241m=\u001b[39mstream_cls,\n\u001b[1;32m   1579\u001b[0m         response_headers\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mNone\u001b[39;00m,\n\u001b[1;32m   1580\u001b[0m     )\n\u001b[1;32m   1582\u001b[0m log\u001b[38;5;241m.\u001b[39mdebug(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mRaising connection error\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n",
      "File \u001b[0;32m~/miniforge3/envs/weave/lib/python3.11/site-packages/openai/_base_client.py:1643\u001b[0m, in \u001b[0;36mAsyncAPIClient._retry_request\u001b[0;34m(self, options, cast_to, remaining_retries, response_headers, stream, stream_cls)\u001b[0m\n\u001b[1;32m   1641\u001b[0m \u001b[38;5;28;01mawait\u001b[39;00m anyio\u001b[38;5;241m.\u001b[39msleep(timeout)\n\u001b[0;32m-> 1643\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;01mawait\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_request(\n\u001b[1;32m   1644\u001b[0m     options\u001b[38;5;241m=\u001b[39moptions,\n\u001b[1;32m   1645\u001b[0m     cast_to\u001b[38;5;241m=\u001b[39mcast_to,\n\u001b[1;32m   1646\u001b[0m     remaining_retries\u001b[38;5;241m=\u001b[39mremaining,\n\u001b[1;32m   1647\u001b[0m     stream\u001b[38;5;241m=\u001b[39mstream,\n\u001b[1;32m   1648\u001b[0m     stream_cls\u001b[38;5;241m=\u001b[39mstream_cls,\n\u001b[1;32m   1649\u001b[0m )\n",
      "File \u001b[0;32m~/miniforge3/envs/weave/lib/python3.11/site-packages/openai/_base_client.py:1573\u001b[0m, in \u001b[0;36mAsyncAPIClient._request\u001b[0;34m(self, cast_to, options, stream, stream_cls, remaining_retries)\u001b[0m\n\u001b[1;32m   1572\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m retries \u001b[38;5;241m>\u001b[39m \u001b[38;5;241m0\u001b[39m:\n\u001b[0;32m-> 1573\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;01mawait\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_retry_request(\n\u001b[1;32m   1574\u001b[0m         input_options,\n\u001b[1;32m   1575\u001b[0m         cast_to,\n\u001b[1;32m   1576\u001b[0m         retries,\n\u001b[1;32m   1577\u001b[0m         stream\u001b[38;5;241m=\u001b[39mstream,\n\u001b[1;32m   1578\u001b[0m         stream_cls\u001b[38;5;241m=\u001b[39mstream_cls,\n\u001b[1;32m   1579\u001b[0m         response_headers\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mNone\u001b[39;00m,\n\u001b[1;32m   1580\u001b[0m     )\n\u001b[1;32m   1582\u001b[0m log\u001b[38;5;241m.\u001b[39mdebug(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mRaising connection error\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n",
      "File \u001b[0;32m~/miniforge3/envs/weave/lib/python3.11/site-packages/openai/_base_client.py:1643\u001b[0m, in \u001b[0;36mAsyncAPIClient._retry_request\u001b[0;34m(self, options, cast_to, remaining_retries, response_headers, stream, stream_cls)\u001b[0m\n\u001b[1;32m   1641\u001b[0m \u001b[38;5;28;01mawait\u001b[39;00m anyio\u001b[38;5;241m.\u001b[39msleep(timeout)\n\u001b[0;32m-> 1643\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;01mawait\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_request(\n\u001b[1;32m   1644\u001b[0m     options\u001b[38;5;241m=\u001b[39moptions,\n\u001b[1;32m   1645\u001b[0m     cast_to\u001b[38;5;241m=\u001b[39mcast_to,\n\u001b[1;32m   1646\u001b[0m     remaining_retries\u001b[38;5;241m=\u001b[39mremaining,\n\u001b[1;32m   1647\u001b[0m     stream\u001b[38;5;241m=\u001b[39mstream,\n\u001b[1;32m   1648\u001b[0m     stream_cls\u001b[38;5;241m=\u001b[39mstream_cls,\n\u001b[1;32m   1649\u001b[0m )\n",
      "File \u001b[0;32m~/miniforge3/envs/weave/lib/python3.11/site-packages/openai/_base_client.py:1611\u001b[0m, in \u001b[0;36mAsyncAPIClient._request\u001b[0;34m(self, cast_to, options, stream, stream_cls, remaining_retries)\u001b[0m\n\u001b[1;32m   1610\u001b[0m     log\u001b[38;5;241m.\u001b[39mdebug(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mRe-raising status error\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[0;32m-> 1611\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_make_status_error_from_response(err\u001b[38;5;241m.\u001b[39mresponse) \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[1;32m   1613\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;01mawait\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_process_response(\n\u001b[1;32m   1614\u001b[0m     cast_to\u001b[38;5;241m=\u001b[39mcast_to,\n\u001b[1;32m   1615\u001b[0m     options\u001b[38;5;241m=\u001b[39moptions,\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m   1619\u001b[0m     retries_taken\u001b[38;5;241m=\u001b[39moptions\u001b[38;5;241m.\u001b[39mget_max_retries(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mmax_retries) \u001b[38;5;241m-\u001b[39m retries,\n\u001b[1;32m   1620\u001b[0m )\n",
      "\u001b[0;31mInternalServerError\u001b[0m: Error code: 500 - {'error': 'Internal server error'}",
      "\nDuring handling of the above exception, another exception occurred:\n",
      "\u001b[0;31mOpenAIError\u001b[0m                               Traceback (most recent call last)",
      "File \u001b[0;32m~/miniforge3/envs/weave/lib/python3.11/site-packages/litellm/main.py:427\u001b[0m, in \u001b[0;36macompletion\u001b[0;34m(model, messages, functions, function_call, timeout, temperature, top_p, n, stream, stream_options, stop, max_tokens, presence_penalty, frequency_penalty, logit_bias, user, response_format, seed, tools, tool_choice, parallel_tool_calls, logprobs, top_logprobs, deployment_id, base_url, api_version, api_key, model_list, extra_headers, **kwargs)\u001b[0m\n\u001b[1;32m    426\u001b[0m \u001b[38;5;28;01melif\u001b[39;00m asyncio\u001b[38;5;241m.\u001b[39miscoroutine(init_response):\n\u001b[0;32m--> 427\u001b[0m     response \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mawait\u001b[39;00m init_response\n\u001b[1;32m    428\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n",
      "File \u001b[0;32m~/miniforge3/envs/weave/lib/python3.11/site-packages/litellm/llms/openai.py:1136\u001b[0m, in \u001b[0;36mOpenAIChatCompletion.acompletion\u001b[0;34m(self, data, model_response, logging_obj, timeout, api_key, api_base, organization, client, max_retries, headers, drop_params)\u001b[0m\n\u001b[1;32m   1135\u001b[0m error_headers \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mgetattr\u001b[39m(e, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mheaders\u001b[39m\u001b[38;5;124m\"\u001b[39m, \u001b[38;5;28;01mNone\u001b[39;00m)\n\u001b[0;32m-> 1136\u001b[0m \u001b[38;5;28;01mraise\u001b[39;00m OpenAIError(\n\u001b[1;32m   1137\u001b[0m     status_code\u001b[38;5;241m=\u001b[39mstatus_code, message\u001b[38;5;241m=\u001b[39m\u001b[38;5;28mstr\u001b[39m(e), headers\u001b[38;5;241m=\u001b[39merror_headers\n\u001b[1;32m   1138\u001b[0m )\n",
      "\u001b[0;31mOpenAIError\u001b[0m: Error code: 500 - {'error': 'Internal server error'}",
      "\nDuring handling of the above exception, another exception occurred:\n",
      "\u001b[0;31mAPIError\u001b[0m                                  Traceback (most recent call last)",
      "\u001b[0;32m~/miniforge3/envs/weave/lib/python3.11/site-packages/instructor/retry.py\u001b[0m in \u001b[0;36m?\u001b[0;34m(func, response_model, validation_context, args, kwargs, max_retries, strict, mode)\u001b[0m\n\u001b[1;32m    265\u001b[0m     \u001b[0;32mexcept\u001b[0m \u001b[0mRetryError\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0me\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    266\u001b[0m         \u001b[0mlogger\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mexception\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34mf\"Failed after retries: {e.last_attempt.exception}\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 267\u001b[0;31m         raise InstructorRetryException(\n\u001b[0m\u001b[1;32m    268\u001b[0m             \u001b[0me\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/miniforge3/envs/weave/lib/python3.11/site-packages/weave/trace/op.py\u001b[0m in \u001b[0;36m?\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m    331\u001b[0m                         \u001b[0;32mreturn\u001b[0m \u001b[0;32mawait\u001b[0m \u001b[0mfunc\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    332\u001b[0m                     \u001b[0mcall\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0m_create_call\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mwrapper\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m  \u001b[0;31m# type: ignore\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 333\u001b[0;31m                     \u001b[0mres\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0m_\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;32mawait\u001b[0m \u001b[0m_execute_call\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mwrapper\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcall\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m  \u001b[0;31m# type: ignore\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    334\u001b[0m                     \u001b[0;32mreturn\u001b[0m \u001b[0mres\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/miniforge3/envs/weave/lib/python3.11/site-packages/weave/trace/op.py\u001b[0m in \u001b[0;36m?\u001b[0;34m()\u001b[0m\n\u001b[1;32m    215\u001b[0m                 \u001b[0;32mreturn\u001b[0m \u001b[0mprocess\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mres\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    216\u001b[0m             \u001b[0;32mfinally\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 217\u001b[0;31m                 \u001b[0mcall_context\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpop_call\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mcall\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mid\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;32m~/miniforge3/envs/weave/lib/python3.11/site-packages/weave/trace/op.py\u001b[0m in \u001b[0;36m?\u001b[0;34m()\u001b[0m\n\u001b[1;32m    215\u001b[0m                 \u001b[0;32mreturn\u001b[0m \u001b[0mprocess\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mres\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    216\u001b[0m             \u001b[0;32mfinally\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 217\u001b[0;31m                 \u001b[0mcall_context\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpop_call\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mcall\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mid\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;32m~/miniforge3/envs/weave/lib/python3.11/site-packages/litellm/utils.py\u001b[0m in \u001b[0;36m?\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m   1591\u001b[0m                         \u001b[0mkwargs\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m\"model\"\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mcontext_window_fallback_dict\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mmodel\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1592\u001b[0m                     \u001b[0;32mreturn\u001b[0m \u001b[0;32mawait\u001b[0m \u001b[0moriginal_function\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1593\u001b[0;31m             \u001b[0;32mraise\u001b[0m \u001b[0me\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;32m~/miniforge3/envs/weave/lib/python3.11/site-packages/litellm/utils.py\u001b[0m in \u001b[0;36m?\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m   1591\u001b[0m                         \u001b[0mkwargs\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m\"model\"\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mcontext_window_fallback_dict\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mmodel\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1592\u001b[0m                     \u001b[0;32mreturn\u001b[0m \u001b[0;32mawait\u001b[0m \u001b[0moriginal_function\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1593\u001b[0;31m             \u001b[0;32mraise\u001b[0m \u001b[0me\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;32m~/miniforge3/envs/weave/lib/python3.11/site-packages/litellm/main.py\u001b[0m in \u001b[0;36m?\u001b[0;34m(model, messages, functions, function_call, timeout, temperature, top_p, n, stream, stream_options, stop, max_tokens, presence_penalty, frequency_penalty, logit_bias, user, response_format, seed, tools, tool_choice, parallel_tool_calls, logprobs, top_logprobs, deployment_id, base_url, api_version, api_key, model_list, extra_headers, **kwargs)\u001b[0m\n\u001b[1;32m    447\u001b[0m     \u001b[0;32mexcept\u001b[0m \u001b[0mException\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0me\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    448\u001b[0m         \u001b[0mcustom_llm_provider\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mcustom_llm_provider\u001b[0m \u001b[0;32mor\u001b[0m \u001b[0;34m\"openai\"\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 449\u001b[0;31m         raise exception_type(\n\u001b[0m\u001b[1;32m    450\u001b[0m             \u001b[0mmodel\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mmodel\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/miniforge3/envs/weave/lib/python3.11/site-packages/litellm/utils.py\u001b[0m in \u001b[0;36m?\u001b[0;34m(model, original_exception, custom_llm_provider, completion_kwargs, extra_kwargs)\u001b[0m\n\u001b[1;32m   8185\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mexception_mapping_worked\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   8186\u001b[0m             \u001b[0msetattr\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0me\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m\"litellm_response_headers\"\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mlitellm_response_headers\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 8187\u001b[0;31m             \u001b[0;32mraise\u001b[0m \u001b[0me\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   8188\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/miniforge3/envs/weave/lib/python3.11/site-packages/litellm/utils.py\u001b[0m in \u001b[0;36m?\u001b[0;34m(model, original_exception, custom_llm_provider, completion_kwargs, extra_kwargs)\u001b[0m\n\u001b[1;32m   8185\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mexception_mapping_worked\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   8186\u001b[0m             \u001b[0msetattr\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0me\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m\"litellm_response_headers\"\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mlitellm_response_headers\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 8187\u001b[0;31m             \u001b[0;32mraise\u001b[0m \u001b[0me\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   8188\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mAPIError\u001b[0m: litellm.APIError: APIError: MistralException - Error code: 500 - {'error': 'Internal server error'}",
      "\nThe above exception was the direct cause of the following exception:\n",
      "\u001b[0;31mRetryError\u001b[0m                                Traceback (most recent call last)",
      "File \u001b[0;32m~/miniforge3/envs/weave/lib/python3.11/site-packages/instructor/retry.py:235\u001b[0m, in \u001b[0;36mretry_async\u001b[0;34m(func, response_model, validation_context, args, kwargs, max_retries, strict, mode)\u001b[0m\n\u001b[1;32m    234\u001b[0m response \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[0;32m--> 235\u001b[0m \u001b[38;5;28;01masync\u001b[39;00m \u001b[38;5;28;01mfor\u001b[39;00m attempt \u001b[38;5;129;01min\u001b[39;00m max_retries:\n\u001b[1;32m    236\u001b[0m     logger\u001b[38;5;241m.\u001b[39mdebug(\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mRetrying, attempt: \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mattempt\u001b[38;5;241m.\u001b[39mretry_state\u001b[38;5;241m.\u001b[39mattempt_number\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m\"\u001b[39m)\n",
      "File \u001b[0;32m~/miniforge3/envs/weave/lib/python3.11/site-packages/tenacity/asyncio/__init__.py:166\u001b[0m, in \u001b[0;36mAsyncRetrying.__anext__\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    165\u001b[0m \u001b[38;5;28;01mwhile\u001b[39;00m \u001b[38;5;28;01mTrue\u001b[39;00m:\n\u001b[0;32m--> 166\u001b[0m     do \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mawait\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39miter(retry_state\u001b[38;5;241m=\u001b[39m\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_retry_state)\n\u001b[1;32m    167\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m do \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n",
      "File \u001b[0;32m~/miniforge3/envs/weave/lib/python3.11/site-packages/tenacity/asyncio/__init__.py:153\u001b[0m, in \u001b[0;36mAsyncRetrying.iter\u001b[0;34m(self, retry_state)\u001b[0m\n\u001b[1;32m    152\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m action \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39miter_state\u001b[38;5;241m.\u001b[39mactions:\n\u001b[0;32m--> 153\u001b[0m     result \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mawait\u001b[39;00m action(retry_state)\n\u001b[1;32m    154\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m result\n",
      "File \u001b[0;32m~/miniforge3/envs/weave/lib/python3.11/site-packages/tenacity/_utils.py:99\u001b[0m, in \u001b[0;36mwrap_to_async_func.<locals>.inner\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m     98\u001b[0m \u001b[38;5;28;01masync\u001b[39;00m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21minner\u001b[39m(\u001b[38;5;241m*\u001b[39margs: typing\u001b[38;5;241m.\u001b[39mAny, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs: typing\u001b[38;5;241m.\u001b[39mAny) \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m>\u001b[39m typing\u001b[38;5;241m.\u001b[39mAny:\n\u001b[0;32m---> 99\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mcall\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/miniforge3/envs/weave/lib/python3.11/site-packages/tenacity/__init__.py:419\u001b[0m, in \u001b[0;36mBaseRetrying._post_stop_check_actions.<locals>.exc_check\u001b[0;34m(rs)\u001b[0m\n\u001b[1;32m    418\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m retry_exc\u001b[38;5;241m.\u001b[39mreraise()\n\u001b[0;32m--> 419\u001b[0m \u001b[38;5;28;01mraise\u001b[39;00m retry_exc \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mfut\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mexception\u001b[39;00m()\n",
      "\u001b[0;31mRetryError\u001b[0m: RetryError[<Future at 0x3388790d0 state=finished raised APIError>]",
      "\nThe above exception was the direct cause of the following exception:\n",
      "\u001b[0;31mInstructorRetryException\u001b[0m                  Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[7], line 1\u001b[0m\n\u001b[0;32m----> 1\u001b[0m reflection_result \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mawait\u001b[39;00m rag_solver_with_reflection(\n\u001b[1;32m      2\u001b[0m     retriever, problem, max_iterations\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m2\u001b[39m, timeout\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m30\u001b[39m\n\u001b[1;32m      3\u001b[0m )\n\u001b[1;32m      5\u001b[0m \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m*\u001b[39m\u001b[38;5;124m\"\u001b[39m \u001b[38;5;241m*\u001b[39m \u001b[38;5;241m80\u001b[39m)\n\u001b[1;32m      6\u001b[0m \u001b[38;5;28mprint\u001b[39m(reflection_result[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124msolution\u001b[39m\u001b[38;5;124m\"\u001b[39m]\u001b[38;5;241m.\u001b[39msource_code)\n",
      "File \u001b[0;32m~/miniforge3/envs/weave/lib/python3.11/site-packages/weave/trace/op.py:333\u001b[0m, in \u001b[0;36mop.<locals>.op_deco.<locals>.create_wrapper.<locals>.wrapper\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m    331\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;01mawait\u001b[39;00m func(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n\u001b[1;32m    332\u001b[0m call \u001b[38;5;241m=\u001b[39m _create_call(wrapper, \u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)  \u001b[38;5;66;03m# type: ignore\u001b[39;00m\n\u001b[0;32m--> 333\u001b[0m res, _ \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mawait\u001b[39;00m _execute_call(wrapper, call, \u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)  \u001b[38;5;66;03m# type: ignore\u001b[39;00m\n\u001b[1;32m    334\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m res\n",
      "File \u001b[0;32m~/miniforge3/envs/weave/lib/python3.11/site-packages/weave/trace/op.py:213\u001b[0m, in \u001b[0;36m_execute_call.<locals>._call_async\u001b[0;34m()\u001b[0m\n\u001b[1;32m    211\u001b[0m     res \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mawait\u001b[39;00m func(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n\u001b[1;32m    212\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mException\u001b[39;00m \u001b[38;5;28;01mas\u001b[39;00m e:\n\u001b[0;32m--> 213\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mhandle_exception\u001b[49m\u001b[43m(\u001b[49m\u001b[43me\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    214\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m    215\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m process(res)\n",
      "File \u001b[0;32m~/miniforge3/envs/weave/lib/python3.11/site-packages/weave/trace/op.py:211\u001b[0m, in \u001b[0;36m_execute_call.<locals>._call_async\u001b[0;34m()\u001b[0m\n\u001b[1;32m    209\u001b[0m call_context\u001b[38;5;241m.\u001b[39mpush_call(call)\n\u001b[1;32m    210\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[0;32m--> 211\u001b[0m     res \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mawait\u001b[39;00m func(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n\u001b[1;32m    212\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mException\u001b[39;00m \u001b[38;5;28;01mas\u001b[39;00m e:\n\u001b[1;32m    213\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m handle_exception(e)\n",
      "Cell \u001b[0;32mIn[6], line 14\u001b[0m, in \u001b[0;36mrag_solver_with_reflection\u001b[0;34m(retriever, problem, model, temperature, max_iterations, timeout)\u001b[0m\n\u001b[1;32m     12\u001b[0m solution \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[1;32m     13\u001b[0m \u001b[38;5;28;01mwhile\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m test_report \u001b[38;5;241m==\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mpassed\u001b[39m\u001b[38;5;124m\"\u001b[39m \u001b[38;5;129;01mand\u001b[39;00m num_iterations \u001b[38;5;241m<\u001b[39m max_iterations:\n\u001b[0;32m---> 14\u001b[0m     rag_result \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mawait\u001b[39;00m rag_solver(\n\u001b[1;32m     15\u001b[0m         retriever\u001b[38;5;241m=\u001b[39mretriever,\n\u001b[1;32m     16\u001b[0m         problem\u001b[38;5;241m=\u001b[39mproblem,\n\u001b[1;32m     17\u001b[0m         timeout\u001b[38;5;241m=\u001b[39mtimeout,\n\u001b[1;32m     18\u001b[0m         model\u001b[38;5;241m=\u001b[39mmodel,\n\u001b[1;32m     19\u001b[0m         temperature\u001b[38;5;241m=\u001b[39mtemperature,\n\u001b[1;32m     20\u001b[0m     )\n\u001b[1;32m     21\u001b[0m     solution \u001b[38;5;241m=\u001b[39m rag_result[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124msolution\u001b[39m\u001b[38;5;124m\"\u001b[39m]\n\u001b[1;32m     22\u001b[0m     test_report \u001b[38;5;241m=\u001b[39m rag_result[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mtest_report\u001b[39m\u001b[38;5;124m\"\u001b[39m]\n",
      "File \u001b[0;32m~/miniforge3/envs/weave/lib/python3.11/site-packages/weave/trace/op.py:333\u001b[0m, in \u001b[0;36mop.<locals>.op_deco.<locals>.create_wrapper.<locals>.wrapper\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m    331\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;01mawait\u001b[39;00m func(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n\u001b[1;32m    332\u001b[0m call \u001b[38;5;241m=\u001b[39m _create_call(wrapper, \u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)  \u001b[38;5;66;03m# type: ignore\u001b[39;00m\n\u001b[0;32m--> 333\u001b[0m res, _ \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mawait\u001b[39;00m _execute_call(wrapper, call, \u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)  \u001b[38;5;66;03m# type: ignore\u001b[39;00m\n\u001b[1;32m    334\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m res\n",
      "File \u001b[0;32m~/miniforge3/envs/weave/lib/python3.11/site-packages/weave/trace/op.py:213\u001b[0m, in \u001b[0;36m_execute_call.<locals>._call_async\u001b[0;34m()\u001b[0m\n\u001b[1;32m    211\u001b[0m     res \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mawait\u001b[39;00m func(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n\u001b[1;32m    212\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mException\u001b[39;00m \u001b[38;5;28;01mas\u001b[39;00m e:\n\u001b[0;32m--> 213\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mhandle_exception\u001b[49m\u001b[43m(\u001b[49m\u001b[43me\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    214\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m    215\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m process(res)\n",
      "File \u001b[0;32m~/miniforge3/envs/weave/lib/python3.11/site-packages/weave/trace/op.py:211\u001b[0m, in \u001b[0;36m_execute_call.<locals>._call_async\u001b[0;34m()\u001b[0m\n\u001b[1;32m    209\u001b[0m call_context\u001b[38;5;241m.\u001b[39mpush_call(call)\n\u001b[1;32m    210\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[0;32m--> 211\u001b[0m     res \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mawait\u001b[39;00m func(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n\u001b[1;32m    212\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mException\u001b[39;00m \u001b[38;5;28;01mas\u001b[39;00m e:\n\u001b[1;32m    213\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m handle_exception(e)\n",
      "File \u001b[0;32m~/work/hackercup_rag/agent.py:398\u001b[0m, in \u001b[0;36mrag_solver\u001b[0;34m(retriever, problem, model, temperature, timeout)\u001b[0m\n\u001b[1;32m    395\u001b[0m     logger\u001b[38;5;241m.\u001b[39minfo(\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mRAG Solution Result: \u001b[39m\u001b[38;5;132;01m{\u001b[39;00m\u001b[38;5;28mrepr\u001b[39m(test_report)\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m\"\u001b[39m)\n\u001b[1;32m    396\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m {\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124msolution\u001b[39m\u001b[38;5;124m\"\u001b[39m: rag_solution, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mtest_report\u001b[39m\u001b[38;5;124m\"\u001b[39m: test_report}\n\u001b[0;32m--> 398\u001b[0m rag_result \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mawait\u001b[39;00m rag_solution(problem, solution, model, temperature, timeout)\n\u001b[1;32m    399\u001b[0m solution \u001b[38;5;241m=\u001b[39m rag_result[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124msolution\u001b[39m\u001b[38;5;124m\"\u001b[39m]\n\u001b[1;32m    400\u001b[0m test_report \u001b[38;5;241m=\u001b[39m rag_result[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mtest_report\u001b[39m\u001b[38;5;124m\"\u001b[39m]\n",
      "File \u001b[0;32m~/miniforge3/envs/weave/lib/python3.11/site-packages/weave/trace/op.py:333\u001b[0m, in \u001b[0;36mop.<locals>.op_deco.<locals>.create_wrapper.<locals>.wrapper\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m    331\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;01mawait\u001b[39;00m func(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n\u001b[1;32m    332\u001b[0m call \u001b[38;5;241m=\u001b[39m _create_call(wrapper, \u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)  \u001b[38;5;66;03m# type: ignore\u001b[39;00m\n\u001b[0;32m--> 333\u001b[0m res, _ \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mawait\u001b[39;00m _execute_call(wrapper, call, \u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)  \u001b[38;5;66;03m# type: ignore\u001b[39;00m\n\u001b[1;32m    334\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m res\n",
      "File \u001b[0;32m~/miniforge3/envs/weave/lib/python3.11/site-packages/weave/trace/op.py:213\u001b[0m, in \u001b[0;36m_execute_call.<locals>._call_async\u001b[0;34m()\u001b[0m\n\u001b[1;32m    211\u001b[0m     res \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mawait\u001b[39;00m func(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n\u001b[1;32m    212\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mException\u001b[39;00m \u001b[38;5;28;01mas\u001b[39;00m e:\n\u001b[0;32m--> 213\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mhandle_exception\u001b[49m\u001b[43m(\u001b[49m\u001b[43me\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    214\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m    215\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m process(res)\n",
      "File \u001b[0;32m~/miniforge3/envs/weave/lib/python3.11/site-packages/weave/trace/op.py:211\u001b[0m, in \u001b[0;36m_execute_call.<locals>._call_async\u001b[0;34m()\u001b[0m\n\u001b[1;32m    209\u001b[0m call_context\u001b[38;5;241m.\u001b[39mpush_call(call)\n\u001b[1;32m    210\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[0;32m--> 211\u001b[0m     res \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mawait\u001b[39;00m func(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n\u001b[1;32m    212\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mException\u001b[39;00m \u001b[38;5;28;01mas\u001b[39;00m e:\n\u001b[1;32m    213\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m handle_exception(e)\n",
      "File \u001b[0;32m~/work/hackercup_rag/agent.py:382\u001b[0m, in \u001b[0;36mrag_solver.<locals>.rag_solution\u001b[0;34m(problem, draft_solution, model, temperature, timeout)\u001b[0m\n\u001b[1;32m    373\u001b[0m \u001b[38;5;129m@weave\u001b[39m\u001b[38;5;241m.\u001b[39mop\n\u001b[1;32m    374\u001b[0m \u001b[38;5;28;01masync\u001b[39;00m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mrag_solution\u001b[39m(\n\u001b[1;32m    375\u001b[0m     problem: Problem,\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    379\u001b[0m     timeout: \u001b[38;5;28mint\u001b[39m \u001b[38;5;241m=\u001b[39m timeout,\n\u001b[1;32m    380\u001b[0m ) \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m>\u001b[39m \u001b[38;5;28mdict\u001b[39m:\n\u001b[1;32m    381\u001b[0m     logger\u001b[38;5;241m.\u001b[39minfo(\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mGenerating RAG solution:\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[0;32m--> 382\u001b[0m     examplars \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mawait\u001b[39;00m create_examplars(problem, draft_solution)\n\u001b[1;32m    383\u001b[0m     rag_solution \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mawait\u001b[39;00m generate_solution(\n\u001b[1;32m    384\u001b[0m         problem\u001b[38;5;241m=\u001b[39mproblem,\n\u001b[1;32m    385\u001b[0m         examples\u001b[38;5;241m=\u001b[39mexamplars,\n\u001b[1;32m    386\u001b[0m         model\u001b[38;5;241m=\u001b[39mmodel,\n\u001b[1;32m    387\u001b[0m         temperature\u001b[38;5;241m=\u001b[39mtemperature,\n\u001b[1;32m    388\u001b[0m     )\n\u001b[1;32m    389\u001b[0m     test_report \u001b[38;5;241m=\u001b[39m check_correctness(\n\u001b[1;32m    390\u001b[0m         rag_solution\u001b[38;5;241m.\u001b[39msource_code,\n\u001b[1;32m    391\u001b[0m         problem\u001b[38;5;241m.\u001b[39msample_input,\n\u001b[1;32m    392\u001b[0m         problem\u001b[38;5;241m.\u001b[39msample_output,\n\u001b[1;32m    393\u001b[0m         timeout,\n\u001b[1;32m    394\u001b[0m     )\n",
      "File \u001b[0;32m~/miniforge3/envs/weave/lib/python3.11/site-packages/weave/trace/op.py:333\u001b[0m, in \u001b[0;36mop.<locals>.op_deco.<locals>.create_wrapper.<locals>.wrapper\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m    331\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;01mawait\u001b[39;00m func(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n\u001b[1;32m    332\u001b[0m call \u001b[38;5;241m=\u001b[39m _create_call(wrapper, \u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)  \u001b[38;5;66;03m# type: ignore\u001b[39;00m\n\u001b[0;32m--> 333\u001b[0m res, _ \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mawait\u001b[39;00m _execute_call(wrapper, call, \u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)  \u001b[38;5;66;03m# type: ignore\u001b[39;00m\n\u001b[1;32m    334\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m res\n",
      "File \u001b[0;32m~/miniforge3/envs/weave/lib/python3.11/site-packages/weave/trace/op.py:213\u001b[0m, in \u001b[0;36m_execute_call.<locals>._call_async\u001b[0;34m()\u001b[0m\n\u001b[1;32m    211\u001b[0m     res \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mawait\u001b[39;00m func(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n\u001b[1;32m    212\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mException\u001b[39;00m \u001b[38;5;28;01mas\u001b[39;00m e:\n\u001b[0;32m--> 213\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mhandle_exception\u001b[49m\u001b[43m(\u001b[49m\u001b[43me\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    214\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m    215\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m process(res)\n",
      "File \u001b[0;32m~/miniforge3/envs/weave/lib/python3.11/site-packages/weave/trace/op.py:211\u001b[0m, in \u001b[0;36m_execute_call.<locals>._call_async\u001b[0;34m()\u001b[0m\n\u001b[1;32m    209\u001b[0m call_context\u001b[38;5;241m.\u001b[39mpush_call(call)\n\u001b[1;32m    210\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[0;32m--> 211\u001b[0m     res \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mawait\u001b[39;00m func(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n\u001b[1;32m    212\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mException\u001b[39;00m \u001b[38;5;28;01mas\u001b[39;00m e:\n\u001b[1;32m    213\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m handle_exception(e)\n",
      "File \u001b[0;32m~/work/hackercup_rag/agent.py:369\u001b[0m, in \u001b[0;36mrag_solver.<locals>.create_examplars\u001b[0;34m(problem, solution, top_k, top_n)\u001b[0m\n\u001b[1;32m    367\u001b[0m retrieve_docs \u001b[38;5;241m=\u001b[39m retriever\u001b[38;5;241m.\u001b[39mretrieve(solution\u001b[38;5;241m.\u001b[39msource_code, top_k)\n\u001b[1;32m    368\u001b[0m reranked_docs \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mawait\u001b[39;00m rerank_docs(problem, solution, retrieve_docs, top_n)\n\u001b[0;32m--> 369\u001b[0m analyses \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mawait\u001b[39;00m describe_examples(reranked_docs)\n\u001b[1;32m    370\u001b[0m examplars \u001b[38;5;241m=\u001b[39m format_examples(reranked_docs, analyses)\n\u001b[1;32m    371\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m examplars\n",
      "File \u001b[0;32m~/miniforge3/envs/weave/lib/python3.11/site-packages/weave/trace/op.py:333\u001b[0m, in \u001b[0;36mop.<locals>.op_deco.<locals>.create_wrapper.<locals>.wrapper\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m    331\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;01mawait\u001b[39;00m func(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n\u001b[1;32m    332\u001b[0m call \u001b[38;5;241m=\u001b[39m _create_call(wrapper, \u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)  \u001b[38;5;66;03m# type: ignore\u001b[39;00m\n\u001b[0;32m--> 333\u001b[0m res, _ \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mawait\u001b[39;00m _execute_call(wrapper, call, \u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)  \u001b[38;5;66;03m# type: ignore\u001b[39;00m\n\u001b[1;32m    334\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m res\n",
      "File \u001b[0;32m~/miniforge3/envs/weave/lib/python3.11/site-packages/weave/trace/op.py:213\u001b[0m, in \u001b[0;36m_execute_call.<locals>._call_async\u001b[0;34m()\u001b[0m\n\u001b[1;32m    211\u001b[0m     res \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mawait\u001b[39;00m func(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n\u001b[1;32m    212\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mException\u001b[39;00m \u001b[38;5;28;01mas\u001b[39;00m e:\n\u001b[0;32m--> 213\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mhandle_exception\u001b[49m\u001b[43m(\u001b[49m\u001b[43me\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    214\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m    215\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m process(res)\n",
      "File \u001b[0;32m~/miniforge3/envs/weave/lib/python3.11/site-packages/weave/trace/op.py:211\u001b[0m, in \u001b[0;36m_execute_call.<locals>._call_async\u001b[0;34m()\u001b[0m\n\u001b[1;32m    209\u001b[0m call_context\u001b[38;5;241m.\u001b[39mpush_call(call)\n\u001b[1;32m    210\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[0;32m--> 211\u001b[0m     res \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mawait\u001b[39;00m func(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n\u001b[1;32m    212\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mException\u001b[39;00m \u001b[38;5;28;01mas\u001b[39;00m e:\n\u001b[1;32m    213\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m handle_exception(e)\n",
      "File \u001b[0;32m~/work/hackercup_rag/agent.py:163\u001b[0m, in \u001b[0;36mdescribe_examples\u001b[0;34m(docs)\u001b[0m\n\u001b[1;32m    161\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m doc \u001b[38;5;129;01min\u001b[39;00m docs:\n\u001b[1;32m    162\u001b[0m     tasks\u001b[38;5;241m.\u001b[39mappend(describe_example(doc))\n\u001b[0;32m--> 163\u001b[0m descriptions \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mawait\u001b[39;00m asyncio\u001b[38;5;241m.\u001b[39mgather(\u001b[38;5;241m*\u001b[39mtasks)\n\u001b[1;32m    164\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m descriptions\n",
      "File \u001b[0;32m~/miniforge3/envs/weave/lib/python3.11/asyncio/tasks.py:349\u001b[0m, in \u001b[0;36mTask.__wakeup\u001b[0;34m(self, future)\u001b[0m\n\u001b[1;32m    347\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21m__wakeup\u001b[39m(\u001b[38;5;28mself\u001b[39m, future):\n\u001b[1;32m    348\u001b[0m     \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[0;32m--> 349\u001b[0m         \u001b[43mfuture\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mresult\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    350\u001b[0m     \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mBaseException\u001b[39;00m \u001b[38;5;28;01mas\u001b[39;00m exc:\n\u001b[1;32m    351\u001b[0m         \u001b[38;5;66;03m# This may also be a cancellation.\u001b[39;00m\n\u001b[1;32m    352\u001b[0m         \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m__step(exc)\n",
      "File \u001b[0;32m~/miniforge3/envs/weave/lib/python3.11/asyncio/tasks.py:277\u001b[0m, in \u001b[0;36mTask.__step\u001b[0;34m(***failed resolving arguments***)\u001b[0m\n\u001b[1;32m    273\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[1;32m    274\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m exc \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[1;32m    275\u001b[0m         \u001b[38;5;66;03m# We use the `send` method directly, because coroutines\u001b[39;00m\n\u001b[1;32m    276\u001b[0m         \u001b[38;5;66;03m# don't have `__iter__` and `__next__` methods.\u001b[39;00m\n\u001b[0;32m--> 277\u001b[0m         result \u001b[38;5;241m=\u001b[39m coro\u001b[38;5;241m.\u001b[39msend(\u001b[38;5;28;01mNone\u001b[39;00m)\n\u001b[1;32m    278\u001b[0m     \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m    279\u001b[0m         result \u001b[38;5;241m=\u001b[39m coro\u001b[38;5;241m.\u001b[39mthrow(exc)\n",
      "File \u001b[0;32m~/miniforge3/envs/weave/lib/python3.11/site-packages/weave/trace/op.py:333\u001b[0m, in \u001b[0;36mop.<locals>.op_deco.<locals>.create_wrapper.<locals>.wrapper\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m    331\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;01mawait\u001b[39;00m func(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n\u001b[1;32m    332\u001b[0m call \u001b[38;5;241m=\u001b[39m _create_call(wrapper, \u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)  \u001b[38;5;66;03m# type: ignore\u001b[39;00m\n\u001b[0;32m--> 333\u001b[0m res, _ \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mawait\u001b[39;00m _execute_call(wrapper, call, \u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)  \u001b[38;5;66;03m# type: ignore\u001b[39;00m\n\u001b[1;32m    334\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m res\n",
      "File \u001b[0;32m~/miniforge3/envs/weave/lib/python3.11/site-packages/weave/trace/op.py:213\u001b[0m, in \u001b[0;36m_execute_call.<locals>._call_async\u001b[0;34m()\u001b[0m\n\u001b[1;32m    211\u001b[0m     res \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mawait\u001b[39;00m func(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n\u001b[1;32m    212\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mException\u001b[39;00m \u001b[38;5;28;01mas\u001b[39;00m e:\n\u001b[0;32m--> 213\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mhandle_exception\u001b[49m\u001b[43m(\u001b[49m\u001b[43me\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    214\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m    215\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m process(res)\n",
      "File \u001b[0;32m~/miniforge3/envs/weave/lib/python3.11/site-packages/weave/trace/op.py:211\u001b[0m, in \u001b[0;36m_execute_call.<locals>._call_async\u001b[0;34m()\u001b[0m\n\u001b[1;32m    209\u001b[0m call_context\u001b[38;5;241m.\u001b[39mpush_call(call)\n\u001b[1;32m    210\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[0;32m--> 211\u001b[0m     res \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mawait\u001b[39;00m func(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n\u001b[1;32m    212\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mException\u001b[39;00m \u001b[38;5;28;01mas\u001b[39;00m e:\n\u001b[1;32m    213\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m handle_exception(e)\n",
      "File \u001b[0;32m~/work/hackercup_rag/agent.py:141\u001b[0m, in \u001b[0;36mdescribe_example\u001b[0;34m(example)\u001b[0m\n\u001b[1;32m    135\u001b[0m \u001b[38;5;129m@weave\u001b[39m\u001b[38;5;241m.\u001b[39mop\n\u001b[1;32m    136\u001b[0m \u001b[38;5;28;01masync\u001b[39;00m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mdescribe_example\u001b[39m(example: \u001b[38;5;28mdict\u001b[39m) \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m>\u001b[39m Analysis:\n\u001b[1;32m    137\u001b[0m     user_prompt \u001b[38;5;241m=\u001b[39m \u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\"\"\u001b[39m\u001b[38;5;132;01m{\u001b[39;00mformat_example(example)\u001b[38;5;132;01m}\u001b[39;00m\n\u001b[1;32m    138\u001b[0m \n\u001b[1;32m    139\u001b[0m \u001b[38;5;124mLet\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124ms think step by step to analyze the problem and plan a solution to the problem:\u001b[39m\n\u001b[1;32m    140\u001b[0m \u001b[38;5;124m\"\"\"\u001b[39m\n\u001b[0;32m--> 141\u001b[0m     response \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mawait\u001b[39;00m async_client\u001b[38;5;241m.\u001b[39mchat\u001b[38;5;241m.\u001b[39mcompletions\u001b[38;5;241m.\u001b[39mcreate(\n\u001b[1;32m    142\u001b[0m         model\u001b[38;5;241m=\u001b[39mFAST_LLM,\n\u001b[1;32m    143\u001b[0m         messages\u001b[38;5;241m=\u001b[39m[\n\u001b[1;32m    144\u001b[0m             {\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mrole\u001b[39m\u001b[38;5;124m\"\u001b[39m: \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124msystem\u001b[39m\u001b[38;5;124m\"\u001b[39m, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mcontent\u001b[39m\u001b[38;5;124m\"\u001b[39m: ANALYSIS_INSTRUCTIONS},\n\u001b[1;32m    145\u001b[0m             {\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mrole\u001b[39m\u001b[38;5;124m\"\u001b[39m: \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124muser\u001b[39m\u001b[38;5;124m\"\u001b[39m, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mcontent\u001b[39m\u001b[38;5;124m\"\u001b[39m: user_prompt},\n\u001b[1;32m    146\u001b[0m         ],\n\u001b[1;32m    147\u001b[0m         response_model\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mNone\u001b[39;00m,\n\u001b[1;32m    148\u001b[0m         max_tokens\u001b[38;5;241m=\u001b[39mMAX_TOKENS,\n\u001b[1;32m    149\u001b[0m         base_url\u001b[38;5;241m=\u001b[39mBASE_URL,\n\u001b[1;32m    150\u001b[0m     )\n\u001b[1;32m    152\u001b[0m     formatted_response \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mawait\u001b[39;00m format_response(\n\u001b[1;32m    153\u001b[0m         response\u001b[38;5;241m.\u001b[39mchoices[\u001b[38;5;241m0\u001b[39m]\u001b[38;5;241m.\u001b[39mmessage\u001b[38;5;241m.\u001b[39mcontent, Analysis\n\u001b[1;32m    154\u001b[0m     )\n\u001b[1;32m    155\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m formatted_response\n",
      "File \u001b[0;32m~/miniforge3/envs/weave/lib/python3.11/site-packages/instructor/client.py:300\u001b[0m, in \u001b[0;36mAsyncInstructor.create\u001b[0;34m(self, response_model, messages, max_retries, validation_context, strict, **kwargs)\u001b[0m\n\u001b[1;32m    290\u001b[0m \u001b[38;5;28;01masync\u001b[39;00m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mcreate\u001b[39m(\n\u001b[1;32m    291\u001b[0m     \u001b[38;5;28mself\u001b[39m,\n\u001b[1;32m    292\u001b[0m     response_model: \u001b[38;5;28mtype\u001b[39m[T] \u001b[38;5;241m|\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m,\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    297\u001b[0m     \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs: Any,\n\u001b[1;32m    298\u001b[0m ) \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m>\u001b[39m T \u001b[38;5;241m|\u001b[39m Any:\n\u001b[1;32m    299\u001b[0m     kwargs \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mhandle_kwargs(kwargs)\n\u001b[0;32m--> 300\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;01mawait\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mcreate_fn(\n\u001b[1;32m    301\u001b[0m         response_model\u001b[38;5;241m=\u001b[39mresponse_model,\n\u001b[1;32m    302\u001b[0m         validation_context\u001b[38;5;241m=\u001b[39mvalidation_context,\n\u001b[1;32m    303\u001b[0m         max_retries\u001b[38;5;241m=\u001b[39mmax_retries,\n\u001b[1;32m    304\u001b[0m         messages\u001b[38;5;241m=\u001b[39mmessages,\n\u001b[1;32m    305\u001b[0m         strict\u001b[38;5;241m=\u001b[39mstrict,\n\u001b[1;32m    306\u001b[0m         \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs,\n\u001b[1;32m    307\u001b[0m     )\n",
      "File \u001b[0;32m~/miniforge3/envs/weave/lib/python3.11/site-packages/instructor/patch.py:119\u001b[0m, in \u001b[0;36mpatch.<locals>.new_create_async\u001b[0;34m(response_model, validation_context, max_retries, strict, *args, **kwargs)\u001b[0m\n\u001b[1;32m    107\u001b[0m \u001b[38;5;129m@wraps\u001b[39m(func)\n\u001b[1;32m    108\u001b[0m \u001b[38;5;28;01masync\u001b[39;00m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mnew_create_async\u001b[39m(\n\u001b[1;32m    109\u001b[0m     response_model: \u001b[38;5;28mtype\u001b[39m[T_Model] \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m,\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    114\u001b[0m     \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs: T_ParamSpec\u001b[38;5;241m.\u001b[39mkwargs,\n\u001b[1;32m    115\u001b[0m ) \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m>\u001b[39m T_Model:\n\u001b[1;32m    116\u001b[0m     response_model, new_kwargs \u001b[38;5;241m=\u001b[39m handle_response_model(\n\u001b[1;32m    117\u001b[0m         response_model\u001b[38;5;241m=\u001b[39mresponse_model, mode\u001b[38;5;241m=\u001b[39mmode, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs\n\u001b[1;32m    118\u001b[0m     )\n\u001b[0;32m--> 119\u001b[0m     response \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mawait\u001b[39;00m retry_async(\n\u001b[1;32m    120\u001b[0m         func\u001b[38;5;241m=\u001b[39mfunc,\n\u001b[1;32m    121\u001b[0m         response_model\u001b[38;5;241m=\u001b[39mresponse_model,\n\u001b[1;32m    122\u001b[0m         validation_context\u001b[38;5;241m=\u001b[39mvalidation_context,\n\u001b[1;32m    123\u001b[0m         max_retries\u001b[38;5;241m=\u001b[39mmax_retries,\n\u001b[1;32m    124\u001b[0m         args\u001b[38;5;241m=\u001b[39margs,\n\u001b[1;32m    125\u001b[0m         kwargs\u001b[38;5;241m=\u001b[39mnew_kwargs,\n\u001b[1;32m    126\u001b[0m         strict\u001b[38;5;241m=\u001b[39mstrict,\n\u001b[1;32m    127\u001b[0m         mode\u001b[38;5;241m=\u001b[39mmode,\n\u001b[1;32m    128\u001b[0m     )\n\u001b[1;32m    129\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m response\n",
      "File \u001b[0;32m~/miniforge3/envs/weave/lib/python3.11/site-packages/instructor/retry.py:267\u001b[0m, in \u001b[0;36mretry_async\u001b[0;34m(func, response_model, validation_context, args, kwargs, max_retries, strict, mode)\u001b[0m\n\u001b[1;32m    265\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m RetryError \u001b[38;5;28;01mas\u001b[39;00m e:\n\u001b[1;32m    266\u001b[0m     logger\u001b[38;5;241m.\u001b[39mexception(\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mFailed after retries: \u001b[39m\u001b[38;5;132;01m{\u001b[39;00me\u001b[38;5;241m.\u001b[39mlast_attempt\u001b[38;5;241m.\u001b[39mexception\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m\"\u001b[39m)\n\u001b[0;32m--> 267\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m InstructorRetryException(\n\u001b[1;32m    268\u001b[0m         e,\n\u001b[1;32m    269\u001b[0m         last_completion\u001b[38;5;241m=\u001b[39mresponse,\n\u001b[1;32m    270\u001b[0m         n_attempts\u001b[38;5;241m=\u001b[39mattempt\u001b[38;5;241m.\u001b[39mretry_state\u001b[38;5;241m.\u001b[39mattempt_number,\n\u001b[1;32m    271\u001b[0m         messages\u001b[38;5;241m=\u001b[39mkwargs\u001b[38;5;241m.\u001b[39mget(\n\u001b[1;32m    272\u001b[0m             \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mmessages\u001b[39m\u001b[38;5;124m\"\u001b[39m, kwargs\u001b[38;5;241m.\u001b[39mget(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mcontents\u001b[39m\u001b[38;5;124m\"\u001b[39m, kwargs\u001b[38;5;241m.\u001b[39mget(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mchat_history\u001b[39m\u001b[38;5;124m\"\u001b[39m, []))\n\u001b[1;32m    273\u001b[0m         ),\n\u001b[1;32m    274\u001b[0m         total_usage\u001b[38;5;241m=\u001b[39mtotal_usage,\n\u001b[1;32m    275\u001b[0m     ) \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01me\u001b[39;00m\n",
      "\u001b[0;31mInstructorRetryException\u001b[0m: RetryError[<Future at 0x3388790d0 state=finished raised APIError>]"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2024-09-05 23:02:42,624 : INFO : HTTP Request: POST http://195.242.24.252:8000/v1/chat/completions \"HTTP/1.1 500 Internal Server Error\"\n",
      "2024-09-05 23:02:42,627 : INFO : Retrying request to /chat/completions in 0.945439 seconds\n",
      "2024-09-05 23:02:58,686 : INFO : HTTP Request: POST http://195.242.24.252:8000/v1/chat/completions \"HTTP/1.1 200 OK\"\n"
     ]
    }
   ],
   "source": [
    "reflection_result = await rag_solver_with_reflection(\n",
    "    retriever, problem, max_iterations=2, timeout=30\n",
    ")\n",
    "\n",
    "print(\"*\" * 80)\n",
    "print(reflection_result[\"solution\"].source_code)\n",
    "print(\"*\" * 80)\n",
    "print(reflection_result[\"test_report\"])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Great, now, we are ready to evaluate a more complex agent that uses reflection\n",
    "This agent will try to solve the problem using the retriever\n",
    "and if it fails, it will ask the model to reflect on the problem\n",
    "and then re-work the solution\n",
    "and repeat this process for a fixed number of iterations\n",
    "or until the solution is correct or the iteration limit is reached\n",
    "\n",
    "But the best part is that we can use the same evaluation framework we used for the zero-shot and RAG agent to evaluate the RAG reflection agent."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class RAGReflectionAgent(weave.Model):\n",
    "    retriever: Retriever\n",
    "    max_iterations: int = 2\n",
    "    timeout: int = 30\n",
    "    model: str = STRONG_LLM\n",
    "    temperature: float = 0.0\n",
    "\n",
    "    @weave.op\n",
    "    async def predict(self, problem: Problem):\n",
    "        return await rag_solver_with_reflection(\n",
    "            self.retriever,\n",
    "            Problem(**problem),\n",
    "            model=self.model,\n",
    "            temperature=self.temperature,\n",
    "            max_iterations=self.max_iterations,\n",
    "            timeout=self.timeout,\n",
    "        )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# This is a simple depection of the evaluation.\n",
    "# We expect the output to be `\"passed\"` for all the problems if the agent is working correctly.\n",
    "examples = [{\"problem\": problem, \"expected\": \"passed\"} for problem in problems]\n",
    "\n",
    "\n",
    "# A simple scorer that checks if the code generated by agent passed the test case\n",
    "@weave.op\n",
    "def scorer(expected: str, model_output: dict) -> dict:\n",
    "    return {\"passed\": expected == model_output[\"test_report\"]}\n",
    "\n",
    "\n",
    "# This is a simple evaluation that checks if the code generated by agent passed the test\n",
    "evaluation = weave.Evaluation(dataset=examples, scorers=[scorer])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2024-09-05 23:04:42,922 : INFO : Drafting intial zero-shot solution\n",
      "\u001b[92m23:04:42 - LiteLLM:INFO\u001b[0m: utils.py:2975 - \n",
      "LiteLLM completion() model= mistral-large-latest; provider = mistral\n",
      "2024-09-05 23:04:42,929 : INFO : \n",
      "LiteLLM completion() model= mistral-large-latest; provider = mistral\n",
      "2024-09-05 23:04:43,248 : INFO : Drafting intial zero-shot solution\n",
      "\u001b[92m23:04:43 - LiteLLM:INFO\u001b[0m: utils.py:2975 - \n",
      "LiteLLM completion() model= mistral-large-latest; provider = mistral\n",
      "2024-09-05 23:04:43,255 : INFO : \n",
      "LiteLLM completion() model= mistral-large-latest; provider = mistral\n",
      "2024-09-05 23:04:43,575 : INFO : Drafting intial zero-shot solution\n",
      "\u001b[92m23:04:43 - LiteLLM:INFO\u001b[0m: utils.py:2975 - \n",
      "LiteLLM completion() model= mistral-large-latest; provider = mistral\n",
      "2024-09-05 23:04:43,582 : INFO : \n",
      "LiteLLM completion() model= mistral-large-latest; provider = mistral\n",
      "2024-09-05 23:04:43,923 : INFO : Drafting intial zero-shot solution\n",
      "\u001b[92m23:04:43 - LiteLLM:INFO\u001b[0m: utils.py:2975 - \n",
      "LiteLLM completion() model= mistral-large-latest; provider = mistral\n",
      "2024-09-05 23:04:43,930 : INFO : \n",
      "LiteLLM completion() model= mistral-large-latest; provider = mistral\n",
      "2024-09-05 23:04:45,392 : INFO : Drafting intial zero-shot solution\n",
      "\u001b[92m23:04:45 - LiteLLM:INFO\u001b[0m: utils.py:2975 - \n",
      "LiteLLM completion() model= mistral-large-latest; provider = mistral\n",
      "2024-09-05 23:04:45,401 : INFO : \n",
      "LiteLLM completion() model= mistral-large-latest; provider = mistral\n",
      "2024-09-05 23:04:45,712 : INFO : Drafting intial zero-shot solution\n",
      "\u001b[92m23:04:45 - LiteLLM:INFO\u001b[0m: utils.py:2975 - \n",
      "LiteLLM completion() model= mistral-large-latest; provider = mistral\n",
      "2024-09-05 23:04:45,723 : INFO : \n",
      "LiteLLM completion() model= mistral-large-latest; provider = mistral\n",
      "2024-09-05 23:04:46,059 : INFO : Drafting intial zero-shot solution\n",
      "\u001b[92m23:04:46 - LiteLLM:INFO\u001b[0m: utils.py:2975 - \n",
      "LiteLLM completion() model= mistral-large-latest; provider = mistral\n",
      "2024-09-05 23:04:46,068 : INFO : \n",
      "LiteLLM completion() model= mistral-large-latest; provider = mistral\n",
      "2024-09-05 23:04:46,529 : INFO : Drafting intial zero-shot solution\n",
      "\u001b[92m23:04:46 - LiteLLM:INFO\u001b[0m: utils.py:2975 - \n",
      "LiteLLM completion() model= mistral-large-latest; provider = mistral\n",
      "2024-09-05 23:04:46,536 : INFO : \n",
      "LiteLLM completion() model= mistral-large-latest; provider = mistral\n",
      "2024-09-05 23:04:46,840 : INFO : Drafting intial zero-shot solution\n",
      "\u001b[92m23:04:46 - LiteLLM:INFO\u001b[0m: utils.py:2975 - \n",
      "LiteLLM completion() model= mistral-large-latest; provider = mistral\n",
      "2024-09-05 23:04:46,849 : INFO : \n",
      "LiteLLM completion() model= mistral-large-latest; provider = mistral\n",
      "2024-09-05 23:04:47,189 : INFO : Drafting intial zero-shot solution\n",
      "\u001b[92m23:04:47 - LiteLLM:INFO\u001b[0m: utils.py:2975 - \n",
      "LiteLLM completion() model= mistral-large-latest; provider = mistral\n",
      "2024-09-05 23:04:47,193 : INFO : \n",
      "LiteLLM completion() model= mistral-large-latest; provider = mistral\n",
      "2024-09-05 23:05:05,325 : INFO : HTTP Request: POST http://195.242.24.252:8000/v1/chat/completions \"HTTP/1.1 200 OK\"\n",
      "\u001b[92m23:05:05 - LiteLLM:INFO\u001b[0m: utils.py:2975 - \n",
      "LiteLLM completion() model= open-mistral-nemo-2407; provider = mistral\n",
      "2024-09-05 23:05:05,385 : INFO : \n",
      "LiteLLM completion() model= open-mistral-nemo-2407; provider = mistral\n",
      "2024-09-05 23:05:07,467 : INFO : HTTP Request: POST http://195.242.24.252:8000/v1/chat/completions \"HTTP/1.1 500 Internal Server Error\"\n",
      "2024-09-05 23:05:07,470 : INFO : HTTP Request: POST http://195.242.24.252:8000/v1/chat/completions \"HTTP/1.1 500 Internal Server Error\"\n",
      "2024-09-05 23:05:07,471 : INFO : HTTP Request: POST http://195.242.24.252:8000/v1/chat/completions \"HTTP/1.1 500 Internal Server Error\"\n",
      "2024-09-05 23:05:07,473 : INFO : HTTP Request: POST http://195.242.24.252:8000/v1/chat/completions \"HTTP/1.1 500 Internal Server Error\"\n",
      "2024-09-05 23:05:07,474 : INFO : HTTP Request: POST http://195.242.24.252:8000/v1/chat/completions \"HTTP/1.1 500 Internal Server Error\"\n",
      "2024-09-05 23:05:07,475 : INFO : HTTP Request: POST http://195.242.24.252:8000/v1/chat/completions \"HTTP/1.1 500 Internal Server Error\"\n",
      "2024-09-05 23:05:07,476 : INFO : HTTP Request: POST http://195.242.24.252:8000/v1/chat/completions \"HTTP/1.1 500 Internal Server Error\"\n",
      "2024-09-05 23:05:07,477 : INFO : HTTP Request: POST http://195.242.24.252:8000/v1/chat/completions \"HTTP/1.1 500 Internal Server Error\"\n",
      "2024-09-05 23:05:07,478 : INFO : HTTP Request: POST http://195.242.24.252:8000/v1/chat/completions \"HTTP/1.1 500 Internal Server Error\"\n",
      "2024-09-05 23:05:07,479 : INFO : Retrying request to /chat/completions in 0.979615 seconds\n",
      "2024-09-05 23:05:07,479 : INFO : Retrying request to /chat/completions in 0.833062 seconds\n",
      "2024-09-05 23:05:07,480 : INFO : Retrying request to /chat/completions in 0.986007 seconds\n",
      "2024-09-05 23:05:07,481 : INFO : Retrying request to /chat/completions in 0.994008 seconds\n",
      "2024-09-05 23:05:07,482 : INFO : Retrying request to /chat/completions in 0.918137 seconds\n",
      "2024-09-05 23:05:07,483 : INFO : Retrying request to /chat/completions in 0.978586 seconds\n",
      "2024-09-05 23:05:07,485 : INFO : Retrying request to /chat/completions in 0.814813 seconds\n",
      "2024-09-05 23:05:07,485 : INFO : Retrying request to /chat/completions in 0.762679 seconds\n",
      "2024-09-05 23:05:07,486 : INFO : Retrying request to /chat/completions in 0.888629 seconds\n",
      "2024-09-05 23:05:12,474 : INFO : HTTP Request: POST http://195.242.24.252:8000/v1/chat/completions \"HTTP/1.1 200 OK\"\n",
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
      "2024-09-05 23:05:12,995 : INFO : Draft solution result: 'passed'\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\">Evaluated <span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">1</span> of <span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">5</span> examples\n",
       "</pre>\n"
      ],
      "text/plain": [
       "Evaluated \u001b[1;36m1\u001b[0m of \u001b[1;36m5\u001b[0m examples\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2024-09-05 23:05:27,607 : INFO : HTTP Request: POST http://195.242.24.252:8000/v1/chat/completions \"HTTP/1.1 200 OK\"\n",
      "\u001b[92m23:05:27 - LiteLLM:INFO\u001b[0m: utils.py:2975 - \n",
      "LiteLLM completion() model= open-mistral-nemo-2407; provider = mistral\n",
      "2024-09-05 23:05:27,615 : INFO : \n",
      "LiteLLM completion() model= open-mistral-nemo-2407; provider = mistral\n",
      "2024-09-05 23:05:27,675 : INFO : HTTP Request: POST http://195.242.24.252:8000/v1/chat/completions \"HTTP/1.1 429 Too Many Requests\"\n",
      "2024-09-05 23:05:27,676 : INFO : Retrying request to /chat/completions in 0.880277 seconds\n",
      "2024-09-05 23:05:28,330 : INFO : HTTP Request: POST http://195.242.24.252:8000/v1/chat/completions \"HTTP/1.1 500 Internal Server Error\"\n",
      "2024-09-05 23:05:28,331 : INFO : Retrying request to /chat/completions in 1.691389 seconds\n",
      "2024-09-05 23:05:28,381 : INFO : HTTP Request: POST http://195.242.24.252:8000/v1/chat/completions \"HTTP/1.1 500 Internal Server Error\"\n",
      "2024-09-05 23:05:28,383 : INFO : Retrying request to /chat/completions in 1.936831 seconds\n",
      "2024-09-05 23:05:28,390 : INFO : HTTP Request: POST http://195.242.24.252:8000/v1/chat/completions \"HTTP/1.1 500 Internal Server Error\"\n",
      "2024-09-05 23:05:28,392 : INFO : Retrying request to /chat/completions in 1.577588 seconds\n",
      "2024-09-05 23:05:28,475 : INFO : HTTP Request: POST http://195.242.24.252:8000/v1/chat/completions \"HTTP/1.1 500 Internal Server Error\"\n",
      "2024-09-05 23:05:28,479 : INFO : Retrying request to /chat/completions in 1.829893 seconds\n",
      "2024-09-05 23:05:28,571 : INFO : HTTP Request: POST http://195.242.24.252:8000/v1/chat/completions \"HTTP/1.1 500 Internal Server Error\"\n",
      "2024-09-05 23:05:28,573 : INFO : HTTP Request: POST http://195.242.24.252:8000/v1/chat/completions \"HTTP/1.1 500 Internal Server Error\"\n",
      "2024-09-05 23:05:28,574 : INFO : HTTP Request: POST http://195.242.24.252:8000/v1/chat/completions \"HTTP/1.1 500 Internal Server Error\"\n",
      "2024-09-05 23:05:28,576 : INFO : HTTP Request: POST http://195.242.24.252:8000/v1/chat/completions \"HTTP/1.1 500 Internal Server Error\"\n",
      "2024-09-05 23:05:28,577 : INFO : Retrying request to /chat/completions in 1.837287 seconds\n",
      "2024-09-05 23:05:28,578 : INFO : Retrying request to /chat/completions in 1.568383 seconds\n",
      "2024-09-05 23:05:28,579 : INFO : Retrying request to /chat/completions in 1.799139 seconds\n",
      "2024-09-05 23:05:28,579 : INFO : Retrying request to /chat/completions in 1.882536 seconds\n",
      "2024-09-05 23:05:28,615 : INFO : HTTP Request: POST http://195.242.24.252:8000/v1/chat/completions \"HTTP/1.1 429 Too Many Requests\"\n",
      "2024-09-05 23:05:28,619 : INFO : Retrying request to /chat/completions in 1.856262 seconds\n",
      "2024-09-05 23:05:30,033 : INFO : HTTP Request: POST http://195.242.24.252:8000/v1/chat/completions \"HTTP/1.1 429 Too Many Requests\"\n",
      "\u001b[92m23:05:30 - LiteLLM:INFO\u001b[0m: utils.py:2975 - \n",
      "LiteLLM completion() model= mistral-large-latest; provider = mistral\n",
      "2024-09-05 23:05:30,054 : INFO : \n",
      "LiteLLM completion() model= mistral-large-latest; provider = mistral\n",
      "2024-09-05 23:05:30,085 : INFO : HTTP Request: POST http://195.242.24.252:8000/v1/chat/completions \"HTTP/1.1 429 Too Many Requests\"\n",
      "\u001b[92m23:05:30 - LiteLLM:INFO\u001b[0m: utils.py:2975 - \n",
      "LiteLLM completion() model= mistral-large-latest; provider = mistral\n",
      "2024-09-05 23:05:30,094 : INFO : \n",
      "LiteLLM completion() model= mistral-large-latest; provider = mistral\n",
      "2024-09-05 23:05:30,117 : INFO : HTTP Request: POST http://195.242.24.252:8000/v1/chat/completions \"HTTP/1.1 429 Too Many Requests\"\n",
      "2024-09-05 23:05:30,118 : INFO : Retrying request to /chat/completions in 0.888099 seconds\n",
      "2024-09-05 23:05:30,155 : INFO : HTTP Request: POST http://195.242.24.252:8000/v1/chat/completions \"HTTP/1.1 429 Too Many Requests\"\n",
      "2024-09-05 23:05:30,156 : INFO : Retrying request to /chat/completions in 0.863482 seconds\n",
      "2024-09-05 23:05:30,217 : INFO : HTTP Request: POST http://195.242.24.252:8000/v1/chat/completions \"HTTP/1.1 429 Too Many Requests\"\n",
      "\u001b[92m23:05:30 - LiteLLM:INFO\u001b[0m: utils.py:2975 - \n",
      "LiteLLM completion() model= mistral-large-latest; provider = mistral\n",
      "2024-09-05 23:05:30,225 : INFO : \n",
      "LiteLLM completion() model= mistral-large-latest; provider = mistral\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "\u001b[1;31mGive Feedback / Get Help: https://github.com/BerriAI/litellm/issues/new\u001b[0m\n",
      "LiteLLM.Info: If you need to debug this error, use `litellm.set_verbose=True'.\n",
      "\n",
      "\n",
      "\u001b[1;31mGive Feedback / Get Help: https://github.com/BerriAI/litellm/issues/new\u001b[0m\n",
      "LiteLLM.Info: If you need to debug this error, use `litellm.set_verbose=True'.\n",
      "\n",
      "\n",
      "\u001b[1;31mGive Feedback / Get Help: https://github.com/BerriAI/litellm/issues/new\u001b[0m\n",
      "LiteLLM.Info: If you need to debug this error, use `litellm.set_verbose=True'.\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2024-09-05 23:05:30,285 : INFO : HTTP Request: POST http://195.242.24.252:8000/v1/chat/completions \"HTTP/1.1 429 Too Many Requests\"\n",
      "2024-09-05 23:05:30,291 : INFO : Retrying request to /chat/completions in 0.987926 seconds\n",
      "2024-09-05 23:05:30,369 : INFO : HTTP Request: POST http://195.242.24.252:8000/v1/chat/completions \"HTTP/1.1 429 Too Many Requests\"\n",
      "\u001b[92m23:05:30 - LiteLLM:INFO\u001b[0m: utils.py:2975 - \n",
      "LiteLLM completion() model= mistral-large-latest; provider = mistral\n",
      "2024-09-05 23:05:30,376 : INFO : \n",
      "LiteLLM completion() model= mistral-large-latest; provider = mistral\n",
      "2024-09-05 23:05:30,381 : INFO : HTTP Request: POST http://195.242.24.252:8000/v1/chat/completions \"HTTP/1.1 429 Too Many Requests\"\n",
      "\u001b[92m23:05:30 - LiteLLM:INFO\u001b[0m: utils.py:2975 - \n",
      "LiteLLM completion() model= mistral-large-latest; provider = mistral\n",
      "2024-09-05 23:05:30,389 : INFO : \n",
      "LiteLLM completion() model= mistral-large-latest; provider = mistral\n",
      "2024-09-05 23:05:30,432 : INFO : HTTP Request: POST http://195.242.24.252:8000/v1/chat/completions \"HTTP/1.1 429 Too Many Requests\"\n",
      "2024-09-05 23:05:30,433 : INFO : Retrying request to /chat/completions in 0.880084 seconds\n",
      "2024-09-05 23:05:30,435 : INFO : HTTP Request: POST http://195.242.24.252:8000/v1/chat/completions \"HTTP/1.1 429 Too Many Requests\"\n",
      "\u001b[92m23:05:30 - LiteLLM:INFO\u001b[0m: utils.py:2975 - \n",
      "LiteLLM completion() model= mistral-large-latest; provider = mistral\n",
      "2024-09-05 23:05:30,442 : INFO : \n",
      "LiteLLM completion() model= mistral-large-latest; provider = mistral\n",
      "2024-09-05 23:05:30,446 : INFO : HTTP Request: POST http://195.242.24.252:8000/v1/chat/completions \"HTTP/1.1 429 Too Many Requests\"\n",
      "2024-09-05 23:05:30,447 : INFO : Retrying request to /chat/completions in 0.949252 seconds\n",
      "2024-09-05 23:05:30,468 : INFO : HTTP Request: POST http://195.242.24.252:8000/v1/chat/completions \"HTTP/1.1 429 Too Many Requests\"\n",
      "\u001b[92m23:05:30 - LiteLLM:INFO\u001b[0m: utils.py:2975 - \n",
      "LiteLLM completion() model= mistral-large-latest; provider = mistral\n",
      "2024-09-05 23:05:30,474 : INFO : \n",
      "LiteLLM completion() model= mistral-large-latest; provider = mistral\n",
      "2024-09-05 23:05:30,496 : INFO : HTTP Request: POST http://195.242.24.252:8000/v1/chat/completions \"HTTP/1.1 429 Too Many Requests\"\n",
      "2024-09-05 23:05:30,497 : INFO : Retrying request to /chat/completions in 0.863049 seconds\n",
      "2024-09-05 23:05:30,516 : INFO : HTTP Request: POST http://195.242.24.252:8000/v1/chat/completions \"HTTP/1.1 429 Too Many Requests\"\n",
      "\u001b[92m23:05:30 - LiteLLM:INFO\u001b[0m: utils.py:2975 - \n",
      "LiteLLM completion() model= mistral-large-latest; provider = mistral\n",
      "2024-09-05 23:05:30,522 : INFO : \n",
      "LiteLLM completion() model= mistral-large-latest; provider = mistral\n",
      "2024-09-05 23:05:30,530 : INFO : HTTP Request: POST http://195.242.24.252:8000/v1/chat/completions \"HTTP/1.1 429 Too Many Requests\"\n",
      "2024-09-05 23:05:30,532 : INFO : Retrying request to /chat/completions in 0.985874 seconds\n",
      "2024-09-05 23:05:30,535 : INFO : HTTP Request: POST http://195.242.24.252:8000/v1/chat/completions \"HTTP/1.1 429 Too Many Requests\"\n",
      "\u001b[92m23:05:30 - LiteLLM:INFO\u001b[0m: utils.py:2975 - \n",
      "LiteLLM completion() model= open-mistral-nemo-2407; provider = mistral\n",
      "2024-09-05 23:05:30,542 : INFO : \n",
      "LiteLLM completion() model= open-mistral-nemo-2407; provider = mistral\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "\u001b[1;31mGive Feedback / Get Help: https://github.com/BerriAI/litellm/issues/new\u001b[0m\n",
      "LiteLLM.Info: If you need to debug this error, use `litellm.set_verbose=True'.\n",
      "\n",
      "\n",
      "\u001b[1;31mGive Feedback / Get Help: https://github.com/BerriAI/litellm/issues/new\u001b[0m\n",
      "LiteLLM.Info: If you need to debug this error, use `litellm.set_verbose=True'.\n",
      "\n",
      "\n",
      "\u001b[1;31mGive Feedback / Get Help: https://github.com/BerriAI/litellm/issues/new\u001b[0m\n",
      "LiteLLM.Info: If you need to debug this error, use `litellm.set_verbose=True'.\n",
      "\n",
      "\n",
      "\u001b[1;31mGive Feedback / Get Help: https://github.com/BerriAI/litellm/issues/new\u001b[0m\n",
      "LiteLLM.Info: If you need to debug this error, use `litellm.set_verbose=True'.\n",
      "\n",
      "\n",
      "\u001b[1;31mGive Feedback / Get Help: https://github.com/BerriAI/litellm/issues/new\u001b[0m\n",
      "LiteLLM.Info: If you need to debug this error, use `litellm.set_verbose=True'.\n",
      "\n",
      "\n",
      "\u001b[1;31mGive Feedback / Get Help: https://github.com/BerriAI/litellm/issues/new\u001b[0m\n",
      "LiteLLM.Info: If you need to debug this error, use `litellm.set_verbose=True'.\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2024-09-05 23:05:30,577 : INFO : HTTP Request: POST http://195.242.24.252:8000/v1/chat/completions \"HTTP/1.1 429 Too Many Requests\"\n",
      "2024-09-05 23:05:30,578 : INFO : Retrying request to /chat/completions in 0.916738 seconds\n",
      "2024-09-05 23:05:30,601 : INFO : HTTP Request: POST http://195.242.24.252:8000/v1/chat/completions \"HTTP/1.1 429 Too Many Requests\"\n",
      "2024-09-05 23:05:30,602 : INFO : Retrying request to /chat/completions in 0.964177 seconds\n",
      "2024-09-05 23:05:31,066 : INFO : HTTP Request: POST http://195.242.24.252:8000/v1/chat/completions \"HTTP/1.1 429 Too Many Requests\"\n",
      "2024-09-05 23:05:31,074 : INFO : Retrying request to /chat/completions in 1.530212 seconds\n",
      "2024-09-05 23:05:31,075 : INFO : HTTP Request: POST http://195.242.24.252:8000/v1/chat/completions \"HTTP/1.1 429 Too Many Requests\"\n",
      "2024-09-05 23:05:31,078 : INFO : Retrying request to /chat/completions in 1.892913 seconds\n",
      "2024-09-05 23:05:31,340 : INFO : HTTP Request: POST http://195.242.24.252:8000/v1/chat/completions \"HTTP/1.1 429 Too Many Requests\"\n",
      "2024-09-05 23:05:31,343 : INFO : Retrying request to /chat/completions in 1.645183 seconds\n",
      "2024-09-05 23:05:31,368 : INFO : HTTP Request: POST http://195.242.24.252:8000/v1/chat/completions \"HTTP/1.1 429 Too Many Requests\"\n",
      "2024-09-05 23:05:31,370 : INFO : Retrying request to /chat/completions in 1.987628 seconds\n",
      "2024-09-05 23:05:31,413 : INFO : HTTP Request: POST http://195.242.24.252:8000/v1/chat/completions \"HTTP/1.1 429 Too Many Requests\"\n",
      "2024-09-05 23:05:31,414 : INFO : Retrying request to /chat/completions in 1.828338 seconds\n",
      "2024-09-05 23:05:31,454 : INFO : HTTP Request: POST http://195.242.24.252:8000/v1/chat/completions \"HTTP/1.1 429 Too Many Requests\"\n",
      "2024-09-05 23:05:31,455 : INFO : Retrying request to /chat/completions in 1.568512 seconds\n",
      "2024-09-05 23:05:31,548 : INFO : HTTP Request: POST http://195.242.24.252:8000/v1/chat/completions \"HTTP/1.1 429 Too Many Requests\"\n",
      "2024-09-05 23:05:31,550 : INFO : Retrying request to /chat/completions in 1.895402 seconds\n",
      "2024-09-05 23:05:31,572 : INFO : HTTP Request: POST http://195.242.24.252:8000/v1/chat/completions \"HTTP/1.1 429 Too Many Requests\"\n",
      "2024-09-05 23:05:31,573 : INFO : Retrying request to /chat/completions in 1.601990 seconds\n",
      "2024-09-05 23:05:31,621 : INFO : HTTP Request: POST http://195.242.24.252:8000/v1/chat/completions \"HTTP/1.1 429 Too Many Requests\"\n",
      "2024-09-05 23:05:31,622 : INFO : Retrying request to /chat/completions in 1.663643 seconds\n",
      "2024-09-05 23:05:32,667 : INFO : HTTP Request: POST http://195.242.24.252:8000/v1/chat/completions \"HTTP/1.1 429 Too Many Requests\"\n",
      "\u001b[92m23:05:32 - LiteLLM:INFO\u001b[0m: utils.py:2975 - \n",
      "LiteLLM completion() model= mistral-large-latest; provider = mistral\n",
      "2024-09-05 23:05:32,687 : INFO : \n",
      "LiteLLM completion() model= mistral-large-latest; provider = mistral\n",
      "2024-09-05 23:05:32,748 : INFO : HTTP Request: POST http://195.242.24.252:8000/v1/chat/completions \"HTTP/1.1 429 Too Many Requests\"\n",
      "2024-09-05 23:05:32,749 : INFO : Retrying request to /chat/completions in 0.962287 seconds\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "\u001b[1;31mGive Feedback / Get Help: https://github.com/BerriAI/litellm/issues/new\u001b[0m\n",
      "LiteLLM.Info: If you need to debug this error, use `litellm.set_verbose=True'.\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2024-09-05 23:05:33,031 : INFO : HTTP Request: POST http://195.242.24.252:8000/v1/chat/completions \"HTTP/1.1 429 Too Many Requests\"\n",
      "\u001b[92m23:05:33 - LiteLLM:INFO\u001b[0m: utils.py:2975 - \n",
      "LiteLLM completion() model= mistral-large-latest; provider = mistral\n",
      "2024-09-05 23:05:33,047 : INFO : HTTP Request: POST http://195.242.24.252:8000/v1/chat/completions \"HTTP/1.1 429 Too Many Requests\"\n",
      "2024-09-05 23:05:33,046 : INFO : \n",
      "LiteLLM completion() model= mistral-large-latest; provider = mistral\n",
      "\u001b[92m23:05:33 - LiteLLM:INFO\u001b[0m: utils.py:2975 - \n",
      "LiteLLM completion() model= mistral-large-latest; provider = mistral\n",
      "2024-09-05 23:05:33,060 : INFO : \n",
      "LiteLLM completion() model= mistral-large-latest; provider = mistral\n",
      "2024-09-05 23:05:33,085 : INFO : HTTP Request: POST http://195.242.24.252:8000/v1/chat/completions \"HTTP/1.1 429 Too Many Requests\"\n",
      "\u001b[92m23:05:33 - LiteLLM:INFO\u001b[0m: utils.py:2975 - \n",
      "LiteLLM completion() model= mistral-large-latest; provider = mistral\n",
      "2024-09-05 23:05:33,093 : INFO : \n",
      "LiteLLM completion() model= mistral-large-latest; provider = mistral\n",
      "2024-09-05 23:05:33,124 : INFO : HTTP Request: POST http://195.242.24.252:8000/v1/chat/completions \"HTTP/1.1 429 Too Many Requests\"\n",
      "2024-09-05 23:05:33,125 : INFO : Retrying request to /chat/completions in 0.953899 seconds\n",
      "2024-09-05 23:05:33,126 : INFO : HTTP Request: POST http://195.242.24.252:8000/v1/chat/completions \"HTTP/1.1 429 Too Many Requests\"\n",
      "2024-09-05 23:05:33,126 : INFO : Retrying request to /chat/completions in 0.903914 seconds\n",
      "2024-09-05 23:05:33,151 : INFO : HTTP Request: POST http://195.242.24.252:8000/v1/chat/completions \"HTTP/1.1 429 Too Many Requests\"\n",
      "2024-09-05 23:05:33,153 : INFO : Retrying request to /chat/completions in 0.804276 seconds\n",
      "2024-09-05 23:05:33,227 : INFO : HTTP Request: POST http://195.242.24.252:8000/v1/chat/completions \"HTTP/1.1 429 Too Many Requests\"\n",
      "\u001b[92m23:05:33 - LiteLLM:INFO\u001b[0m: utils.py:2975 - \n",
      "LiteLLM completion() model= mistral-large-latest; provider = mistral\n",
      "2024-09-05 23:05:33,236 : INFO : \n",
      "LiteLLM completion() model= mistral-large-latest; provider = mistral\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "\u001b[1;31mGive Feedback / Get Help: https://github.com/BerriAI/litellm/issues/new\u001b[0m\n",
      "LiteLLM.Info: If you need to debug this error, use `litellm.set_verbose=True'.\n",
      "\n",
      "\n",
      "\u001b[1;31mGive Feedback / Get Help: https://github.com/BerriAI/litellm/issues/new\u001b[0m\n",
      "LiteLLM.Info: If you need to debug this error, use `litellm.set_verbose=True'.\n",
      "\n",
      "\n",
      "\u001b[1;31mGive Feedback / Get Help: https://github.com/BerriAI/litellm/issues/new\u001b[0m\n",
      "LiteLLM.Info: If you need to debug this error, use `litellm.set_verbose=True'.\n",
      "\n",
      "\n",
      "\u001b[1;31mGive Feedback / Get Help: https://github.com/BerriAI/litellm/issues/new\u001b[0m\n",
      "LiteLLM.Info: If you need to debug this error, use `litellm.set_verbose=True'.\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2024-09-05 23:05:33,293 : INFO : HTTP Request: POST http://195.242.24.252:8000/v1/chat/completions \"HTTP/1.1 429 Too Many Requests\"\n",
      "2024-09-05 23:05:33,294 : INFO : Retrying request to /chat/completions in 0.785657 seconds\n",
      "2024-09-05 23:05:33,298 : INFO : HTTP Request: POST http://195.242.24.252:8000/v1/chat/completions \"HTTP/1.1 429 Too Many Requests\"\n",
      "\u001b[92m23:05:33 - LiteLLM:INFO\u001b[0m: utils.py:2975 - \n",
      "LiteLLM completion() model= mistral-large-latest; provider = mistral\n",
      "2024-09-05 23:05:33,307 : INFO : \n",
      "LiteLLM completion() model= mistral-large-latest; provider = mistral\n",
      "2024-09-05 23:05:33,341 : INFO : HTTP Request: POST http://195.242.24.252:8000/v1/chat/completions \"HTTP/1.1 429 Too Many Requests\"\n",
      "\u001b[92m23:05:33 - LiteLLM:INFO\u001b[0m: utils.py:2975 - \n",
      "LiteLLM completion() model= open-mistral-nemo-2407; provider = mistral\n",
      "2024-09-05 23:05:33,350 : INFO : \n",
      "LiteLLM completion() model= open-mistral-nemo-2407; provider = mistral\n",
      "2024-09-05 23:05:33,364 : INFO : HTTP Request: POST http://195.242.24.252:8000/v1/chat/completions \"HTTP/1.1 429 Too Many Requests\"\n",
      "2024-09-05 23:05:33,365 : INFO : Retrying request to /chat/completions in 0.894818 seconds\n",
      "2024-09-05 23:05:33,406 : INFO : HTTP Request: POST http://195.242.24.252:8000/v1/chat/completions \"HTTP/1.1 429 Too Many Requests\"\n",
      "2024-09-05 23:05:33,407 : INFO : Retrying request to /chat/completions in 0.867338 seconds\n",
      "2024-09-05 23:05:33,412 : INFO : HTTP Request: POST http://195.242.24.252:8000/v1/chat/completions \"HTTP/1.1 429 Too Many Requests\"\n",
      "\u001b[92m23:05:33 - LiteLLM:INFO\u001b[0m: utils.py:2975 - \n",
      "LiteLLM completion() model= mistral-large-latest; provider = mistral\n",
      "2024-09-05 23:05:33,419 : INFO : \n",
      "LiteLLM completion() model= mistral-large-latest; provider = mistral\n",
      "2024-09-05 23:05:33,478 : INFO : HTTP Request: POST http://195.242.24.252:8000/v1/chat/completions \"HTTP/1.1 429 Too Many Requests\"\n",
      "2024-09-05 23:05:33,479 : INFO : Retrying request to /chat/completions in 0.760683 seconds\n",
      "2024-09-05 23:05:33,496 : INFO : HTTP Request: POST http://195.242.24.252:8000/v1/chat/completions \"HTTP/1.1 429 Too Many Requests\"\n",
      "\u001b[92m23:05:33 - LiteLLM:INFO\u001b[0m: utils.py:2975 - \n",
      "LiteLLM completion() model= mistral-large-latest; provider = mistral\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "\u001b[1;31mGive Feedback / Get Help: https://github.com/BerriAI/litellm/issues/new\u001b[0m\n",
      "LiteLLM.Info: If you need to debug this error, use `litellm.set_verbose=True'.\n",
      "\n",
      "\n",
      "\u001b[1;31mGive Feedback / Get Help: https://github.com/BerriAI/litellm/issues/new\u001b[0m\n",
      "LiteLLM.Info: If you need to debug this error, use `litellm.set_verbose=True'.\n",
      "\n",
      "\n",
      "\u001b[1;31mGive Feedback / Get Help: https://github.com/BerriAI/litellm/issues/new\u001b[0m\n",
      "LiteLLM.Info: If you need to debug this error, use `litellm.set_verbose=True'.\n",
      "\n",
      "\n",
      "\u001b[1;31mGive Feedback / Get Help: https://github.com/BerriAI/litellm/issues/new\u001b[0m\n",
      "LiteLLM.Info: If you need to debug this error, use `litellm.set_verbose=True'.\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2024-09-05 23:05:33,504 : INFO : \n",
      "LiteLLM completion() model= mistral-large-latest; provider = mistral\n",
      "2024-09-05 23:05:33,584 : INFO : HTTP Request: POST http://195.242.24.252:8000/v1/chat/completions \"HTTP/1.1 429 Too Many Requests\"\n",
      "2024-09-05 23:05:33,586 : INFO : Retrying request to /chat/completions in 0.919296 seconds\n",
      "2024-09-05 23:05:33,766 : INFO : HTTP Request: POST http://195.242.24.252:8000/v1/chat/completions \"HTTP/1.1 429 Too Many Requests\"\n",
      "2024-09-05 23:05:33,768 : INFO : Retrying request to /chat/completions in 1.919040 seconds\n",
      "2024-09-05 23:05:34,011 : INFO : HTTP Request: POST http://195.242.24.252:8000/v1/chat/completions \"HTTP/1.1 429 Too Many Requests\"\n",
      "2024-09-05 23:05:34,013 : INFO : Retrying request to /chat/completions in 1.892414 seconds\n",
      "2024-09-05 23:05:34,087 : INFO : HTTP Request: POST http://195.242.24.252:8000/v1/chat/completions \"HTTP/1.1 429 Too Many Requests\"\n",
      "2024-09-05 23:05:34,088 : INFO : Retrying request to /chat/completions in 1.515055 seconds\n",
      "2024-09-05 23:05:34,135 : INFO : HTTP Request: POST http://195.242.24.252:8000/v1/chat/completions \"HTTP/1.1 429 Too Many Requests\"\n",
      "2024-09-05 23:05:34,136 : INFO : Retrying request to /chat/completions in 1.648645 seconds\n",
      "2024-09-05 23:05:34,137 : INFO : HTTP Request: POST http://195.242.24.252:8000/v1/chat/completions \"HTTP/1.1 429 Too Many Requests\"\n",
      "2024-09-05 23:05:34,138 : INFO : Retrying request to /chat/completions in 1.868700 seconds\n",
      "2024-09-05 23:05:34,297 : INFO : HTTP Request: POST http://195.242.24.252:8000/v1/chat/completions \"HTTP/1.1 429 Too Many Requests\"\n",
      "2024-09-05 23:05:34,298 : INFO : Retrying request to /chat/completions in 1.777785 seconds\n",
      "2024-09-05 23:05:34,314 : INFO : HTTP Request: POST http://195.242.24.252:8000/v1/chat/completions \"HTTP/1.1 429 Too Many Requests\"\n",
      "2024-09-05 23:05:34,315 : INFO : Retrying request to /chat/completions in 1.916969 seconds\n",
      "2024-09-05 23:05:34,328 : INFO : HTTP Request: POST http://195.242.24.252:8000/v1/chat/completions \"HTTP/1.1 429 Too Many Requests\"\n",
      "2024-09-05 23:05:34,330 : INFO : Retrying request to /chat/completions in 1.692741 seconds\n",
      "2024-09-05 23:05:34,562 : INFO : HTTP Request: POST http://195.242.24.252:8000/v1/chat/completions \"HTTP/1.1 429 Too Many Requests\"\n",
      "2024-09-05 23:05:34,563 : INFO : Retrying request to /chat/completions in 1.725361 seconds\n",
      "2024-09-05 23:05:35,657 : INFO : HTTP Request: POST http://195.242.24.252:8000/v1/chat/completions \"HTTP/1.1 429 Too Many Requests\"\n",
      "2024-09-05 23:05:35,665 : ERROR : Failed after retries: <bound method Future.exception of <Future at 0x33d366990 state=finished raised RateLimitError>>\n",
      "Traceback (most recent call last):\n",
      "  File \"/Users/tcapelle/miniforge3/envs/weave/lib/python3.11/site-packages/litellm/llms/openai.py\", line 1089, in acompletion\n",
      "    headers, response = await self.make_openai_chat_completion_request(\n",
      "                        ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/Users/tcapelle/miniforge3/envs/weave/lib/python3.11/site-packages/litellm/llms/openai.py\", line 793, in make_openai_chat_completion_request\n",
      "    raise e\n",
      "  File \"/Users/tcapelle/miniforge3/envs/weave/lib/python3.11/site-packages/litellm/llms/openai.py\", line 781, in make_openai_chat_completion_request\n",
      "    await openai_aclient.chat.completions.with_raw_response.create(\n",
      "  File \"/Users/tcapelle/miniforge3/envs/weave/lib/python3.11/site-packages/openai/_legacy_response.py\", line 367, in wrapped\n",
      "    return cast(LegacyAPIResponse[R], await func(*args, **kwargs))\n",
      "                                      ^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/Users/tcapelle/miniforge3/envs/weave/lib/python3.11/site-packages/weave/trace/op.py\", line 333, in wrapper\n",
      "    res, _ = await _execute_call(wrapper, call, *args, **kwargs)  # type: ignore\n",
      "             ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/Users/tcapelle/miniforge3/envs/weave/lib/python3.11/site-packages/weave/trace/op.py\", line 213, in _call_async\n",
      "    return handle_exception(e)\n",
      "           ^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/Users/tcapelle/miniforge3/envs/weave/lib/python3.11/site-packages/weave/trace/op.py\", line 211, in _call_async\n",
      "    res = await func(*args, **kwargs)\n",
      "          ^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/Users/tcapelle/miniforge3/envs/weave/lib/python3.11/site-packages/weave/integrations/openai/openai_sdk.py\", line 288, in _wrapper\n",
      "    return await fn(*args, **kwargs)\n",
      "           ^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/Users/tcapelle/miniforge3/envs/weave/lib/python3.11/site-packages/openai/resources/chat/completions.py\", line 1339, in create\n",
      "    return await self._post(\n",
      "           ^^^^^^^^^^^^^^^^^\n",
      "  File \"/Users/tcapelle/miniforge3/envs/weave/lib/python3.11/site-packages/openai/_base_client.py\", line 1816, in post\n",
      "    return await self.request(cast_to, opts, stream=stream, stream_cls=stream_cls)\n",
      "           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/Users/tcapelle/miniforge3/envs/weave/lib/python3.11/site-packages/openai/_base_client.py\", line 1510, in request\n",
      "    return await self._request(\n",
      "           ^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/Users/tcapelle/miniforge3/envs/weave/lib/python3.11/site-packages/openai/_base_client.py\", line 1596, in _request\n",
      "    return await self._retry_request(\n",
      "           ^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/Users/tcapelle/miniforge3/envs/weave/lib/python3.11/site-packages/openai/_base_client.py\", line 1643, in _retry_request\n",
      "    return await self._request(\n",
      "           ^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/Users/tcapelle/miniforge3/envs/weave/lib/python3.11/site-packages/openai/_base_client.py\", line 1596, in _request\n",
      "    return await self._retry_request(\n",
      "           ^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/Users/tcapelle/miniforge3/envs/weave/lib/python3.11/site-packages/openai/_base_client.py\", line 1643, in _retry_request\n",
      "    return await self._request(\n",
      "           ^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/Users/tcapelle/miniforge3/envs/weave/lib/python3.11/site-packages/openai/_base_client.py\", line 1611, in _request\n",
      "    raise self._make_status_error_from_response(err.response) from None\n",
      "openai.RateLimitError: Error code: 429 - {'error': {'message': 'Rate limit exceeded. Please try again later.', 'type': 'rate_limit_error', 'param': None, 'code': 'rate_limit_exceeded'}}\n",
      "\n",
      "During handling of the above exception, another exception occurred:\n",
      "\n",
      "Traceback (most recent call last):\n",
      "  File \"/Users/tcapelle/miniforge3/envs/weave/lib/python3.11/site-packages/litellm/main.py\", line 427, in acompletion\n",
      "    response = await init_response\n",
      "               ^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/Users/tcapelle/miniforge3/envs/weave/lib/python3.11/site-packages/litellm/llms/openai.py\", line 1136, in acompletion\n",
      "    raise OpenAIError(\n",
      "litellm.llms.openai.OpenAIError: Error code: 429 - {'error': {'message': 'Rate limit exceeded. Please try again later.', 'type': 'rate_limit_error', 'param': None, 'code': 'rate_limit_exceeded'}}\n",
      "\n",
      "During handling of the above exception, another exception occurred:\n",
      "\n",
      "Traceback (most recent call last):\n",
      "  File \"/Users/tcapelle/miniforge3/envs/weave/lib/python3.11/site-packages/instructor/retry.py\", line 239, in retry_async\n",
      "    response: ChatCompletion = await func(*args, **kwargs)\n",
      "                               ^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/Users/tcapelle/miniforge3/envs/weave/lib/python3.11/site-packages/weave/trace/op.py\", line 333, in wrapper\n",
      "    res, _ = await _execute_call(wrapper, call, *args, **kwargs)  # type: ignore\n",
      "             ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/Users/tcapelle/miniforge3/envs/weave/lib/python3.11/site-packages/weave/trace/op.py\", line 213, in _call_async\n",
      "    return handle_exception(e)\n",
      "           ^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/Users/tcapelle/miniforge3/envs/weave/lib/python3.11/site-packages/weave/trace/op.py\", line 211, in _call_async\n",
      "    res = await func(*args, **kwargs)\n",
      "          ^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/Users/tcapelle/miniforge3/envs/weave/lib/python3.11/site-packages/litellm/utils.py\", line 1593, in wrapper_async\n",
      "    raise e\n",
      "  File \"/Users/tcapelle/miniforge3/envs/weave/lib/python3.11/site-packages/litellm/utils.py\", line 1413, in wrapper_async\n",
      "    result = await original_function(*args, **kwargs)\n",
      "             ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/Users/tcapelle/miniforge3/envs/weave/lib/python3.11/site-packages/litellm/main.py\", line 449, in acompletion\n",
      "    raise exception_type(\n",
      "          ^^^^^^^^^^^^^^^\n",
      "  File \"/Users/tcapelle/miniforge3/envs/weave/lib/python3.11/site-packages/litellm/utils.py\", line 8187, in exception_type\n",
      "    raise e\n",
      "  File \"/Users/tcapelle/miniforge3/envs/weave/lib/python3.11/site-packages/litellm/utils.py\", line 6555, in exception_type\n",
      "    raise RateLimitError(\n",
      "litellm.exceptions.RateLimitError: litellm.RateLimitError: RateLimitError: MistralException - Error code: 429 - {'error': {'message': 'Rate limit exceeded. Please try again later.', 'type': 'rate_limit_error', 'param': None, 'code': 'rate_limit_exceeded'}}\n",
      "\n",
      "The above exception was the direct cause of the following exception:\n",
      "\n",
      "Traceback (most recent call last):\n",
      "  File \"/Users/tcapelle/miniforge3/envs/weave/lib/python3.11/site-packages/instructor/retry.py\", line 235, in retry_async\n",
      "    async for attempt in max_retries:\n",
      "  File \"/Users/tcapelle/miniforge3/envs/weave/lib/python3.11/site-packages/tenacity/asyncio/__init__.py\", line 166, in __anext__\n",
      "    do = await self.iter(retry_state=self._retry_state)\n",
      "         ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/Users/tcapelle/miniforge3/envs/weave/lib/python3.11/site-packages/tenacity/asyncio/__init__.py\", line 153, in iter\n",
      "    result = await action(retry_state)\n",
      "             ^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/Users/tcapelle/miniforge3/envs/weave/lib/python3.11/site-packages/tenacity/_utils.py\", line 99, in inner\n",
      "    return call(*args, **kwargs)\n",
      "           ^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/Users/tcapelle/miniforge3/envs/weave/lib/python3.11/site-packages/tenacity/__init__.py\", line 419, in exc_check\n",
      "    raise retry_exc from fut.exception()\n",
      "tenacity.RetryError: RetryError[<Future at 0x33d366990 state=finished raised RateLimitError>]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "\u001b[1;31mGive Feedback / Get Help: https://github.com/BerriAI/litellm/issues/new\u001b[0m\n",
      "LiteLLM.Info: If you need to debug this error, use `litellm.set_verbose=True'.\n",
      "\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\">model_output failed\n",
       "</pre>\n"
      ],
      "text/plain": [
       "model_output failed\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Traceback (most recent call last):\n",
      "  File \"/Users/tcapelle/miniforge3/envs/weave/lib/python3.11/site-packages/litellm/llms/openai.py\", line 1089, in acompletion\n",
      "    headers, response = await self.make_openai_chat_completion_request(\n",
      "                        ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/Users/tcapelle/miniforge3/envs/weave/lib/python3.11/site-packages/litellm/llms/openai.py\", line 793, in make_openai_chat_completion_request\n",
      "    raise e\n",
      "  File \"/Users/tcapelle/miniforge3/envs/weave/lib/python3.11/site-packages/litellm/llms/openai.py\", line 781, in make_openai_chat_completion_request\n",
      "    await openai_aclient.chat.completions.with_raw_response.create(\n",
      "  File \"/Users/tcapelle/miniforge3/envs/weave/lib/python3.11/site-packages/openai/_legacy_response.py\", line 367, in wrapped\n",
      "    return cast(LegacyAPIResponse[R], await func(*args, **kwargs))\n",
      "                                      ^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/Users/tcapelle/miniforge3/envs/weave/lib/python3.11/site-packages/weave/trace/op.py\", line 333, in wrapper\n",
      "    res, _ = await _execute_call(wrapper, call, *args, **kwargs)  # type: ignore\n",
      "             ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/Users/tcapelle/miniforge3/envs/weave/lib/python3.11/site-packages/weave/trace/op.py\", line 213, in _call_async\n",
      "    return handle_exception(e)\n",
      "           ^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/Users/tcapelle/miniforge3/envs/weave/lib/python3.11/site-packages/weave/trace/op.py\", line 211, in _call_async\n",
      "    res = await func(*args, **kwargs)\n",
      "          ^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/Users/tcapelle/miniforge3/envs/weave/lib/python3.11/site-packages/weave/integrations/openai/openai_sdk.py\", line 288, in _wrapper\n",
      "    return await fn(*args, **kwargs)\n",
      "           ^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/Users/tcapelle/miniforge3/envs/weave/lib/python3.11/site-packages/openai/resources/chat/completions.py\", line 1339, in create\n",
      "    return await self._post(\n",
      "           ^^^^^^^^^^^^^^^^^\n",
      "  File \"/Users/tcapelle/miniforge3/envs/weave/lib/python3.11/site-packages/openai/_base_client.py\", line 1816, in post\n",
      "    return await self.request(cast_to, opts, stream=stream, stream_cls=stream_cls)\n",
      "           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/Users/tcapelle/miniforge3/envs/weave/lib/python3.11/site-packages/openai/_base_client.py\", line 1510, in request\n",
      "    return await self._request(\n",
      "           ^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/Users/tcapelle/miniforge3/envs/weave/lib/python3.11/site-packages/openai/_base_client.py\", line 1596, in _request\n",
      "    return await self._retry_request(\n",
      "           ^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/Users/tcapelle/miniforge3/envs/weave/lib/python3.11/site-packages/openai/_base_client.py\", line 1643, in _retry_request\n",
      "    return await self._request(\n",
      "           ^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/Users/tcapelle/miniforge3/envs/weave/lib/python3.11/site-packages/openai/_base_client.py\", line 1596, in _request\n",
      "    return await self._retry_request(\n",
      "           ^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/Users/tcapelle/miniforge3/envs/weave/lib/python3.11/site-packages/openai/_base_client.py\", line 1643, in _retry_request\n",
      "    return await self._request(\n",
      "           ^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/Users/tcapelle/miniforge3/envs/weave/lib/python3.11/site-packages/openai/_base_client.py\", line 1611, in _request\n",
      "    raise self._make_status_error_from_response(err.response) from None\n",
      "openai.RateLimitError: Error code: 429 - {'error': {'message': 'Rate limit exceeded. Please try again later.', 'type': 'rate_limit_error', 'param': None, 'code': 'rate_limit_exceeded'}}\n",
      "\n",
      "During handling of the above exception, another exception occurred:\n",
      "\n",
      "Traceback (most recent call last):\n",
      "  File \"/Users/tcapelle/miniforge3/envs/weave/lib/python3.11/site-packages/litellm/main.py\", line 427, in acompletion\n",
      "    response = await init_response\n",
      "               ^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/Users/tcapelle/miniforge3/envs/weave/lib/python3.11/site-packages/litellm/llms/openai.py\", line 1136, in acompletion\n",
      "    raise OpenAIError(\n",
      "litellm.llms.openai.OpenAIError: Error code: 429 - {'error': {'message': 'Rate limit exceeded. Please try again later.', 'type': 'rate_limit_error', 'param': None, 'code': 'rate_limit_exceeded'}}\n",
      "\n",
      "During handling of the above exception, another exception occurred:\n",
      "\n",
      "Traceback (most recent call last):\n",
      "  File \"/Users/tcapelle/miniforge3/envs/weave/lib/python3.11/site-packages/instructor/retry.py\", line 239, in retry_async\n",
      "    response: ChatCompletion = await func(*args, **kwargs)\n",
      "                               ^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/Users/tcapelle/miniforge3/envs/weave/lib/python3.11/site-packages/weave/trace/op.py\", line 333, in wrapper\n",
      "    res, _ = await _execute_call(wrapper, call, *args, **kwargs)  # type: ignore\n",
      "             ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/Users/tcapelle/miniforge3/envs/weave/lib/python3.11/site-packages/weave/trace/op.py\", line 213, in _call_async\n",
      "    return handle_exception(e)\n",
      "           ^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/Users/tcapelle/miniforge3/envs/weave/lib/python3.11/site-packages/weave/trace/op.py\", line 211, in _call_async\n",
      "    res = await func(*args, **kwargs)\n",
      "          ^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/Users/tcapelle/miniforge3/envs/weave/lib/python3.11/site-packages/litellm/utils.py\", line 1593, in wrapper_async\n",
      "    raise e\n",
      "  File \"/Users/tcapelle/miniforge3/envs/weave/lib/python3.11/site-packages/litellm/utils.py\", line 1413, in wrapper_async\n",
      "    result = await original_function(*args, **kwargs)\n",
      "             ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/Users/tcapelle/miniforge3/envs/weave/lib/python3.11/site-packages/litellm/main.py\", line 449, in acompletion\n",
      "    raise exception_type(\n",
      "          ^^^^^^^^^^^^^^^\n",
      "  File \"/Users/tcapelle/miniforge3/envs/weave/lib/python3.11/site-packages/litellm/utils.py\", line 8187, in exception_type\n",
      "    raise e\n",
      "  File \"/Users/tcapelle/miniforge3/envs/weave/lib/python3.11/site-packages/litellm/utils.py\", line 6555, in exception_type\n",
      "    raise RateLimitError(\n",
      "litellm.exceptions.RateLimitError: litellm.RateLimitError: RateLimitError: MistralException - Error code: 429 - {'error': {'message': 'Rate limit exceeded. Please try again later.', 'type': 'rate_limit_error', 'param': None, 'code': 'rate_limit_exceeded'}}\n",
      "\n",
      "The above exception was the direct cause of the following exception:\n",
      "\n",
      "Traceback (most recent call last):\n",
      "  File \"/Users/tcapelle/miniforge3/envs/weave/lib/python3.11/site-packages/instructor/retry.py\", line 235, in retry_async\n",
      "    async for attempt in max_retries:\n",
      "  File \"/Users/tcapelle/miniforge3/envs/weave/lib/python3.11/site-packages/tenacity/asyncio/__init__.py\", line 166, in __anext__\n",
      "    do = await self.iter(retry_state=self._retry_state)\n",
      "         ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/Users/tcapelle/miniforge3/envs/weave/lib/python3.11/site-packages/tenacity/asyncio/__init__.py\", line 153, in iter\n",
      "    result = await action(retry_state)\n",
      "             ^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/Users/tcapelle/miniforge3/envs/weave/lib/python3.11/site-packages/tenacity/_utils.py\", line 99, in inner\n",
      "    return call(*args, **kwargs)\n",
      "           ^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/Users/tcapelle/miniforge3/envs/weave/lib/python3.11/site-packages/tenacity/__init__.py\", line 419, in exc_check\n",
      "    raise retry_exc from fut.exception()\n",
      "tenacity.RetryError: RetryError[<Future at 0x33d366990 state=finished raised RateLimitError>]\n",
      "\n",
      "The above exception was the direct cause of the following exception:\n",
      "\n",
      "Traceback (most recent call last):\n",
      "  File \"/Users/tcapelle/miniforge3/envs/weave/lib/python3.11/site-packages/weave/flow/eval.py\", line 168, in predict_and_score\n",
      "    model_output = await async_call(model_predict, **model_predict_args)\n",
      "                   ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/Users/tcapelle/miniforge3/envs/weave/lib/python3.11/site-packages/weave/trace/op.py\", line 333, in wrapper\n",
      "    res, _ = await _execute_call(wrapper, call, *args, **kwargs)  # type: ignore\n",
      "             ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/Users/tcapelle/miniforge3/envs/weave/lib/python3.11/site-packages/weave/trace/op.py\", line 213, in _call_async\n",
      "    return handle_exception(e)\n",
      "           ^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/Users/tcapelle/miniforge3/envs/weave/lib/python3.11/site-packages/weave/trace/op.py\", line 211, in _call_async\n",
      "    res = await func(*args, **kwargs)\n",
      "          ^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/var/folders/sf/tgv7vcv96x557p38bvvp1ms40000gn/T/ipykernel_6554/970167077.py\", line 10, in predict\n",
      "    return await rag_solver_with_reflection(\n",
      "           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/Users/tcapelle/miniforge3/envs/weave/lib/python3.11/site-packages/weave/trace/op.py\", line 333, in wrapper\n",
      "    res, _ = await _execute_call(wrapper, call, *args, **kwargs)  # type: ignore\n",
      "             ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/Users/tcapelle/miniforge3/envs/weave/lib/python3.11/site-packages/weave/trace/op.py\", line 213, in _call_async\n",
      "    return handle_exception(e)\n",
      "           ^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/Users/tcapelle/miniforge3/envs/weave/lib/python3.11/site-packages/weave/trace/op.py\", line 211, in _call_async\n",
      "    res = await func(*args, **kwargs)\n",
      "          ^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/var/folders/sf/tgv7vcv96x557p38bvvp1ms40000gn/T/ipykernel_6554/4070537091.py\", line 14, in rag_solver_with_reflection\n",
      "    rag_result = await rag_solver(\n",
      "                 ^^^^^^^^^^^^^^^^^\n",
      "  File \"/Users/tcapelle/miniforge3/envs/weave/lib/python3.11/site-packages/weave/trace/op.py\", line 333, in wrapper\n",
      "    res, _ = await _execute_call(wrapper, call, *args, **kwargs)  # type: ignore\n",
      "             ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/Users/tcapelle/miniforge3/envs/weave/lib/python3.11/site-packages/weave/trace/op.py\", line 213, in _call_async\n",
      "    return handle_exception(e)\n",
      "           ^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/Users/tcapelle/miniforge3/envs/weave/lib/python3.11/site-packages/weave/trace/op.py\", line 211, in _call_async\n",
      "    res = await func(*args, **kwargs)\n",
      "          ^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/Users/tcapelle/work/hackercup_rag/agent.py\", line 350, in rag_solver\n",
      "    zero_shot_result = await zero_shot_solver(\n",
      "                       ^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/Users/tcapelle/miniforge3/envs/weave/lib/python3.11/site-packages/weave/trace/op.py\", line 333, in wrapper\n",
      "    res, _ = await _execute_call(wrapper, call, *args, **kwargs)  # type: ignore\n",
      "             ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/Users/tcapelle/miniforge3/envs/weave/lib/python3.11/site-packages/weave/trace/op.py\", line 213, in _call_async\n",
      "    return handle_exception(e)\n",
      "           ^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/Users/tcapelle/miniforge3/envs/weave/lib/python3.11/site-packages/weave/trace/op.py\", line 211, in _call_async\n",
      "    res = await func(*args, **kwargs)\n",
      "          ^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/Users/tcapelle/work/hackercup_rag/agent.py\", line 330, in zero_shot_solver\n",
      "    solution = await draft_solution(\n",
      "               ^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/Users/tcapelle/miniforge3/envs/weave/lib/python3.11/site-packages/weave/trace/op.py\", line 333, in wrapper\n",
      "    res, _ = await _execute_call(wrapper, call, *args, **kwargs)  # type: ignore\n",
      "             ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/Users/tcapelle/miniforge3/envs/weave/lib/python3.11/site-packages/weave/trace/op.py\", line 213, in _call_async\n",
      "    return handle_exception(e)\n",
      "           ^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/Users/tcapelle/miniforge3/envs/weave/lib/python3.11/site-packages/weave/trace/op.py\", line 211, in _call_async\n",
      "    res = await func(*args, **kwargs)\n",
      "          ^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/Users/tcapelle/work/hackercup_rag/agent.py\", line 83, in draft_solution\n",
      "    response = await async_client.chat.completions.create(\n",
      "               ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/Users/tcapelle/miniforge3/envs/weave/lib/python3.11/site-packages/instructor/client.py\", line 300, in create\n",
      "    return await self.create_fn(\n",
      "           ^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/Users/tcapelle/miniforge3/envs/weave/lib/python3.11/site-packages/instructor/patch.py\", line 119, in new_create_async\n",
      "    response = await retry_async(\n",
      "               ^^^^^^^^^^^^^^^^^^\n",
      "  File \"/Users/tcapelle/miniforge3/envs/weave/lib/python3.11/site-packages/instructor/retry.py\", line 267, in retry_async\n",
      "    raise InstructorRetryException(\n",
      "instructor.exceptions.InstructorRetryException: RetryError[<Future at 0x33d366990 state=finished raised RateLimitError>]\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\">Predict and score failed\n",
       "</pre>\n"
      ],
      "text/plain": [
       "Predict and score failed\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Traceback (most recent call last):\n",
      "  File \"/Users/tcapelle/miniforge3/envs/weave/lib/python3.11/site-packages/weave/flow/eval.py\", line 287, in eval_example\n",
      "    eval_row = await self.predict_and_score(model, example)\n",
      "               ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/Users/tcapelle/miniforge3/envs/weave/lib/python3.11/site-packages/weave/trace/op.py\", line 333, in wrapper\n",
      "    res, _ = await _execute_call(wrapper, call, *args, **kwargs)  # type: ignore\n",
      "             ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/Users/tcapelle/miniforge3/envs/weave/lib/python3.11/site-packages/weave/trace/op.py\", line 213, in _call_async\n",
      "    return handle_exception(e)\n",
      "           ^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/Users/tcapelle/miniforge3/envs/weave/lib/python3.11/site-packages/weave/trace/op.py\", line 211, in _call_async\n",
      "    res = await func(*args, **kwargs)\n",
      "          ^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/Users/tcapelle/miniforge3/envs/weave/lib/python3.11/site-packages/weave/flow/eval.py\", line 224, in predict_and_score\n",
      "    result = await async_call(score_fn, **score_args)\n",
      "             ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/Users/tcapelle/miniforge3/envs/weave/lib/python3.11/asyncio/threads.py\", line 25, in to_thread\n",
      "    return await loop.run_in_executor(None, func_call)\n",
      "           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/Users/tcapelle/miniforge3/envs/weave/lib/python3.11/asyncio/futures.py\", line 287, in __await__\n",
      "    yield self  # This tells Task to wait for completion.\n",
      "    ^^^^^^^^^^\n",
      "  File \"/Users/tcapelle/miniforge3/envs/weave/lib/python3.11/asyncio/tasks.py\", line 349, in __wakeup\n",
      "    future.result()\n",
      "  File \"/Users/tcapelle/miniforge3/envs/weave/lib/python3.11/asyncio/futures.py\", line 203, in result\n",
      "    raise self._exception.with_traceback(self._exception_tb)\n",
      "  File \"/Users/tcapelle/miniforge3/envs/weave/lib/python3.11/concurrent/futures/thread.py\", line 58, in run\n",
      "    result = self.fn(*self.args, **self.kwargs)\n",
      "             ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/Users/tcapelle/miniforge3/envs/weave/lib/python3.11/site-packages/weave/trace/op.py\", line 344, in wrapper\n",
      "    res, _ = _execute_call(wrapper, call, *args, **kwargs)  # type: ignore\n",
      "             ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/Users/tcapelle/miniforge3/envs/weave/lib/python3.11/site-packages/weave/trace/op.py\", line 224, in _execute_call\n",
      "    handle_exception(e)\n",
      "  File \"/Users/tcapelle/miniforge3/envs/weave/lib/python3.11/site-packages/weave/trace/op.py\", line 222, in _execute_call\n",
      "    res = func(*args, **kwargs)\n",
      "          ^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/var/folders/sf/tgv7vcv96x557p38bvvp1ms40000gn/T/ipykernel_6554/249536452.py\", line 9, in scorer\n",
      "    return {\"passed\": expected == model_output[\"test_report\"]}\n",
      "                                  ~~~~~~~~~~~~^^^^^^^^^^^^^^^\n",
      "TypeError: 'NoneType' object is not subscriptable\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\">Evaluated <span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">2</span> of <span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">5</span> examples\n",
       "</pre>\n"
      ],
      "text/plain": [
       "Evaluated \u001b[1;36m2\u001b[0m of \u001b[1;36m5\u001b[0m examples\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2024-09-05 23:05:35,741 : INFO : HTTP Request: POST http://195.242.24.252:8000/v1/chat/completions \"HTTP/1.1 429 Too Many Requests\"\n",
      "2024-09-05 23:05:35,748 : ERROR : Failed after retries: <bound method Future.exception of <Future at 0x33d388d50 state=finished raised RateLimitError>>\n",
      "Traceback (most recent call last):\n",
      "  File \"/Users/tcapelle/miniforge3/envs/weave/lib/python3.11/site-packages/litellm/llms/openai.py\", line 1089, in acompletion\n",
      "    headers, response = await self.make_openai_chat_completion_request(\n",
      "                        ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/Users/tcapelle/miniforge3/envs/weave/lib/python3.11/site-packages/litellm/llms/openai.py\", line 793, in make_openai_chat_completion_request\n",
      "    raise e\n",
      "  File \"/Users/tcapelle/miniforge3/envs/weave/lib/python3.11/site-packages/litellm/llms/openai.py\", line 781, in make_openai_chat_completion_request\n",
      "    await openai_aclient.chat.completions.with_raw_response.create(\n",
      "  File \"/Users/tcapelle/miniforge3/envs/weave/lib/python3.11/site-packages/openai/_legacy_response.py\", line 367, in wrapped\n",
      "    return cast(LegacyAPIResponse[R], await func(*args, **kwargs))\n",
      "                                      ^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/Users/tcapelle/miniforge3/envs/weave/lib/python3.11/site-packages/weave/trace/op.py\", line 333, in wrapper\n",
      "    res, _ = await _execute_call(wrapper, call, *args, **kwargs)  # type: ignore\n",
      "             ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/Users/tcapelle/miniforge3/envs/weave/lib/python3.11/site-packages/weave/trace/op.py\", line 213, in _call_async\n",
      "    return handle_exception(e)\n",
      "           ^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/Users/tcapelle/miniforge3/envs/weave/lib/python3.11/site-packages/weave/trace/op.py\", line 211, in _call_async\n",
      "    res = await func(*args, **kwargs)\n",
      "          ^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/Users/tcapelle/miniforge3/envs/weave/lib/python3.11/site-packages/weave/integrations/openai/openai_sdk.py\", line 288, in _wrapper\n",
      "    return await fn(*args, **kwargs)\n",
      "           ^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/Users/tcapelle/miniforge3/envs/weave/lib/python3.11/site-packages/openai/resources/chat/completions.py\", line 1339, in create\n",
      "    return await self._post(\n",
      "           ^^^^^^^^^^^^^^^^^\n",
      "  File \"/Users/tcapelle/miniforge3/envs/weave/lib/python3.11/site-packages/openai/_base_client.py\", line 1816, in post\n",
      "    return await self.request(cast_to, opts, stream=stream, stream_cls=stream_cls)\n",
      "           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/Users/tcapelle/miniforge3/envs/weave/lib/python3.11/site-packages/openai/_base_client.py\", line 1510, in request\n",
      "    return await self._request(\n",
      "           ^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/Users/tcapelle/miniforge3/envs/weave/lib/python3.11/site-packages/openai/_base_client.py\", line 1596, in _request\n",
      "    return await self._retry_request(\n",
      "           ^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/Users/tcapelle/miniforge3/envs/weave/lib/python3.11/site-packages/openai/_base_client.py\", line 1643, in _retry_request\n",
      "    return await self._request(\n",
      "           ^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/Users/tcapelle/miniforge3/envs/weave/lib/python3.11/site-packages/openai/_base_client.py\", line 1596, in _request\n",
      "    return await self._retry_request(\n",
      "           ^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/Users/tcapelle/miniforge3/envs/weave/lib/python3.11/site-packages/openai/_base_client.py\", line 1643, in _retry_request\n",
      "    return await self._request(\n",
      "           ^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/Users/tcapelle/miniforge3/envs/weave/lib/python3.11/site-packages/openai/_base_client.py\", line 1611, in _request\n",
      "    raise self._make_status_error_from_response(err.response) from None\n",
      "openai.RateLimitError: Error code: 429 - {'error': {'message': 'Rate limit exceeded. Please try again later.', 'type': 'rate_limit_error', 'param': None, 'code': 'rate_limit_exceeded'}}\n",
      "\n",
      "During handling of the above exception, another exception occurred:\n",
      "\n",
      "Traceback (most recent call last):\n",
      "  File \"/Users/tcapelle/miniforge3/envs/weave/lib/python3.11/site-packages/litellm/main.py\", line 427, in acompletion\n",
      "    response = await init_response\n",
      "               ^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/Users/tcapelle/miniforge3/envs/weave/lib/python3.11/site-packages/litellm/llms/openai.py\", line 1136, in acompletion\n",
      "    raise OpenAIError(\n",
      "litellm.llms.openai.OpenAIError: Error code: 429 - {'error': {'message': 'Rate limit exceeded. Please try again later.', 'type': 'rate_limit_error', 'param': None, 'code': 'rate_limit_exceeded'}}\n",
      "\n",
      "During handling of the above exception, another exception occurred:\n",
      "\n",
      "Traceback (most recent call last):\n",
      "  File \"/Users/tcapelle/miniforge3/envs/weave/lib/python3.11/site-packages/instructor/retry.py\", line 239, in retry_async\n",
      "    response: ChatCompletion = await func(*args, **kwargs)\n",
      "                               ^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/Users/tcapelle/miniforge3/envs/weave/lib/python3.11/site-packages/weave/trace/op.py\", line 333, in wrapper\n",
      "    res, _ = await _execute_call(wrapper, call, *args, **kwargs)  # type: ignore\n",
      "             ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/Users/tcapelle/miniforge3/envs/weave/lib/python3.11/site-packages/weave/trace/op.py\", line 213, in _call_async\n",
      "    return handle_exception(e)\n",
      "           ^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/Users/tcapelle/miniforge3/envs/weave/lib/python3.11/site-packages/weave/trace/op.py\", line 211, in _call_async\n",
      "    res = await func(*args, **kwargs)\n",
      "          ^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/Users/tcapelle/miniforge3/envs/weave/lib/python3.11/site-packages/litellm/utils.py\", line 1593, in wrapper_async\n",
      "    raise e\n",
      "  File \"/Users/tcapelle/miniforge3/envs/weave/lib/python3.11/site-packages/litellm/utils.py\", line 1413, in wrapper_async\n",
      "    result = await original_function(*args, **kwargs)\n",
      "             ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/Users/tcapelle/miniforge3/envs/weave/lib/python3.11/site-packages/litellm/main.py\", line 449, in acompletion\n",
      "    raise exception_type(\n",
      "          ^^^^^^^^^^^^^^^\n",
      "  File \"/Users/tcapelle/miniforge3/envs/weave/lib/python3.11/site-packages/litellm/utils.py\", line 8187, in exception_type\n",
      "    raise e\n",
      "  File \"/Users/tcapelle/miniforge3/envs/weave/lib/python3.11/site-packages/litellm/utils.py\", line 6555, in exception_type\n",
      "    raise RateLimitError(\n",
      "litellm.exceptions.RateLimitError: litellm.RateLimitError: RateLimitError: MistralException - Error code: 429 - {'error': {'message': 'Rate limit exceeded. Please try again later.', 'type': 'rate_limit_error', 'param': None, 'code': 'rate_limit_exceeded'}}\n",
      "\n",
      "The above exception was the direct cause of the following exception:\n",
      "\n",
      "Traceback (most recent call last):\n",
      "  File \"/Users/tcapelle/miniforge3/envs/weave/lib/python3.11/site-packages/instructor/retry.py\", line 235, in retry_async\n",
      "    async for attempt in max_retries:\n",
      "  File \"/Users/tcapelle/miniforge3/envs/weave/lib/python3.11/site-packages/tenacity/asyncio/__init__.py\", line 166, in __anext__\n",
      "    do = await self.iter(retry_state=self._retry_state)\n",
      "         ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/Users/tcapelle/miniforge3/envs/weave/lib/python3.11/site-packages/tenacity/asyncio/__init__.py\", line 153, in iter\n",
      "    result = await action(retry_state)\n",
      "             ^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/Users/tcapelle/miniforge3/envs/weave/lib/python3.11/site-packages/tenacity/_utils.py\", line 99, in inner\n",
      "    return call(*args, **kwargs)\n",
      "           ^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/Users/tcapelle/miniforge3/envs/weave/lib/python3.11/site-packages/tenacity/__init__.py\", line 419, in exc_check\n",
      "    raise retry_exc from fut.exception()\n",
      "tenacity.RetryError: RetryError[<Future at 0x33d388d50 state=finished raised RateLimitError>]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "\u001b[1;31mGive Feedback / Get Help: https://github.com/BerriAI/litellm/issues/new\u001b[0m\n",
      "LiteLLM.Info: If you need to debug this error, use `litellm.set_verbose=True'.\n",
      "\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\">model_output failed\n",
       "</pre>\n"
      ],
      "text/plain": [
       "model_output failed\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Traceback (most recent call last):\n",
      "  File \"/Users/tcapelle/miniforge3/envs/weave/lib/python3.11/site-packages/litellm/llms/openai.py\", line 1089, in acompletion\n",
      "    headers, response = await self.make_openai_chat_completion_request(\n",
      "                        ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/Users/tcapelle/miniforge3/envs/weave/lib/python3.11/site-packages/litellm/llms/openai.py\", line 793, in make_openai_chat_completion_request\n",
      "    raise e\n",
      "  File \"/Users/tcapelle/miniforge3/envs/weave/lib/python3.11/site-packages/litellm/llms/openai.py\", line 781, in make_openai_chat_completion_request\n",
      "    await openai_aclient.chat.completions.with_raw_response.create(\n",
      "  File \"/Users/tcapelle/miniforge3/envs/weave/lib/python3.11/site-packages/openai/_legacy_response.py\", line 367, in wrapped\n",
      "    return cast(LegacyAPIResponse[R], await func(*args, **kwargs))\n",
      "                                      ^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/Users/tcapelle/miniforge3/envs/weave/lib/python3.11/site-packages/weave/trace/op.py\", line 333, in wrapper\n",
      "    res, _ = await _execute_call(wrapper, call, *args, **kwargs)  # type: ignore\n",
      "             ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/Users/tcapelle/miniforge3/envs/weave/lib/python3.11/site-packages/weave/trace/op.py\", line 213, in _call_async\n",
      "    return handle_exception(e)\n",
      "           ^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/Users/tcapelle/miniforge3/envs/weave/lib/python3.11/site-packages/weave/trace/op.py\", line 211, in _call_async\n",
      "    res = await func(*args, **kwargs)\n",
      "          ^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/Users/tcapelle/miniforge3/envs/weave/lib/python3.11/site-packages/weave/integrations/openai/openai_sdk.py\", line 288, in _wrapper\n",
      "    return await fn(*args, **kwargs)\n",
      "           ^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/Users/tcapelle/miniforge3/envs/weave/lib/python3.11/site-packages/openai/resources/chat/completions.py\", line 1339, in create\n",
      "    return await self._post(\n",
      "           ^^^^^^^^^^^^^^^^^\n",
      "  File \"/Users/tcapelle/miniforge3/envs/weave/lib/python3.11/site-packages/openai/_base_client.py\", line 1816, in post\n",
      "    return await self.request(cast_to, opts, stream=stream, stream_cls=stream_cls)\n",
      "           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/Users/tcapelle/miniforge3/envs/weave/lib/python3.11/site-packages/openai/_base_client.py\", line 1510, in request\n",
      "    return await self._request(\n",
      "           ^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/Users/tcapelle/miniforge3/envs/weave/lib/python3.11/site-packages/openai/_base_client.py\", line 1596, in _request\n",
      "    return await self._retry_request(\n",
      "           ^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/Users/tcapelle/miniforge3/envs/weave/lib/python3.11/site-packages/openai/_base_client.py\", line 1643, in _retry_request\n",
      "    return await self._request(\n",
      "           ^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/Users/tcapelle/miniforge3/envs/weave/lib/python3.11/site-packages/openai/_base_client.py\", line 1596, in _request\n",
      "    return await self._retry_request(\n",
      "           ^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/Users/tcapelle/miniforge3/envs/weave/lib/python3.11/site-packages/openai/_base_client.py\", line 1643, in _retry_request\n",
      "    return await self._request(\n",
      "           ^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/Users/tcapelle/miniforge3/envs/weave/lib/python3.11/site-packages/openai/_base_client.py\", line 1611, in _request\n",
      "    raise self._make_status_error_from_response(err.response) from None\n",
      "openai.RateLimitError: Error code: 429 - {'error': {'message': 'Rate limit exceeded. Please try again later.', 'type': 'rate_limit_error', 'param': None, 'code': 'rate_limit_exceeded'}}\n",
      "\n",
      "During handling of the above exception, another exception occurred:\n",
      "\n",
      "Traceback (most recent call last):\n",
      "  File \"/Users/tcapelle/miniforge3/envs/weave/lib/python3.11/site-packages/litellm/main.py\", line 427, in acompletion\n",
      "    response = await init_response\n",
      "               ^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/Users/tcapelle/miniforge3/envs/weave/lib/python3.11/site-packages/litellm/llms/openai.py\", line 1136, in acompletion\n",
      "    raise OpenAIError(\n",
      "litellm.llms.openai.OpenAIError: Error code: 429 - {'error': {'message': 'Rate limit exceeded. Please try again later.', 'type': 'rate_limit_error', 'param': None, 'code': 'rate_limit_exceeded'}}\n",
      "\n",
      "During handling of the above exception, another exception occurred:\n",
      "\n",
      "Traceback (most recent call last):\n",
      "  File \"/Users/tcapelle/miniforge3/envs/weave/lib/python3.11/site-packages/instructor/retry.py\", line 239, in retry_async\n",
      "    response: ChatCompletion = await func(*args, **kwargs)\n",
      "                               ^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/Users/tcapelle/miniforge3/envs/weave/lib/python3.11/site-packages/weave/trace/op.py\", line 333, in wrapper\n",
      "    res, _ = await _execute_call(wrapper, call, *args, **kwargs)  # type: ignore\n",
      "             ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/Users/tcapelle/miniforge3/envs/weave/lib/python3.11/site-packages/weave/trace/op.py\", line 213, in _call_async\n",
      "    return handle_exception(e)\n",
      "           ^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/Users/tcapelle/miniforge3/envs/weave/lib/python3.11/site-packages/weave/trace/op.py\", line 211, in _call_async\n",
      "    res = await func(*args, **kwargs)\n",
      "          ^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/Users/tcapelle/miniforge3/envs/weave/lib/python3.11/site-packages/litellm/utils.py\", line 1593, in wrapper_async\n",
      "    raise e\n",
      "  File \"/Users/tcapelle/miniforge3/envs/weave/lib/python3.11/site-packages/litellm/utils.py\", line 1413, in wrapper_async\n",
      "    result = await original_function(*args, **kwargs)\n",
      "             ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/Users/tcapelle/miniforge3/envs/weave/lib/python3.11/site-packages/litellm/main.py\", line 449, in acompletion\n",
      "    raise exception_type(\n",
      "          ^^^^^^^^^^^^^^^\n",
      "  File \"/Users/tcapelle/miniforge3/envs/weave/lib/python3.11/site-packages/litellm/utils.py\", line 8187, in exception_type\n",
      "    raise e\n",
      "  File \"/Users/tcapelle/miniforge3/envs/weave/lib/python3.11/site-packages/litellm/utils.py\", line 6555, in exception_type\n",
      "    raise RateLimitError(\n",
      "litellm.exceptions.RateLimitError: litellm.RateLimitError: RateLimitError: MistralException - Error code: 429 - {'error': {'message': 'Rate limit exceeded. Please try again later.', 'type': 'rate_limit_error', 'param': None, 'code': 'rate_limit_exceeded'}}\n",
      "\n",
      "The above exception was the direct cause of the following exception:\n",
      "\n",
      "Traceback (most recent call last):\n",
      "  File \"/Users/tcapelle/miniforge3/envs/weave/lib/python3.11/site-packages/instructor/retry.py\", line 235, in retry_async\n",
      "    async for attempt in max_retries:\n",
      "  File \"/Users/tcapelle/miniforge3/envs/weave/lib/python3.11/site-packages/tenacity/asyncio/__init__.py\", line 166, in __anext__\n",
      "    do = await self.iter(retry_state=self._retry_state)\n",
      "         ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/Users/tcapelle/miniforge3/envs/weave/lib/python3.11/site-packages/tenacity/asyncio/__init__.py\", line 153, in iter\n",
      "    result = await action(retry_state)\n",
      "             ^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/Users/tcapelle/miniforge3/envs/weave/lib/python3.11/site-packages/tenacity/_utils.py\", line 99, in inner\n",
      "    return call(*args, **kwargs)\n",
      "           ^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/Users/tcapelle/miniforge3/envs/weave/lib/python3.11/site-packages/tenacity/__init__.py\", line 419, in exc_check\n",
      "    raise retry_exc from fut.exception()\n",
      "tenacity.RetryError: RetryError[<Future at 0x33d388d50 state=finished raised RateLimitError>]\n",
      "\n",
      "The above exception was the direct cause of the following exception:\n",
      "\n",
      "Traceback (most recent call last):\n",
      "  File \"/Users/tcapelle/miniforge3/envs/weave/lib/python3.11/site-packages/weave/flow/eval.py\", line 168, in predict_and_score\n",
      "    model_output = await async_call(model_predict, **model_predict_args)\n",
      "                   ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/Users/tcapelle/miniforge3/envs/weave/lib/python3.11/site-packages/weave/trace/op.py\", line 333, in wrapper\n",
      "    res, _ = await _execute_call(wrapper, call, *args, **kwargs)  # type: ignore\n",
      "             ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/Users/tcapelle/miniforge3/envs/weave/lib/python3.11/site-packages/weave/trace/op.py\", line 213, in _call_async\n",
      "    return handle_exception(e)\n",
      "           ^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/Users/tcapelle/miniforge3/envs/weave/lib/python3.11/site-packages/weave/trace/op.py\", line 211, in _call_async\n",
      "    res = await func(*args, **kwargs)\n",
      "          ^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/var/folders/sf/tgv7vcv96x557p38bvvp1ms40000gn/T/ipykernel_6554/970167077.py\", line 10, in predict\n",
      "    return await rag_solver_with_reflection(\n",
      "           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/Users/tcapelle/miniforge3/envs/weave/lib/python3.11/site-packages/weave/trace/op.py\", line 333, in wrapper\n",
      "    res, _ = await _execute_call(wrapper, call, *args, **kwargs)  # type: ignore\n",
      "             ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/Users/tcapelle/miniforge3/envs/weave/lib/python3.11/site-packages/weave/trace/op.py\", line 213, in _call_async\n",
      "    return handle_exception(e)\n",
      "           ^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/Users/tcapelle/miniforge3/envs/weave/lib/python3.11/site-packages/weave/trace/op.py\", line 211, in _call_async\n",
      "    res = await func(*args, **kwargs)\n",
      "          ^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/var/folders/sf/tgv7vcv96x557p38bvvp1ms40000gn/T/ipykernel_6554/4070537091.py\", line 14, in rag_solver_with_reflection\n",
      "    rag_result = await rag_solver(\n",
      "                 ^^^^^^^^^^^^^^^^^\n",
      "  File \"/Users/tcapelle/miniforge3/envs/weave/lib/python3.11/site-packages/weave/trace/op.py\", line 333, in wrapper\n",
      "    res, _ = await _execute_call(wrapper, call, *args, **kwargs)  # type: ignore\n",
      "             ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/Users/tcapelle/miniforge3/envs/weave/lib/python3.11/site-packages/weave/trace/op.py\", line 213, in _call_async\n",
      "    return handle_exception(e)\n",
      "           ^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/Users/tcapelle/miniforge3/envs/weave/lib/python3.11/site-packages/weave/trace/op.py\", line 211, in _call_async\n",
      "    res = await func(*args, **kwargs)\n",
      "          ^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/Users/tcapelle/work/hackercup_rag/agent.py\", line 350, in rag_solver\n",
      "    zero_shot_result = await zero_shot_solver(\n",
      "                       ^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/Users/tcapelle/miniforge3/envs/weave/lib/python3.11/site-packages/weave/trace/op.py\", line 333, in wrapper\n",
      "    res, _ = await _execute_call(wrapper, call, *args, **kwargs)  # type: ignore\n",
      "             ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/Users/tcapelle/miniforge3/envs/weave/lib/python3.11/site-packages/weave/trace/op.py\", line 213, in _call_async\n",
      "    return handle_exception(e)\n",
      "           ^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/Users/tcapelle/miniforge3/envs/weave/lib/python3.11/site-packages/weave/trace/op.py\", line 211, in _call_async\n",
      "    res = await func(*args, **kwargs)\n",
      "          ^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/Users/tcapelle/work/hackercup_rag/agent.py\", line 330, in zero_shot_solver\n",
      "    solution = await draft_solution(\n",
      "               ^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/Users/tcapelle/miniforge3/envs/weave/lib/python3.11/site-packages/weave/trace/op.py\", line 333, in wrapper\n",
      "    res, _ = await _execute_call(wrapper, call, *args, **kwargs)  # type: ignore\n",
      "             ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/Users/tcapelle/miniforge3/envs/weave/lib/python3.11/site-packages/weave/trace/op.py\", line 213, in _call_async\n",
      "    return handle_exception(e)\n",
      "           ^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/Users/tcapelle/miniforge3/envs/weave/lib/python3.11/site-packages/weave/trace/op.py\", line 211, in _call_async\n",
      "    res = await func(*args, **kwargs)\n",
      "          ^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/Users/tcapelle/work/hackercup_rag/agent.py\", line 83, in draft_solution\n",
      "    response = await async_client.chat.completions.create(\n",
      "               ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/Users/tcapelle/miniforge3/envs/weave/lib/python3.11/site-packages/instructor/client.py\", line 300, in create\n",
      "    return await self.create_fn(\n",
      "           ^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/Users/tcapelle/miniforge3/envs/weave/lib/python3.11/site-packages/instructor/patch.py\", line 119, in new_create_async\n",
      "    response = await retry_async(\n",
      "               ^^^^^^^^^^^^^^^^^^\n",
      "  File \"/Users/tcapelle/miniforge3/envs/weave/lib/python3.11/site-packages/instructor/retry.py\", line 267, in retry_async\n",
      "    raise InstructorRetryException(\n",
      "instructor.exceptions.InstructorRetryException: RetryError[<Future at 0x33d388d50 state=finished raised RateLimitError>]\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\">Predict and score failed\n",
       "</pre>\n"
      ],
      "text/plain": [
       "Predict and score failed\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Traceback (most recent call last):\n",
      "  File \"/Users/tcapelle/miniforge3/envs/weave/lib/python3.11/site-packages/weave/flow/eval.py\", line 287, in eval_example\n",
      "    eval_row = await self.predict_and_score(model, example)\n",
      "               ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/Users/tcapelle/miniforge3/envs/weave/lib/python3.11/site-packages/weave/trace/op.py\", line 333, in wrapper\n",
      "    res, _ = await _execute_call(wrapper, call, *args, **kwargs)  # type: ignore\n",
      "             ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/Users/tcapelle/miniforge3/envs/weave/lib/python3.11/site-packages/weave/trace/op.py\", line 213, in _call_async\n",
      "    return handle_exception(e)\n",
      "           ^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/Users/tcapelle/miniforge3/envs/weave/lib/python3.11/site-packages/weave/trace/op.py\", line 211, in _call_async\n",
      "    res = await func(*args, **kwargs)\n",
      "          ^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/Users/tcapelle/miniforge3/envs/weave/lib/python3.11/site-packages/weave/flow/eval.py\", line 224, in predict_and_score\n",
      "    result = await async_call(score_fn, **score_args)\n",
      "             ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/Users/tcapelle/miniforge3/envs/weave/lib/python3.11/asyncio/threads.py\", line 25, in to_thread\n",
      "    return await loop.run_in_executor(None, func_call)\n",
      "           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/Users/tcapelle/miniforge3/envs/weave/lib/python3.11/asyncio/futures.py\", line 287, in __await__\n",
      "    yield self  # This tells Task to wait for completion.\n",
      "    ^^^^^^^^^^\n",
      "  File \"/Users/tcapelle/miniforge3/envs/weave/lib/python3.11/asyncio/tasks.py\", line 349, in __wakeup\n",
      "    future.result()\n",
      "  File \"/Users/tcapelle/miniforge3/envs/weave/lib/python3.11/asyncio/futures.py\", line 203, in result\n",
      "    raise self._exception.with_traceback(self._exception_tb)\n",
      "  File \"/Users/tcapelle/miniforge3/envs/weave/lib/python3.11/concurrent/futures/thread.py\", line 58, in run\n",
      "    result = self.fn(*self.args, **self.kwargs)\n",
      "             ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/Users/tcapelle/miniforge3/envs/weave/lib/python3.11/site-packages/weave/trace/op.py\", line 344, in wrapper\n",
      "    res, _ = _execute_call(wrapper, call, *args, **kwargs)  # type: ignore\n",
      "             ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/Users/tcapelle/miniforge3/envs/weave/lib/python3.11/site-packages/weave/trace/op.py\", line 224, in _execute_call\n",
      "    handle_exception(e)\n",
      "  File \"/Users/tcapelle/miniforge3/envs/weave/lib/python3.11/site-packages/weave/trace/op.py\", line 222, in _execute_call\n",
      "    res = func(*args, **kwargs)\n",
      "          ^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/var/folders/sf/tgv7vcv96x557p38bvvp1ms40000gn/T/ipykernel_6554/249536452.py\", line 9, in scorer\n",
      "    return {\"passed\": expected == model_output[\"test_report\"]}\n",
      "                                  ~~~~~~~~~~~~^^^^^^^^^^^^^^^\n",
      "TypeError: 'NoneType' object is not subscriptable\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\">Evaluated <span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">3</span> of <span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">5</span> examples\n",
       "</pre>\n"
      ],
      "text/plain": [
       "Evaluated \u001b[1;36m3\u001b[0m of \u001b[1;36m5\u001b[0m examples\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2024-09-05 23:05:35,842 : INFO : HTTP Request: POST http://195.242.24.252:8000/v1/chat/completions \"HTTP/1.1 429 Too Many Requests\"\n",
      "2024-09-05 23:05:35,851 : ERROR : Failed after retries: <bound method Future.exception of <Future at 0x33d391010 state=finished raised RateLimitError>>\n",
      "Traceback (most recent call last):\n",
      "  File \"/Users/tcapelle/miniforge3/envs/weave/lib/python3.11/site-packages/litellm/llms/openai.py\", line 1089, in acompletion\n",
      "    headers, response = await self.make_openai_chat_completion_request(\n",
      "                        ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/Users/tcapelle/miniforge3/envs/weave/lib/python3.11/site-packages/litellm/llms/openai.py\", line 793, in make_openai_chat_completion_request\n",
      "    raise e\n",
      "  File \"/Users/tcapelle/miniforge3/envs/weave/lib/python3.11/site-packages/litellm/llms/openai.py\", line 781, in make_openai_chat_completion_request\n",
      "    await openai_aclient.chat.completions.with_raw_response.create(\n",
      "  File \"/Users/tcapelle/miniforge3/envs/weave/lib/python3.11/site-packages/openai/_legacy_response.py\", line 367, in wrapped\n",
      "    return cast(LegacyAPIResponse[R], await func(*args, **kwargs))\n",
      "                                      ^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/Users/tcapelle/miniforge3/envs/weave/lib/python3.11/site-packages/weave/trace/op.py\", line 333, in wrapper\n",
      "    res, _ = await _execute_call(wrapper, call, *args, **kwargs)  # type: ignore\n",
      "             ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/Users/tcapelle/miniforge3/envs/weave/lib/python3.11/site-packages/weave/trace/op.py\", line 213, in _call_async\n",
      "    return handle_exception(e)\n",
      "           ^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/Users/tcapelle/miniforge3/envs/weave/lib/python3.11/site-packages/weave/trace/op.py\", line 211, in _call_async\n",
      "    res = await func(*args, **kwargs)\n",
      "          ^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/Users/tcapelle/miniforge3/envs/weave/lib/python3.11/site-packages/weave/integrations/openai/openai_sdk.py\", line 288, in _wrapper\n",
      "    return await fn(*args, **kwargs)\n",
      "           ^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/Users/tcapelle/miniforge3/envs/weave/lib/python3.11/site-packages/openai/resources/chat/completions.py\", line 1339, in create\n",
      "    return await self._post(\n",
      "           ^^^^^^^^^^^^^^^^^\n",
      "  File \"/Users/tcapelle/miniforge3/envs/weave/lib/python3.11/site-packages/openai/_base_client.py\", line 1816, in post\n",
      "    return await self.request(cast_to, opts, stream=stream, stream_cls=stream_cls)\n",
      "           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/Users/tcapelle/miniforge3/envs/weave/lib/python3.11/site-packages/openai/_base_client.py\", line 1510, in request\n",
      "    return await self._request(\n",
      "           ^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/Users/tcapelle/miniforge3/envs/weave/lib/python3.11/site-packages/openai/_base_client.py\", line 1596, in _request\n",
      "    return await self._retry_request(\n",
      "           ^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/Users/tcapelle/miniforge3/envs/weave/lib/python3.11/site-packages/openai/_base_client.py\", line 1643, in _retry_request\n",
      "    return await self._request(\n",
      "           ^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/Users/tcapelle/miniforge3/envs/weave/lib/python3.11/site-packages/openai/_base_client.py\", line 1596, in _request\n",
      "    return await self._retry_request(\n",
      "           ^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/Users/tcapelle/miniforge3/envs/weave/lib/python3.11/site-packages/openai/_base_client.py\", line 1643, in _retry_request\n",
      "    return await self._request(\n",
      "           ^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/Users/tcapelle/miniforge3/envs/weave/lib/python3.11/site-packages/openai/_base_client.py\", line 1611, in _request\n",
      "    raise self._make_status_error_from_response(err.response) from None\n",
      "openai.RateLimitError: Error code: 429 - {'error': {'message': 'Rate limit exceeded. Please try again later.', 'type': 'rate_limit_error', 'param': None, 'code': 'rate_limit_exceeded'}}\n",
      "\n",
      "During handling of the above exception, another exception occurred:\n",
      "\n",
      "Traceback (most recent call last):\n",
      "  File \"/Users/tcapelle/miniforge3/envs/weave/lib/python3.11/site-packages/litellm/main.py\", line 427, in acompletion\n",
      "    response = await init_response\n",
      "               ^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/Users/tcapelle/miniforge3/envs/weave/lib/python3.11/site-packages/litellm/llms/openai.py\", line 1136, in acompletion\n",
      "    raise OpenAIError(\n",
      "litellm.llms.openai.OpenAIError: Error code: 429 - {'error': {'message': 'Rate limit exceeded. Please try again later.', 'type': 'rate_limit_error', 'param': None, 'code': 'rate_limit_exceeded'}}\n",
      "\n",
      "During handling of the above exception, another exception occurred:\n",
      "\n",
      "Traceback (most recent call last):\n",
      "  File \"/Users/tcapelle/miniforge3/envs/weave/lib/python3.11/site-packages/instructor/retry.py\", line 239, in retry_async\n",
      "    response: ChatCompletion = await func(*args, **kwargs)\n",
      "                               ^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/Users/tcapelle/miniforge3/envs/weave/lib/python3.11/site-packages/weave/trace/op.py\", line 333, in wrapper\n",
      "    res, _ = await _execute_call(wrapper, call, *args, **kwargs)  # type: ignore\n",
      "             ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/Users/tcapelle/miniforge3/envs/weave/lib/python3.11/site-packages/weave/trace/op.py\", line 213, in _call_async\n",
      "    return handle_exception(e)\n",
      "           ^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/Users/tcapelle/miniforge3/envs/weave/lib/python3.11/site-packages/weave/trace/op.py\", line 211, in _call_async\n",
      "    res = await func(*args, **kwargs)\n",
      "          ^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/Users/tcapelle/miniforge3/envs/weave/lib/python3.11/site-packages/litellm/utils.py\", line 1593, in wrapper_async\n",
      "    raise e\n",
      "  File \"/Users/tcapelle/miniforge3/envs/weave/lib/python3.11/site-packages/litellm/utils.py\", line 1413, in wrapper_async\n",
      "    result = await original_function(*args, **kwargs)\n",
      "             ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/Users/tcapelle/miniforge3/envs/weave/lib/python3.11/site-packages/litellm/main.py\", line 449, in acompletion\n",
      "    raise exception_type(\n",
      "          ^^^^^^^^^^^^^^^\n",
      "  File \"/Users/tcapelle/miniforge3/envs/weave/lib/python3.11/site-packages/litellm/utils.py\", line 8187, in exception_type\n",
      "    raise e\n",
      "  File \"/Users/tcapelle/miniforge3/envs/weave/lib/python3.11/site-packages/litellm/utils.py\", line 6555, in exception_type\n",
      "    raise RateLimitError(\n",
      "litellm.exceptions.RateLimitError: litellm.RateLimitError: RateLimitError: MistralException - Error code: 429 - {'error': {'message': 'Rate limit exceeded. Please try again later.', 'type': 'rate_limit_error', 'param': None, 'code': 'rate_limit_exceeded'}}\n",
      "\n",
      "The above exception was the direct cause of the following exception:\n",
      "\n",
      "Traceback (most recent call last):\n",
      "  File \"/Users/tcapelle/miniforge3/envs/weave/lib/python3.11/site-packages/instructor/retry.py\", line 235, in retry_async\n",
      "    async for attempt in max_retries:\n",
      "  File \"/Users/tcapelle/miniforge3/envs/weave/lib/python3.11/site-packages/tenacity/asyncio/__init__.py\", line 166, in __anext__\n",
      "    do = await self.iter(retry_state=self._retry_state)\n",
      "         ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/Users/tcapelle/miniforge3/envs/weave/lib/python3.11/site-packages/tenacity/asyncio/__init__.py\", line 153, in iter\n",
      "    result = await action(retry_state)\n",
      "             ^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/Users/tcapelle/miniforge3/envs/weave/lib/python3.11/site-packages/tenacity/_utils.py\", line 99, in inner\n",
      "    return call(*args, **kwargs)\n",
      "           ^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/Users/tcapelle/miniforge3/envs/weave/lib/python3.11/site-packages/tenacity/__init__.py\", line 419, in exc_check\n",
      "    raise retry_exc from fut.exception()\n",
      "tenacity.RetryError: RetryError[<Future at 0x33d391010 state=finished raised RateLimitError>]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "\u001b[1;31mGive Feedback / Get Help: https://github.com/BerriAI/litellm/issues/new\u001b[0m\n",
      "LiteLLM.Info: If you need to debug this error, use `litellm.set_verbose=True'.\n",
      "\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\">model_output failed\n",
       "</pre>\n"
      ],
      "text/plain": [
       "model_output failed\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Traceback (most recent call last):\n",
      "  File \"/Users/tcapelle/miniforge3/envs/weave/lib/python3.11/site-packages/litellm/llms/openai.py\", line 1089, in acompletion\n",
      "    headers, response = await self.make_openai_chat_completion_request(\n",
      "                        ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/Users/tcapelle/miniforge3/envs/weave/lib/python3.11/site-packages/litellm/llms/openai.py\", line 793, in make_openai_chat_completion_request\n",
      "    raise e\n",
      "  File \"/Users/tcapelle/miniforge3/envs/weave/lib/python3.11/site-packages/litellm/llms/openai.py\", line 781, in make_openai_chat_completion_request\n",
      "    await openai_aclient.chat.completions.with_raw_response.create(\n",
      "  File \"/Users/tcapelle/miniforge3/envs/weave/lib/python3.11/site-packages/openai/_legacy_response.py\", line 367, in wrapped\n",
      "    return cast(LegacyAPIResponse[R], await func(*args, **kwargs))\n",
      "                                      ^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/Users/tcapelle/miniforge3/envs/weave/lib/python3.11/site-packages/weave/trace/op.py\", line 333, in wrapper\n",
      "    res, _ = await _execute_call(wrapper, call, *args, **kwargs)  # type: ignore\n",
      "             ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/Users/tcapelle/miniforge3/envs/weave/lib/python3.11/site-packages/weave/trace/op.py\", line 213, in _call_async\n",
      "    return handle_exception(e)\n",
      "           ^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/Users/tcapelle/miniforge3/envs/weave/lib/python3.11/site-packages/weave/trace/op.py\", line 211, in _call_async\n",
      "    res = await func(*args, **kwargs)\n",
      "          ^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/Users/tcapelle/miniforge3/envs/weave/lib/python3.11/site-packages/weave/integrations/openai/openai_sdk.py\", line 288, in _wrapper\n",
      "    return await fn(*args, **kwargs)\n",
      "           ^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/Users/tcapelle/miniforge3/envs/weave/lib/python3.11/site-packages/openai/resources/chat/completions.py\", line 1339, in create\n",
      "    return await self._post(\n",
      "           ^^^^^^^^^^^^^^^^^\n",
      "  File \"/Users/tcapelle/miniforge3/envs/weave/lib/python3.11/site-packages/openai/_base_client.py\", line 1816, in post\n",
      "    return await self.request(cast_to, opts, stream=stream, stream_cls=stream_cls)\n",
      "           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/Users/tcapelle/miniforge3/envs/weave/lib/python3.11/site-packages/openai/_base_client.py\", line 1510, in request\n",
      "    return await self._request(\n",
      "           ^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/Users/tcapelle/miniforge3/envs/weave/lib/python3.11/site-packages/openai/_base_client.py\", line 1596, in _request\n",
      "    return await self._retry_request(\n",
      "           ^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/Users/tcapelle/miniforge3/envs/weave/lib/python3.11/site-packages/openai/_base_client.py\", line 1643, in _retry_request\n",
      "    return await self._request(\n",
      "           ^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/Users/tcapelle/miniforge3/envs/weave/lib/python3.11/site-packages/openai/_base_client.py\", line 1596, in _request\n",
      "    return await self._retry_request(\n",
      "           ^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/Users/tcapelle/miniforge3/envs/weave/lib/python3.11/site-packages/openai/_base_client.py\", line 1643, in _retry_request\n",
      "    return await self._request(\n",
      "           ^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/Users/tcapelle/miniforge3/envs/weave/lib/python3.11/site-packages/openai/_base_client.py\", line 1611, in _request\n",
      "    raise self._make_status_error_from_response(err.response) from None\n",
      "openai.RateLimitError: Error code: 429 - {'error': {'message': 'Rate limit exceeded. Please try again later.', 'type': 'rate_limit_error', 'param': None, 'code': 'rate_limit_exceeded'}}\n",
      "\n",
      "During handling of the above exception, another exception occurred:\n",
      "\n",
      "Traceback (most recent call last):\n",
      "  File \"/Users/tcapelle/miniforge3/envs/weave/lib/python3.11/site-packages/litellm/main.py\", line 427, in acompletion\n",
      "    response = await init_response\n",
      "               ^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/Users/tcapelle/miniforge3/envs/weave/lib/python3.11/site-packages/litellm/llms/openai.py\", line 1136, in acompletion\n",
      "    raise OpenAIError(\n",
      "litellm.llms.openai.OpenAIError: Error code: 429 - {'error': {'message': 'Rate limit exceeded. Please try again later.', 'type': 'rate_limit_error', 'param': None, 'code': 'rate_limit_exceeded'}}\n",
      "\n",
      "During handling of the above exception, another exception occurred:\n",
      "\n",
      "Traceback (most recent call last):\n",
      "  File \"/Users/tcapelle/miniforge3/envs/weave/lib/python3.11/site-packages/instructor/retry.py\", line 239, in retry_async\n",
      "    response: ChatCompletion = await func(*args, **kwargs)\n",
      "                               ^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/Users/tcapelle/miniforge3/envs/weave/lib/python3.11/site-packages/weave/trace/op.py\", line 333, in wrapper\n",
      "    res, _ = await _execute_call(wrapper, call, *args, **kwargs)  # type: ignore\n",
      "             ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/Users/tcapelle/miniforge3/envs/weave/lib/python3.11/site-packages/weave/trace/op.py\", line 213, in _call_async\n",
      "    return handle_exception(e)\n",
      "           ^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/Users/tcapelle/miniforge3/envs/weave/lib/python3.11/site-packages/weave/trace/op.py\", line 211, in _call_async\n",
      "    res = await func(*args, **kwargs)\n",
      "          ^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/Users/tcapelle/miniforge3/envs/weave/lib/python3.11/site-packages/litellm/utils.py\", line 1593, in wrapper_async\n",
      "    raise e\n",
      "  File \"/Users/tcapelle/miniforge3/envs/weave/lib/python3.11/site-packages/litellm/utils.py\", line 1413, in wrapper_async\n",
      "    result = await original_function(*args, **kwargs)\n",
      "             ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/Users/tcapelle/miniforge3/envs/weave/lib/python3.11/site-packages/litellm/main.py\", line 449, in acompletion\n",
      "    raise exception_type(\n",
      "          ^^^^^^^^^^^^^^^\n",
      "  File \"/Users/tcapelle/miniforge3/envs/weave/lib/python3.11/site-packages/litellm/utils.py\", line 8187, in exception_type\n",
      "    raise e\n",
      "  File \"/Users/tcapelle/miniforge3/envs/weave/lib/python3.11/site-packages/litellm/utils.py\", line 6555, in exception_type\n",
      "    raise RateLimitError(\n",
      "litellm.exceptions.RateLimitError: litellm.RateLimitError: RateLimitError: MistralException - Error code: 429 - {'error': {'message': 'Rate limit exceeded. Please try again later.', 'type': 'rate_limit_error', 'param': None, 'code': 'rate_limit_exceeded'}}\n",
      "\n",
      "The above exception was the direct cause of the following exception:\n",
      "\n",
      "Traceback (most recent call last):\n",
      "  File \"/Users/tcapelle/miniforge3/envs/weave/lib/python3.11/site-packages/instructor/retry.py\", line 235, in retry_async\n",
      "    async for attempt in max_retries:\n",
      "  File \"/Users/tcapelle/miniforge3/envs/weave/lib/python3.11/site-packages/tenacity/asyncio/__init__.py\", line 166, in __anext__\n",
      "    do = await self.iter(retry_state=self._retry_state)\n",
      "         ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/Users/tcapelle/miniforge3/envs/weave/lib/python3.11/site-packages/tenacity/asyncio/__init__.py\", line 153, in iter\n",
      "    result = await action(retry_state)\n",
      "             ^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/Users/tcapelle/miniforge3/envs/weave/lib/python3.11/site-packages/tenacity/_utils.py\", line 99, in inner\n",
      "    return call(*args, **kwargs)\n",
      "           ^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/Users/tcapelle/miniforge3/envs/weave/lib/python3.11/site-packages/tenacity/__init__.py\", line 419, in exc_check\n",
      "    raise retry_exc from fut.exception()\n",
      "tenacity.RetryError: RetryError[<Future at 0x33d391010 state=finished raised RateLimitError>]\n",
      "\n",
      "The above exception was the direct cause of the following exception:\n",
      "\n",
      "Traceback (most recent call last):\n",
      "  File \"/Users/tcapelle/miniforge3/envs/weave/lib/python3.11/site-packages/weave/flow/eval.py\", line 168, in predict_and_score\n",
      "    model_output = await async_call(model_predict, **model_predict_args)\n",
      "                   ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/Users/tcapelle/miniforge3/envs/weave/lib/python3.11/site-packages/weave/trace/op.py\", line 333, in wrapper\n",
      "    res, _ = await _execute_call(wrapper, call, *args, **kwargs)  # type: ignore\n",
      "             ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/Users/tcapelle/miniforge3/envs/weave/lib/python3.11/site-packages/weave/trace/op.py\", line 213, in _call_async\n",
      "    return handle_exception(e)\n",
      "           ^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/Users/tcapelle/miniforge3/envs/weave/lib/python3.11/site-packages/weave/trace/op.py\", line 211, in _call_async\n",
      "    res = await func(*args, **kwargs)\n",
      "          ^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/var/folders/sf/tgv7vcv96x557p38bvvp1ms40000gn/T/ipykernel_6554/970167077.py\", line 10, in predict\n",
      "    return await rag_solver_with_reflection(\n",
      "           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/Users/tcapelle/miniforge3/envs/weave/lib/python3.11/site-packages/weave/trace/op.py\", line 333, in wrapper\n",
      "    res, _ = await _execute_call(wrapper, call, *args, **kwargs)  # type: ignore\n",
      "             ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/Users/tcapelle/miniforge3/envs/weave/lib/python3.11/site-packages/weave/trace/op.py\", line 213, in _call_async\n",
      "    return handle_exception(e)\n",
      "           ^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/Users/tcapelle/miniforge3/envs/weave/lib/python3.11/site-packages/weave/trace/op.py\", line 211, in _call_async\n",
      "    res = await func(*args, **kwargs)\n",
      "          ^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/var/folders/sf/tgv7vcv96x557p38bvvp1ms40000gn/T/ipykernel_6554/4070537091.py\", line 14, in rag_solver_with_reflection\n",
      "    rag_result = await rag_solver(\n",
      "                 ^^^^^^^^^^^^^^^^^\n",
      "  File \"/Users/tcapelle/miniforge3/envs/weave/lib/python3.11/site-packages/weave/trace/op.py\", line 333, in wrapper\n",
      "    res, _ = await _execute_call(wrapper, call, *args, **kwargs)  # type: ignore\n",
      "             ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/Users/tcapelle/miniforge3/envs/weave/lib/python3.11/site-packages/weave/trace/op.py\", line 213, in _call_async\n",
      "    return handle_exception(e)\n",
      "           ^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/Users/tcapelle/miniforge3/envs/weave/lib/python3.11/site-packages/weave/trace/op.py\", line 211, in _call_async\n",
      "    res = await func(*args, **kwargs)\n",
      "          ^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/Users/tcapelle/work/hackercup_rag/agent.py\", line 350, in rag_solver\n",
      "    zero_shot_result = await zero_shot_solver(\n",
      "                       ^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/Users/tcapelle/miniforge3/envs/weave/lib/python3.11/site-packages/weave/trace/op.py\", line 333, in wrapper\n",
      "    res, _ = await _execute_call(wrapper, call, *args, **kwargs)  # type: ignore\n",
      "             ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/Users/tcapelle/miniforge3/envs/weave/lib/python3.11/site-packages/weave/trace/op.py\", line 213, in _call_async\n",
      "    return handle_exception(e)\n",
      "           ^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/Users/tcapelle/miniforge3/envs/weave/lib/python3.11/site-packages/weave/trace/op.py\", line 211, in _call_async\n",
      "    res = await func(*args, **kwargs)\n",
      "          ^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/Users/tcapelle/work/hackercup_rag/agent.py\", line 330, in zero_shot_solver\n",
      "    solution = await draft_solution(\n",
      "               ^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/Users/tcapelle/miniforge3/envs/weave/lib/python3.11/site-packages/weave/trace/op.py\", line 333, in wrapper\n",
      "    res, _ = await _execute_call(wrapper, call, *args, **kwargs)  # type: ignore\n",
      "             ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/Users/tcapelle/miniforge3/envs/weave/lib/python3.11/site-packages/weave/trace/op.py\", line 213, in _call_async\n",
      "    return handle_exception(e)\n",
      "           ^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/Users/tcapelle/miniforge3/envs/weave/lib/python3.11/site-packages/weave/trace/op.py\", line 211, in _call_async\n",
      "    res = await func(*args, **kwargs)\n",
      "          ^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/Users/tcapelle/work/hackercup_rag/agent.py\", line 83, in draft_solution\n",
      "    response = await async_client.chat.completions.create(\n",
      "               ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/Users/tcapelle/miniforge3/envs/weave/lib/python3.11/site-packages/instructor/client.py\", line 300, in create\n",
      "    return await self.create_fn(\n",
      "           ^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/Users/tcapelle/miniforge3/envs/weave/lib/python3.11/site-packages/instructor/patch.py\", line 119, in new_create_async\n",
      "    response = await retry_async(\n",
      "               ^^^^^^^^^^^^^^^^^^\n",
      "  File \"/Users/tcapelle/miniforge3/envs/weave/lib/python3.11/site-packages/instructor/retry.py\", line 267, in retry_async\n",
      "    raise InstructorRetryException(\n",
      "instructor.exceptions.InstructorRetryException: RetryError[<Future at 0x33d391010 state=finished raised RateLimitError>]\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\">Predict and score failed\n",
       "</pre>\n"
      ],
      "text/plain": [
       "Predict and score failed\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Traceback (most recent call last):\n",
      "  File \"/Users/tcapelle/miniforge3/envs/weave/lib/python3.11/site-packages/weave/flow/eval.py\", line 287, in eval_example\n",
      "    eval_row = await self.predict_and_score(model, example)\n",
      "               ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/Users/tcapelle/miniforge3/envs/weave/lib/python3.11/site-packages/weave/trace/op.py\", line 333, in wrapper\n",
      "    res, _ = await _execute_call(wrapper, call, *args, **kwargs)  # type: ignore\n",
      "             ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/Users/tcapelle/miniforge3/envs/weave/lib/python3.11/site-packages/weave/trace/op.py\", line 213, in _call_async\n",
      "    return handle_exception(e)\n",
      "           ^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/Users/tcapelle/miniforge3/envs/weave/lib/python3.11/site-packages/weave/trace/op.py\", line 211, in _call_async\n",
      "    res = await func(*args, **kwargs)\n",
      "          ^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/Users/tcapelle/miniforge3/envs/weave/lib/python3.11/site-packages/weave/flow/eval.py\", line 224, in predict_and_score\n",
      "    result = await async_call(score_fn, **score_args)\n",
      "             ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/Users/tcapelle/miniforge3/envs/weave/lib/python3.11/asyncio/threads.py\", line 25, in to_thread\n",
      "    return await loop.run_in_executor(None, func_call)\n",
      "           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/Users/tcapelle/miniforge3/envs/weave/lib/python3.11/asyncio/futures.py\", line 287, in __await__\n",
      "    yield self  # This tells Task to wait for completion.\n",
      "    ^^^^^^^^^^\n",
      "  File \"/Users/tcapelle/miniforge3/envs/weave/lib/python3.11/asyncio/tasks.py\", line 349, in __wakeup\n",
      "    future.result()\n",
      "  File \"/Users/tcapelle/miniforge3/envs/weave/lib/python3.11/asyncio/futures.py\", line 203, in result\n",
      "    raise self._exception.with_traceback(self._exception_tb)\n",
      "  File \"/Users/tcapelle/miniforge3/envs/weave/lib/python3.11/concurrent/futures/thread.py\", line 58, in run\n",
      "    result = self.fn(*self.args, **self.kwargs)\n",
      "             ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/Users/tcapelle/miniforge3/envs/weave/lib/python3.11/site-packages/weave/trace/op.py\", line 344, in wrapper\n",
      "    res, _ = _execute_call(wrapper, call, *args, **kwargs)  # type: ignore\n",
      "             ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/Users/tcapelle/miniforge3/envs/weave/lib/python3.11/site-packages/weave/trace/op.py\", line 224, in _execute_call\n",
      "    handle_exception(e)\n",
      "  File \"/Users/tcapelle/miniforge3/envs/weave/lib/python3.11/site-packages/weave/trace/op.py\", line 222, in _execute_call\n",
      "    res = func(*args, **kwargs)\n",
      "          ^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/var/folders/sf/tgv7vcv96x557p38bvvp1ms40000gn/T/ipykernel_6554/249536452.py\", line 9, in scorer\n",
      "    return {\"passed\": expected == model_output[\"test_report\"]}\n",
      "                                  ~~~~~~~~~~~~^^^^^^^^^^^^^^^\n",
      "TypeError: 'NoneType' object is not subscriptable\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\">Evaluated <span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">1</span> of <span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">5</span> examples\n",
       "</pre>\n"
      ],
      "text/plain": [
       "Evaluated \u001b[1;36m1\u001b[0m of \u001b[1;36m5\u001b[0m examples\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2024-09-05 23:05:35,961 : INFO : HTTP Request: POST http://195.242.24.252:8000/v1/chat/completions \"HTTP/1.1 429 Too Many Requests\"\n",
      "2024-09-05 23:05:35,967 : ERROR : Failed after retries: <bound method Future.exception of <Future at 0x33d393450 state=finished raised RateLimitError>>\n",
      "Traceback (most recent call last):\n",
      "  File \"/Users/tcapelle/miniforge3/envs/weave/lib/python3.11/site-packages/litellm/llms/openai.py\", line 1089, in acompletion\n",
      "    headers, response = await self.make_openai_chat_completion_request(\n",
      "                        ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/Users/tcapelle/miniforge3/envs/weave/lib/python3.11/site-packages/litellm/llms/openai.py\", line 793, in make_openai_chat_completion_request\n",
      "    raise e\n",
      "  File \"/Users/tcapelle/miniforge3/envs/weave/lib/python3.11/site-packages/litellm/llms/openai.py\", line 781, in make_openai_chat_completion_request\n",
      "    await openai_aclient.chat.completions.with_raw_response.create(\n",
      "  File \"/Users/tcapelle/miniforge3/envs/weave/lib/python3.11/site-packages/openai/_legacy_response.py\", line 367, in wrapped\n",
      "    return cast(LegacyAPIResponse[R], await func(*args, **kwargs))\n",
      "                                      ^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/Users/tcapelle/miniforge3/envs/weave/lib/python3.11/site-packages/weave/trace/op.py\", line 333, in wrapper\n",
      "    res, _ = await _execute_call(wrapper, call, *args, **kwargs)  # type: ignore\n",
      "             ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/Users/tcapelle/miniforge3/envs/weave/lib/python3.11/site-packages/weave/trace/op.py\", line 213, in _call_async\n",
      "    return handle_exception(e)\n",
      "           ^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/Users/tcapelle/miniforge3/envs/weave/lib/python3.11/site-packages/weave/trace/op.py\", line 211, in _call_async\n",
      "    res = await func(*args, **kwargs)\n",
      "          ^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/Users/tcapelle/miniforge3/envs/weave/lib/python3.11/site-packages/weave/integrations/openai/openai_sdk.py\", line 288, in _wrapper\n",
      "    return await fn(*args, **kwargs)\n",
      "           ^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/Users/tcapelle/miniforge3/envs/weave/lib/python3.11/site-packages/openai/resources/chat/completions.py\", line 1339, in create\n",
      "    return await self._post(\n",
      "           ^^^^^^^^^^^^^^^^^\n",
      "  File \"/Users/tcapelle/miniforge3/envs/weave/lib/python3.11/site-packages/openai/_base_client.py\", line 1816, in post\n",
      "    return await self.request(cast_to, opts, stream=stream, stream_cls=stream_cls)\n",
      "           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/Users/tcapelle/miniforge3/envs/weave/lib/python3.11/site-packages/openai/_base_client.py\", line 1510, in request\n",
      "    return await self._request(\n",
      "           ^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/Users/tcapelle/miniforge3/envs/weave/lib/python3.11/site-packages/openai/_base_client.py\", line 1596, in _request\n",
      "    return await self._retry_request(\n",
      "           ^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/Users/tcapelle/miniforge3/envs/weave/lib/python3.11/site-packages/openai/_base_client.py\", line 1643, in _retry_request\n",
      "    return await self._request(\n",
      "           ^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/Users/tcapelle/miniforge3/envs/weave/lib/python3.11/site-packages/openai/_base_client.py\", line 1596, in _request\n",
      "    return await self._retry_request(\n",
      "           ^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/Users/tcapelle/miniforge3/envs/weave/lib/python3.11/site-packages/openai/_base_client.py\", line 1643, in _retry_request\n",
      "    return await self._request(\n",
      "           ^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/Users/tcapelle/miniforge3/envs/weave/lib/python3.11/site-packages/openai/_base_client.py\", line 1611, in _request\n",
      "    raise self._make_status_error_from_response(err.response) from None\n",
      "openai.RateLimitError: Error code: 429 - {'error': {'message': 'Rate limit exceeded. Please try again later.', 'type': 'rate_limit_error', 'param': None, 'code': 'rate_limit_exceeded'}}\n",
      "\n",
      "During handling of the above exception, another exception occurred:\n",
      "\n",
      "Traceback (most recent call last):\n",
      "  File \"/Users/tcapelle/miniforge3/envs/weave/lib/python3.11/site-packages/litellm/main.py\", line 427, in acompletion\n",
      "    response = await init_response\n",
      "               ^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/Users/tcapelle/miniforge3/envs/weave/lib/python3.11/site-packages/litellm/llms/openai.py\", line 1136, in acompletion\n",
      "    raise OpenAIError(\n",
      "litellm.llms.openai.OpenAIError: Error code: 429 - {'error': {'message': 'Rate limit exceeded. Please try again later.', 'type': 'rate_limit_error', 'param': None, 'code': 'rate_limit_exceeded'}}\n",
      "\n",
      "During handling of the above exception, another exception occurred:\n",
      "\n",
      "Traceback (most recent call last):\n",
      "  File \"/Users/tcapelle/miniforge3/envs/weave/lib/python3.11/site-packages/instructor/retry.py\", line 239, in retry_async\n",
      "    response: ChatCompletion = await func(*args, **kwargs)\n",
      "                               ^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/Users/tcapelle/miniforge3/envs/weave/lib/python3.11/site-packages/weave/trace/op.py\", line 333, in wrapper\n",
      "    res, _ = await _execute_call(wrapper, call, *args, **kwargs)  # type: ignore\n",
      "             ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/Users/tcapelle/miniforge3/envs/weave/lib/python3.11/site-packages/weave/trace/op.py\", line 213, in _call_async\n",
      "    return handle_exception(e)\n",
      "           ^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/Users/tcapelle/miniforge3/envs/weave/lib/python3.11/site-packages/weave/trace/op.py\", line 211, in _call_async\n",
      "    res = await func(*args, **kwargs)\n",
      "          ^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/Users/tcapelle/miniforge3/envs/weave/lib/python3.11/site-packages/litellm/utils.py\", line 1593, in wrapper_async\n",
      "    raise e\n",
      "  File \"/Users/tcapelle/miniforge3/envs/weave/lib/python3.11/site-packages/litellm/utils.py\", line 1413, in wrapper_async\n",
      "    result = await original_function(*args, **kwargs)\n",
      "             ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/Users/tcapelle/miniforge3/envs/weave/lib/python3.11/site-packages/litellm/main.py\", line 449, in acompletion\n",
      "    raise exception_type(\n",
      "          ^^^^^^^^^^^^^^^\n",
      "  File \"/Users/tcapelle/miniforge3/envs/weave/lib/python3.11/site-packages/litellm/utils.py\", line 8187, in exception_type\n",
      "    raise e\n",
      "  File \"/Users/tcapelle/miniforge3/envs/weave/lib/python3.11/site-packages/litellm/utils.py\", line 6555, in exception_type\n",
      "    raise RateLimitError(\n",
      "litellm.exceptions.RateLimitError: litellm.RateLimitError: RateLimitError: MistralException - Error code: 429 - {'error': {'message': 'Rate limit exceeded. Please try again later.', 'type': 'rate_limit_error', 'param': None, 'code': 'rate_limit_exceeded'}}\n",
      "\n",
      "The above exception was the direct cause of the following exception:\n",
      "\n",
      "Traceback (most recent call last):\n",
      "  File \"/Users/tcapelle/miniforge3/envs/weave/lib/python3.11/site-packages/instructor/retry.py\", line 235, in retry_async\n",
      "    async for attempt in max_retries:\n",
      "  File \"/Users/tcapelle/miniforge3/envs/weave/lib/python3.11/site-packages/tenacity/asyncio/__init__.py\", line 166, in __anext__\n",
      "    do = await self.iter(retry_state=self._retry_state)\n",
      "         ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/Users/tcapelle/miniforge3/envs/weave/lib/python3.11/site-packages/tenacity/asyncio/__init__.py\", line 153, in iter\n",
      "    result = await action(retry_state)\n",
      "             ^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/Users/tcapelle/miniforge3/envs/weave/lib/python3.11/site-packages/tenacity/_utils.py\", line 99, in inner\n",
      "    return call(*args, **kwargs)\n",
      "           ^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/Users/tcapelle/miniforge3/envs/weave/lib/python3.11/site-packages/tenacity/__init__.py\", line 419, in exc_check\n",
      "    raise retry_exc from fut.exception()\n",
      "tenacity.RetryError: RetryError[<Future at 0x33d393450 state=finished raised RateLimitError>]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "\u001b[1;31mGive Feedback / Get Help: https://github.com/BerriAI/litellm/issues/new\u001b[0m\n",
      "LiteLLM.Info: If you need to debug this error, use `litellm.set_verbose=True'.\n",
      "\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\">model_output failed\n",
       "</pre>\n"
      ],
      "text/plain": [
       "model_output failed\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Traceback (most recent call last):\n",
      "  File \"/Users/tcapelle/miniforge3/envs/weave/lib/python3.11/site-packages/litellm/llms/openai.py\", line 1089, in acompletion\n",
      "    headers, response = await self.make_openai_chat_completion_request(\n",
      "                        ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/Users/tcapelle/miniforge3/envs/weave/lib/python3.11/site-packages/litellm/llms/openai.py\", line 793, in make_openai_chat_completion_request\n",
      "    raise e\n",
      "  File \"/Users/tcapelle/miniforge3/envs/weave/lib/python3.11/site-packages/litellm/llms/openai.py\", line 781, in make_openai_chat_completion_request\n",
      "    await openai_aclient.chat.completions.with_raw_response.create(\n",
      "  File \"/Users/tcapelle/miniforge3/envs/weave/lib/python3.11/site-packages/openai/_legacy_response.py\", line 367, in wrapped\n",
      "    return cast(LegacyAPIResponse[R], await func(*args, **kwargs))\n",
      "                                      ^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/Users/tcapelle/miniforge3/envs/weave/lib/python3.11/site-packages/weave/trace/op.py\", line 333, in wrapper\n",
      "    res, _ = await _execute_call(wrapper, call, *args, **kwargs)  # type: ignore\n",
      "             ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/Users/tcapelle/miniforge3/envs/weave/lib/python3.11/site-packages/weave/trace/op.py\", line 213, in _call_async\n",
      "    return handle_exception(e)\n",
      "           ^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/Users/tcapelle/miniforge3/envs/weave/lib/python3.11/site-packages/weave/trace/op.py\", line 211, in _call_async\n",
      "    res = await func(*args, **kwargs)\n",
      "          ^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/Users/tcapelle/miniforge3/envs/weave/lib/python3.11/site-packages/weave/integrations/openai/openai_sdk.py\", line 288, in _wrapper\n",
      "    return await fn(*args, **kwargs)\n",
      "           ^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/Users/tcapelle/miniforge3/envs/weave/lib/python3.11/site-packages/openai/resources/chat/completions.py\", line 1339, in create\n",
      "    return await self._post(\n",
      "           ^^^^^^^^^^^^^^^^^\n",
      "  File \"/Users/tcapelle/miniforge3/envs/weave/lib/python3.11/site-packages/openai/_base_client.py\", line 1816, in post\n",
      "    return await self.request(cast_to, opts, stream=stream, stream_cls=stream_cls)\n",
      "           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/Users/tcapelle/miniforge3/envs/weave/lib/python3.11/site-packages/openai/_base_client.py\", line 1510, in request\n",
      "    return await self._request(\n",
      "           ^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/Users/tcapelle/miniforge3/envs/weave/lib/python3.11/site-packages/openai/_base_client.py\", line 1596, in _request\n",
      "    return await self._retry_request(\n",
      "           ^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/Users/tcapelle/miniforge3/envs/weave/lib/python3.11/site-packages/openai/_base_client.py\", line 1643, in _retry_request\n",
      "    return await self._request(\n",
      "           ^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/Users/tcapelle/miniforge3/envs/weave/lib/python3.11/site-packages/openai/_base_client.py\", line 1596, in _request\n",
      "    return await self._retry_request(\n",
      "           ^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/Users/tcapelle/miniforge3/envs/weave/lib/python3.11/site-packages/openai/_base_client.py\", line 1643, in _retry_request\n",
      "    return await self._request(\n",
      "           ^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/Users/tcapelle/miniforge3/envs/weave/lib/python3.11/site-packages/openai/_base_client.py\", line 1611, in _request\n",
      "    raise self._make_status_error_from_response(err.response) from None\n",
      "openai.RateLimitError: Error code: 429 - {'error': {'message': 'Rate limit exceeded. Please try again later.', 'type': 'rate_limit_error', 'param': None, 'code': 'rate_limit_exceeded'}}\n",
      "\n",
      "During handling of the above exception, another exception occurred:\n",
      "\n",
      "Traceback (most recent call last):\n",
      "  File \"/Users/tcapelle/miniforge3/envs/weave/lib/python3.11/site-packages/litellm/main.py\", line 427, in acompletion\n",
      "    response = await init_response\n",
      "               ^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/Users/tcapelle/miniforge3/envs/weave/lib/python3.11/site-packages/litellm/llms/openai.py\", line 1136, in acompletion\n",
      "    raise OpenAIError(\n",
      "litellm.llms.openai.OpenAIError: Error code: 429 - {'error': {'message': 'Rate limit exceeded. Please try again later.', 'type': 'rate_limit_error', 'param': None, 'code': 'rate_limit_exceeded'}}\n",
      "\n",
      "During handling of the above exception, another exception occurred:\n",
      "\n",
      "Traceback (most recent call last):\n",
      "  File \"/Users/tcapelle/miniforge3/envs/weave/lib/python3.11/site-packages/instructor/retry.py\", line 239, in retry_async\n",
      "    response: ChatCompletion = await func(*args, **kwargs)\n",
      "                               ^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/Users/tcapelle/miniforge3/envs/weave/lib/python3.11/site-packages/weave/trace/op.py\", line 333, in wrapper\n",
      "    res, _ = await _execute_call(wrapper, call, *args, **kwargs)  # type: ignore\n",
      "             ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/Users/tcapelle/miniforge3/envs/weave/lib/python3.11/site-packages/weave/trace/op.py\", line 213, in _call_async\n",
      "    return handle_exception(e)\n",
      "           ^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/Users/tcapelle/miniforge3/envs/weave/lib/python3.11/site-packages/weave/trace/op.py\", line 211, in _call_async\n",
      "    res = await func(*args, **kwargs)\n",
      "          ^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/Users/tcapelle/miniforge3/envs/weave/lib/python3.11/site-packages/litellm/utils.py\", line 1593, in wrapper_async\n",
      "    raise e\n",
      "  File \"/Users/tcapelle/miniforge3/envs/weave/lib/python3.11/site-packages/litellm/utils.py\", line 1413, in wrapper_async\n",
      "    result = await original_function(*args, **kwargs)\n",
      "             ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/Users/tcapelle/miniforge3/envs/weave/lib/python3.11/site-packages/litellm/main.py\", line 449, in acompletion\n",
      "    raise exception_type(\n",
      "          ^^^^^^^^^^^^^^^\n",
      "  File \"/Users/tcapelle/miniforge3/envs/weave/lib/python3.11/site-packages/litellm/utils.py\", line 8187, in exception_type\n",
      "    raise e\n",
      "  File \"/Users/tcapelle/miniforge3/envs/weave/lib/python3.11/site-packages/litellm/utils.py\", line 6555, in exception_type\n",
      "    raise RateLimitError(\n",
      "litellm.exceptions.RateLimitError: litellm.RateLimitError: RateLimitError: MistralException - Error code: 429 - {'error': {'message': 'Rate limit exceeded. Please try again later.', 'type': 'rate_limit_error', 'param': None, 'code': 'rate_limit_exceeded'}}\n",
      "\n",
      "The above exception was the direct cause of the following exception:\n",
      "\n",
      "Traceback (most recent call last):\n",
      "  File \"/Users/tcapelle/miniforge3/envs/weave/lib/python3.11/site-packages/instructor/retry.py\", line 235, in retry_async\n",
      "    async for attempt in max_retries:\n",
      "  File \"/Users/tcapelle/miniforge3/envs/weave/lib/python3.11/site-packages/tenacity/asyncio/__init__.py\", line 166, in __anext__\n",
      "    do = await self.iter(retry_state=self._retry_state)\n",
      "         ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/Users/tcapelle/miniforge3/envs/weave/lib/python3.11/site-packages/tenacity/asyncio/__init__.py\", line 153, in iter\n",
      "    result = await action(retry_state)\n",
      "             ^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/Users/tcapelle/miniforge3/envs/weave/lib/python3.11/site-packages/tenacity/_utils.py\", line 99, in inner\n",
      "    return call(*args, **kwargs)\n",
      "           ^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/Users/tcapelle/miniforge3/envs/weave/lib/python3.11/site-packages/tenacity/__init__.py\", line 419, in exc_check\n",
      "    raise retry_exc from fut.exception()\n",
      "tenacity.RetryError: RetryError[<Future at 0x33d393450 state=finished raised RateLimitError>]\n",
      "\n",
      "The above exception was the direct cause of the following exception:\n",
      "\n",
      "Traceback (most recent call last):\n",
      "  File \"/Users/tcapelle/miniforge3/envs/weave/lib/python3.11/site-packages/weave/flow/eval.py\", line 168, in predict_and_score\n",
      "    model_output = await async_call(model_predict, **model_predict_args)\n",
      "                   ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/Users/tcapelle/miniforge3/envs/weave/lib/python3.11/site-packages/weave/trace/op.py\", line 333, in wrapper\n",
      "    res, _ = await _execute_call(wrapper, call, *args, **kwargs)  # type: ignore\n",
      "             ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/Users/tcapelle/miniforge3/envs/weave/lib/python3.11/site-packages/weave/trace/op.py\", line 213, in _call_async\n",
      "    return handle_exception(e)\n",
      "           ^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/Users/tcapelle/miniforge3/envs/weave/lib/python3.11/site-packages/weave/trace/op.py\", line 211, in _call_async\n",
      "    res = await func(*args, **kwargs)\n",
      "          ^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/var/folders/sf/tgv7vcv96x557p38bvvp1ms40000gn/T/ipykernel_6554/970167077.py\", line 10, in predict\n",
      "    return await rag_solver_with_reflection(\n",
      "           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/Users/tcapelle/miniforge3/envs/weave/lib/python3.11/site-packages/weave/trace/op.py\", line 333, in wrapper\n",
      "    res, _ = await _execute_call(wrapper, call, *args, **kwargs)  # type: ignore\n",
      "             ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/Users/tcapelle/miniforge3/envs/weave/lib/python3.11/site-packages/weave/trace/op.py\", line 213, in _call_async\n",
      "    return handle_exception(e)\n",
      "           ^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/Users/tcapelle/miniforge3/envs/weave/lib/python3.11/site-packages/weave/trace/op.py\", line 211, in _call_async\n",
      "    res = await func(*args, **kwargs)\n",
      "          ^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/var/folders/sf/tgv7vcv96x557p38bvvp1ms40000gn/T/ipykernel_6554/4070537091.py\", line 14, in rag_solver_with_reflection\n",
      "    rag_result = await rag_solver(\n",
      "                 ^^^^^^^^^^^^^^^^^\n",
      "  File \"/Users/tcapelle/miniforge3/envs/weave/lib/python3.11/site-packages/weave/trace/op.py\", line 333, in wrapper\n",
      "    res, _ = await _execute_call(wrapper, call, *args, **kwargs)  # type: ignore\n",
      "             ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/Users/tcapelle/miniforge3/envs/weave/lib/python3.11/site-packages/weave/trace/op.py\", line 213, in _call_async\n",
      "    return handle_exception(e)\n",
      "           ^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/Users/tcapelle/miniforge3/envs/weave/lib/python3.11/site-packages/weave/trace/op.py\", line 211, in _call_async\n",
      "    res = await func(*args, **kwargs)\n",
      "          ^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/Users/tcapelle/work/hackercup_rag/agent.py\", line 350, in rag_solver\n",
      "    zero_shot_result = await zero_shot_solver(\n",
      "                       ^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/Users/tcapelle/miniforge3/envs/weave/lib/python3.11/site-packages/weave/trace/op.py\", line 333, in wrapper\n",
      "    res, _ = await _execute_call(wrapper, call, *args, **kwargs)  # type: ignore\n",
      "             ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/Users/tcapelle/miniforge3/envs/weave/lib/python3.11/site-packages/weave/trace/op.py\", line 213, in _call_async\n",
      "    return handle_exception(e)\n",
      "           ^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/Users/tcapelle/miniforge3/envs/weave/lib/python3.11/site-packages/weave/trace/op.py\", line 211, in _call_async\n",
      "    res = await func(*args, **kwargs)\n",
      "          ^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/Users/tcapelle/work/hackercup_rag/agent.py\", line 330, in zero_shot_solver\n",
      "    solution = await draft_solution(\n",
      "               ^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/Users/tcapelle/miniforge3/envs/weave/lib/python3.11/site-packages/weave/trace/op.py\", line 333, in wrapper\n",
      "    res, _ = await _execute_call(wrapper, call, *args, **kwargs)  # type: ignore\n",
      "             ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/Users/tcapelle/miniforge3/envs/weave/lib/python3.11/site-packages/weave/trace/op.py\", line 213, in _call_async\n",
      "    return handle_exception(e)\n",
      "           ^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/Users/tcapelle/miniforge3/envs/weave/lib/python3.11/site-packages/weave/trace/op.py\", line 211, in _call_async\n",
      "    res = await func(*args, **kwargs)\n",
      "          ^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/Users/tcapelle/work/hackercup_rag/agent.py\", line 83, in draft_solution\n",
      "    response = await async_client.chat.completions.create(\n",
      "               ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/Users/tcapelle/miniforge3/envs/weave/lib/python3.11/site-packages/instructor/client.py\", line 300, in create\n",
      "    return await self.create_fn(\n",
      "           ^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/Users/tcapelle/miniforge3/envs/weave/lib/python3.11/site-packages/instructor/patch.py\", line 119, in new_create_async\n",
      "    response = await retry_async(\n",
      "               ^^^^^^^^^^^^^^^^^^\n",
      "  File \"/Users/tcapelle/miniforge3/envs/weave/lib/python3.11/site-packages/instructor/retry.py\", line 267, in retry_async\n",
      "    raise InstructorRetryException(\n",
      "instructor.exceptions.InstructorRetryException: RetryError[<Future at 0x33d393450 state=finished raised RateLimitError>]\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\">Predict and score failed\n",
       "</pre>\n"
      ],
      "text/plain": [
       "Predict and score failed\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Traceback (most recent call last):\n",
      "  File \"/Users/tcapelle/miniforge3/envs/weave/lib/python3.11/site-packages/weave/flow/eval.py\", line 287, in eval_example\n",
      "    eval_row = await self.predict_and_score(model, example)\n",
      "               ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/Users/tcapelle/miniforge3/envs/weave/lib/python3.11/site-packages/weave/trace/op.py\", line 333, in wrapper\n",
      "    res, _ = await _execute_call(wrapper, call, *args, **kwargs)  # type: ignore\n",
      "             ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/Users/tcapelle/miniforge3/envs/weave/lib/python3.11/site-packages/weave/trace/op.py\", line 213, in _call_async\n",
      "    return handle_exception(e)\n",
      "           ^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/Users/tcapelle/miniforge3/envs/weave/lib/python3.11/site-packages/weave/trace/op.py\", line 211, in _call_async\n",
      "    res = await func(*args, **kwargs)\n",
      "          ^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/Users/tcapelle/miniforge3/envs/weave/lib/python3.11/site-packages/weave/flow/eval.py\", line 224, in predict_and_score\n",
      "    result = await async_call(score_fn, **score_args)\n",
      "             ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/Users/tcapelle/miniforge3/envs/weave/lib/python3.11/asyncio/threads.py\", line 25, in to_thread\n",
      "    return await loop.run_in_executor(None, func_call)\n",
      "           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/Users/tcapelle/miniforge3/envs/weave/lib/python3.11/asyncio/futures.py\", line 287, in __await__\n",
      "    yield self  # This tells Task to wait for completion.\n",
      "    ^^^^^^^^^^\n",
      "  File \"/Users/tcapelle/miniforge3/envs/weave/lib/python3.11/asyncio/tasks.py\", line 349, in __wakeup\n",
      "    future.result()\n",
      "  File \"/Users/tcapelle/miniforge3/envs/weave/lib/python3.11/asyncio/futures.py\", line 203, in result\n",
      "    raise self._exception.with_traceback(self._exception_tb)\n",
      "  File \"/Users/tcapelle/miniforge3/envs/weave/lib/python3.11/concurrent/futures/thread.py\", line 58, in run\n",
      "    result = self.fn(*self.args, **self.kwargs)\n",
      "             ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/Users/tcapelle/miniforge3/envs/weave/lib/python3.11/site-packages/weave/trace/op.py\", line 344, in wrapper\n",
      "    res, _ = _execute_call(wrapper, call, *args, **kwargs)  # type: ignore\n",
      "             ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/Users/tcapelle/miniforge3/envs/weave/lib/python3.11/site-packages/weave/trace/op.py\", line 224, in _execute_call\n",
      "    handle_exception(e)\n",
      "  File \"/Users/tcapelle/miniforge3/envs/weave/lib/python3.11/site-packages/weave/trace/op.py\", line 222, in _execute_call\n",
      "    res = func(*args, **kwargs)\n",
      "          ^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/var/folders/sf/tgv7vcv96x557p38bvvp1ms40000gn/T/ipykernel_6554/249536452.py\", line 9, in scorer\n",
      "    return {\"passed\": expected == model_output[\"test_report\"]}\n",
      "                                  ~~~~~~~~~~~~^^^^^^^^^^^^^^^\n",
      "TypeError: 'NoneType' object is not subscriptable\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\">Evaluated <span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">2</span> of <span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">5</span> examples\n",
       "</pre>\n"
      ],
      "text/plain": [
       "Evaluated \u001b[1;36m2\u001b[0m of \u001b[1;36m5\u001b[0m examples\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2024-09-05 23:05:36,059 : INFO : HTTP Request: POST http://195.242.24.252:8000/v1/chat/completions \"HTTP/1.1 429 Too Many Requests\"\n",
      "2024-09-05 23:05:36,065 : ERROR : Failed after retries: <bound method Future.exception of <Future at 0x33d3c08d0 state=finished raised RateLimitError>>\n",
      "Traceback (most recent call last):\n",
      "  File \"/Users/tcapelle/miniforge3/envs/weave/lib/python3.11/site-packages/litellm/llms/openai.py\", line 1089, in acompletion\n",
      "    headers, response = await self.make_openai_chat_completion_request(\n",
      "                        ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/Users/tcapelle/miniforge3/envs/weave/lib/python3.11/site-packages/litellm/llms/openai.py\", line 793, in make_openai_chat_completion_request\n",
      "    raise e\n",
      "  File \"/Users/tcapelle/miniforge3/envs/weave/lib/python3.11/site-packages/litellm/llms/openai.py\", line 781, in make_openai_chat_completion_request\n",
      "    await openai_aclient.chat.completions.with_raw_response.create(\n",
      "  File \"/Users/tcapelle/miniforge3/envs/weave/lib/python3.11/site-packages/openai/_legacy_response.py\", line 367, in wrapped\n",
      "    return cast(LegacyAPIResponse[R], await func(*args, **kwargs))\n",
      "                                      ^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/Users/tcapelle/miniforge3/envs/weave/lib/python3.11/site-packages/weave/trace/op.py\", line 333, in wrapper\n",
      "    res, _ = await _execute_call(wrapper, call, *args, **kwargs)  # type: ignore\n",
      "             ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/Users/tcapelle/miniforge3/envs/weave/lib/python3.11/site-packages/weave/trace/op.py\", line 213, in _call_async\n",
      "    return handle_exception(e)\n",
      "           ^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/Users/tcapelle/miniforge3/envs/weave/lib/python3.11/site-packages/weave/trace/op.py\", line 211, in _call_async\n",
      "    res = await func(*args, **kwargs)\n",
      "          ^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/Users/tcapelle/miniforge3/envs/weave/lib/python3.11/site-packages/weave/integrations/openai/openai_sdk.py\", line 288, in _wrapper\n",
      "    return await fn(*args, **kwargs)\n",
      "           ^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/Users/tcapelle/miniforge3/envs/weave/lib/python3.11/site-packages/openai/resources/chat/completions.py\", line 1339, in create\n",
      "    return await self._post(\n",
      "           ^^^^^^^^^^^^^^^^^\n",
      "  File \"/Users/tcapelle/miniforge3/envs/weave/lib/python3.11/site-packages/openai/_base_client.py\", line 1816, in post\n",
      "    return await self.request(cast_to, opts, stream=stream, stream_cls=stream_cls)\n",
      "           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/Users/tcapelle/miniforge3/envs/weave/lib/python3.11/site-packages/openai/_base_client.py\", line 1510, in request\n",
      "    return await self._request(\n",
      "           ^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/Users/tcapelle/miniforge3/envs/weave/lib/python3.11/site-packages/openai/_base_client.py\", line 1596, in _request\n",
      "    return await self._retry_request(\n",
      "           ^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/Users/tcapelle/miniforge3/envs/weave/lib/python3.11/site-packages/openai/_base_client.py\", line 1643, in _retry_request\n",
      "    return await self._request(\n",
      "           ^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/Users/tcapelle/miniforge3/envs/weave/lib/python3.11/site-packages/openai/_base_client.py\", line 1596, in _request\n",
      "    return await self._retry_request(\n",
      "           ^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/Users/tcapelle/miniforge3/envs/weave/lib/python3.11/site-packages/openai/_base_client.py\", line 1643, in _retry_request\n",
      "    return await self._request(\n",
      "           ^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/Users/tcapelle/miniforge3/envs/weave/lib/python3.11/site-packages/openai/_base_client.py\", line 1611, in _request\n",
      "    raise self._make_status_error_from_response(err.response) from None\n",
      "openai.RateLimitError: Error code: 429 - {'error': {'message': 'Rate limit exceeded. Please try again later.', 'type': 'rate_limit_error', 'param': None, 'code': 'rate_limit_exceeded'}}\n",
      "\n",
      "During handling of the above exception, another exception occurred:\n",
      "\n",
      "Traceback (most recent call last):\n",
      "  File \"/Users/tcapelle/miniforge3/envs/weave/lib/python3.11/site-packages/litellm/main.py\", line 427, in acompletion\n",
      "    response = await init_response\n",
      "               ^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/Users/tcapelle/miniforge3/envs/weave/lib/python3.11/site-packages/litellm/llms/openai.py\", line 1136, in acompletion\n",
      "    raise OpenAIError(\n",
      "litellm.llms.openai.OpenAIError: Error code: 429 - {'error': {'message': 'Rate limit exceeded. Please try again later.', 'type': 'rate_limit_error', 'param': None, 'code': 'rate_limit_exceeded'}}\n",
      "\n",
      "During handling of the above exception, another exception occurred:\n",
      "\n",
      "Traceback (most recent call last):\n",
      "  File \"/Users/tcapelle/miniforge3/envs/weave/lib/python3.11/site-packages/instructor/retry.py\", line 239, in retry_async\n",
      "    response: ChatCompletion = await func(*args, **kwargs)\n",
      "                               ^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/Users/tcapelle/miniforge3/envs/weave/lib/python3.11/site-packages/weave/trace/op.py\", line 333, in wrapper\n",
      "    res, _ = await _execute_call(wrapper, call, *args, **kwargs)  # type: ignore\n",
      "             ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/Users/tcapelle/miniforge3/envs/weave/lib/python3.11/site-packages/weave/trace/op.py\", line 213, in _call_async\n",
      "    return handle_exception(e)\n",
      "           ^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/Users/tcapelle/miniforge3/envs/weave/lib/python3.11/site-packages/weave/trace/op.py\", line 211, in _call_async\n",
      "    res = await func(*args, **kwargs)\n",
      "          ^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/Users/tcapelle/miniforge3/envs/weave/lib/python3.11/site-packages/litellm/utils.py\", line 1593, in wrapper_async\n",
      "    raise e\n",
      "  File \"/Users/tcapelle/miniforge3/envs/weave/lib/python3.11/site-packages/litellm/utils.py\", line 1413, in wrapper_async\n",
      "    result = await original_function(*args, **kwargs)\n",
      "             ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/Users/tcapelle/miniforge3/envs/weave/lib/python3.11/site-packages/litellm/main.py\", line 449, in acompletion\n",
      "    raise exception_type(\n",
      "          ^^^^^^^^^^^^^^^\n",
      "  File \"/Users/tcapelle/miniforge3/envs/weave/lib/python3.11/site-packages/litellm/utils.py\", line 8187, in exception_type\n",
      "    raise e\n",
      "  File \"/Users/tcapelle/miniforge3/envs/weave/lib/python3.11/site-packages/litellm/utils.py\", line 6555, in exception_type\n",
      "    raise RateLimitError(\n",
      "litellm.exceptions.RateLimitError: litellm.RateLimitError: RateLimitError: MistralException - Error code: 429 - {'error': {'message': 'Rate limit exceeded. Please try again later.', 'type': 'rate_limit_error', 'param': None, 'code': 'rate_limit_exceeded'}}\n",
      "\n",
      "The above exception was the direct cause of the following exception:\n",
      "\n",
      "Traceback (most recent call last):\n",
      "  File \"/Users/tcapelle/miniforge3/envs/weave/lib/python3.11/site-packages/instructor/retry.py\", line 235, in retry_async\n",
      "    async for attempt in max_retries:\n",
      "  File \"/Users/tcapelle/miniforge3/envs/weave/lib/python3.11/site-packages/tenacity/asyncio/__init__.py\", line 166, in __anext__\n",
      "    do = await self.iter(retry_state=self._retry_state)\n",
      "         ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/Users/tcapelle/miniforge3/envs/weave/lib/python3.11/site-packages/tenacity/asyncio/__init__.py\", line 153, in iter\n",
      "    result = await action(retry_state)\n",
      "             ^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/Users/tcapelle/miniforge3/envs/weave/lib/python3.11/site-packages/tenacity/_utils.py\", line 99, in inner\n",
      "    return call(*args, **kwargs)\n",
      "           ^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/Users/tcapelle/miniforge3/envs/weave/lib/python3.11/site-packages/tenacity/__init__.py\", line 419, in exc_check\n",
      "    raise retry_exc from fut.exception()\n",
      "tenacity.RetryError: RetryError[<Future at 0x33d3c08d0 state=finished raised RateLimitError>]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "\u001b[1;31mGive Feedback / Get Help: https://github.com/BerriAI/litellm/issues/new\u001b[0m\n",
      "LiteLLM.Info: If you need to debug this error, use `litellm.set_verbose=True'.\n",
      "\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\">model_output failed\n",
       "</pre>\n"
      ],
      "text/plain": [
       "model_output failed\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Traceback (most recent call last):\n",
      "  File \"/Users/tcapelle/miniforge3/envs/weave/lib/python3.11/site-packages/litellm/llms/openai.py\", line 1089, in acompletion\n",
      "    headers, response = await self.make_openai_chat_completion_request(\n",
      "                        ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/Users/tcapelle/miniforge3/envs/weave/lib/python3.11/site-packages/litellm/llms/openai.py\", line 793, in make_openai_chat_completion_request\n",
      "    raise e\n",
      "  File \"/Users/tcapelle/miniforge3/envs/weave/lib/python3.11/site-packages/litellm/llms/openai.py\", line 781, in make_openai_chat_completion_request\n",
      "    await openai_aclient.chat.completions.with_raw_response.create(\n",
      "  File \"/Users/tcapelle/miniforge3/envs/weave/lib/python3.11/site-packages/openai/_legacy_response.py\", line 367, in wrapped\n",
      "    return cast(LegacyAPIResponse[R], await func(*args, **kwargs))\n",
      "                                      ^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/Users/tcapelle/miniforge3/envs/weave/lib/python3.11/site-packages/weave/trace/op.py\", line 333, in wrapper\n",
      "    res, _ = await _execute_call(wrapper, call, *args, **kwargs)  # type: ignore\n",
      "             ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/Users/tcapelle/miniforge3/envs/weave/lib/python3.11/site-packages/weave/trace/op.py\", line 213, in _call_async\n",
      "    return handle_exception(e)\n",
      "           ^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/Users/tcapelle/miniforge3/envs/weave/lib/python3.11/site-packages/weave/trace/op.py\", line 211, in _call_async\n",
      "    res = await func(*args, **kwargs)\n",
      "          ^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/Users/tcapelle/miniforge3/envs/weave/lib/python3.11/site-packages/weave/integrations/openai/openai_sdk.py\", line 288, in _wrapper\n",
      "    return await fn(*args, **kwargs)\n",
      "           ^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/Users/tcapelle/miniforge3/envs/weave/lib/python3.11/site-packages/openai/resources/chat/completions.py\", line 1339, in create\n",
      "    return await self._post(\n",
      "           ^^^^^^^^^^^^^^^^^\n",
      "  File \"/Users/tcapelle/miniforge3/envs/weave/lib/python3.11/site-packages/openai/_base_client.py\", line 1816, in post\n",
      "    return await self.request(cast_to, opts, stream=stream, stream_cls=stream_cls)\n",
      "           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/Users/tcapelle/miniforge3/envs/weave/lib/python3.11/site-packages/openai/_base_client.py\", line 1510, in request\n",
      "    return await self._request(\n",
      "           ^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/Users/tcapelle/miniforge3/envs/weave/lib/python3.11/site-packages/openai/_base_client.py\", line 1596, in _request\n",
      "    return await self._retry_request(\n",
      "           ^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/Users/tcapelle/miniforge3/envs/weave/lib/python3.11/site-packages/openai/_base_client.py\", line 1643, in _retry_request\n",
      "    return await self._request(\n",
      "           ^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/Users/tcapelle/miniforge3/envs/weave/lib/python3.11/site-packages/openai/_base_client.py\", line 1596, in _request\n",
      "    return await self._retry_request(\n",
      "           ^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/Users/tcapelle/miniforge3/envs/weave/lib/python3.11/site-packages/openai/_base_client.py\", line 1643, in _retry_request\n",
      "    return await self._request(\n",
      "           ^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/Users/tcapelle/miniforge3/envs/weave/lib/python3.11/site-packages/openai/_base_client.py\", line 1611, in _request\n",
      "    raise self._make_status_error_from_response(err.response) from None\n",
      "openai.RateLimitError: Error code: 429 - {'error': {'message': 'Rate limit exceeded. Please try again later.', 'type': 'rate_limit_error', 'param': None, 'code': 'rate_limit_exceeded'}}\n",
      "\n",
      "During handling of the above exception, another exception occurred:\n",
      "\n",
      "Traceback (most recent call last):\n",
      "  File \"/Users/tcapelle/miniforge3/envs/weave/lib/python3.11/site-packages/litellm/main.py\", line 427, in acompletion\n",
      "    response = await init_response\n",
      "               ^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/Users/tcapelle/miniforge3/envs/weave/lib/python3.11/site-packages/litellm/llms/openai.py\", line 1136, in acompletion\n",
      "    raise OpenAIError(\n",
      "litellm.llms.openai.OpenAIError: Error code: 429 - {'error': {'message': 'Rate limit exceeded. Please try again later.', 'type': 'rate_limit_error', 'param': None, 'code': 'rate_limit_exceeded'}}\n",
      "\n",
      "During handling of the above exception, another exception occurred:\n",
      "\n",
      "Traceback (most recent call last):\n",
      "  File \"/Users/tcapelle/miniforge3/envs/weave/lib/python3.11/site-packages/instructor/retry.py\", line 239, in retry_async\n",
      "    response: ChatCompletion = await func(*args, **kwargs)\n",
      "                               ^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/Users/tcapelle/miniforge3/envs/weave/lib/python3.11/site-packages/weave/trace/op.py\", line 333, in wrapper\n",
      "    res, _ = await _execute_call(wrapper, call, *args, **kwargs)  # type: ignore\n",
      "             ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/Users/tcapelle/miniforge3/envs/weave/lib/python3.11/site-packages/weave/trace/op.py\", line 213, in _call_async\n",
      "    return handle_exception(e)\n",
      "           ^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/Users/tcapelle/miniforge3/envs/weave/lib/python3.11/site-packages/weave/trace/op.py\", line 211, in _call_async\n",
      "    res = await func(*args, **kwargs)\n",
      "          ^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/Users/tcapelle/miniforge3/envs/weave/lib/python3.11/site-packages/litellm/utils.py\", line 1593, in wrapper_async\n",
      "    raise e\n",
      "  File \"/Users/tcapelle/miniforge3/envs/weave/lib/python3.11/site-packages/litellm/utils.py\", line 1413, in wrapper_async\n",
      "    result = await original_function(*args, **kwargs)\n",
      "             ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/Users/tcapelle/miniforge3/envs/weave/lib/python3.11/site-packages/litellm/main.py\", line 449, in acompletion\n",
      "    raise exception_type(\n",
      "          ^^^^^^^^^^^^^^^\n",
      "  File \"/Users/tcapelle/miniforge3/envs/weave/lib/python3.11/site-packages/litellm/utils.py\", line 8187, in exception_type\n",
      "    raise e\n",
      "  File \"/Users/tcapelle/miniforge3/envs/weave/lib/python3.11/site-packages/litellm/utils.py\", line 6555, in exception_type\n",
      "    raise RateLimitError(\n",
      "litellm.exceptions.RateLimitError: litellm.RateLimitError: RateLimitError: MistralException - Error code: 429 - {'error': {'message': 'Rate limit exceeded. Please try again later.', 'type': 'rate_limit_error', 'param': None, 'code': 'rate_limit_exceeded'}}\n",
      "\n",
      "The above exception was the direct cause of the following exception:\n",
      "\n",
      "Traceback (most recent call last):\n",
      "  File \"/Users/tcapelle/miniforge3/envs/weave/lib/python3.11/site-packages/instructor/retry.py\", line 235, in retry_async\n",
      "    async for attempt in max_retries:\n",
      "  File \"/Users/tcapelle/miniforge3/envs/weave/lib/python3.11/site-packages/tenacity/asyncio/__init__.py\", line 166, in __anext__\n",
      "    do = await self.iter(retry_state=self._retry_state)\n",
      "         ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/Users/tcapelle/miniforge3/envs/weave/lib/python3.11/site-packages/tenacity/asyncio/__init__.py\", line 153, in iter\n",
      "    result = await action(retry_state)\n",
      "             ^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/Users/tcapelle/miniforge3/envs/weave/lib/python3.11/site-packages/tenacity/_utils.py\", line 99, in inner\n",
      "    return call(*args, **kwargs)\n",
      "           ^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/Users/tcapelle/miniforge3/envs/weave/lib/python3.11/site-packages/tenacity/__init__.py\", line 419, in exc_check\n",
      "    raise retry_exc from fut.exception()\n",
      "tenacity.RetryError: RetryError[<Future at 0x33d3c08d0 state=finished raised RateLimitError>]\n",
      "\n",
      "The above exception was the direct cause of the following exception:\n",
      "\n",
      "Traceback (most recent call last):\n",
      "  File \"/Users/tcapelle/miniforge3/envs/weave/lib/python3.11/site-packages/weave/flow/eval.py\", line 168, in predict_and_score\n",
      "    model_output = await async_call(model_predict, **model_predict_args)\n",
      "                   ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/Users/tcapelle/miniforge3/envs/weave/lib/python3.11/site-packages/weave/trace/op.py\", line 333, in wrapper\n",
      "    res, _ = await _execute_call(wrapper, call, *args, **kwargs)  # type: ignore\n",
      "             ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/Users/tcapelle/miniforge3/envs/weave/lib/python3.11/site-packages/weave/trace/op.py\", line 213, in _call_async\n",
      "    return handle_exception(e)\n",
      "           ^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/Users/tcapelle/miniforge3/envs/weave/lib/python3.11/site-packages/weave/trace/op.py\", line 211, in _call_async\n",
      "    res = await func(*args, **kwargs)\n",
      "          ^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/var/folders/sf/tgv7vcv96x557p38bvvp1ms40000gn/T/ipykernel_6554/970167077.py\", line 10, in predict\n",
      "    return await rag_solver_with_reflection(\n",
      "           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/Users/tcapelle/miniforge3/envs/weave/lib/python3.11/site-packages/weave/trace/op.py\", line 333, in wrapper\n",
      "    res, _ = await _execute_call(wrapper, call, *args, **kwargs)  # type: ignore\n",
      "             ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/Users/tcapelle/miniforge3/envs/weave/lib/python3.11/site-packages/weave/trace/op.py\", line 213, in _call_async\n",
      "    return handle_exception(e)\n",
      "           ^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/Users/tcapelle/miniforge3/envs/weave/lib/python3.11/site-packages/weave/trace/op.py\", line 211, in _call_async\n",
      "    res = await func(*args, **kwargs)\n",
      "          ^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/var/folders/sf/tgv7vcv96x557p38bvvp1ms40000gn/T/ipykernel_6554/4070537091.py\", line 14, in rag_solver_with_reflection\n",
      "    rag_result = await rag_solver(\n",
      "                 ^^^^^^^^^^^^^^^^^\n",
      "  File \"/Users/tcapelle/miniforge3/envs/weave/lib/python3.11/site-packages/weave/trace/op.py\", line 333, in wrapper\n",
      "    res, _ = await _execute_call(wrapper, call, *args, **kwargs)  # type: ignore\n",
      "             ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/Users/tcapelle/miniforge3/envs/weave/lib/python3.11/site-packages/weave/trace/op.py\", line 213, in _call_async\n",
      "    return handle_exception(e)\n",
      "           ^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/Users/tcapelle/miniforge3/envs/weave/lib/python3.11/site-packages/weave/trace/op.py\", line 211, in _call_async\n",
      "    res = await func(*args, **kwargs)\n",
      "          ^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/Users/tcapelle/work/hackercup_rag/agent.py\", line 350, in rag_solver\n",
      "    zero_shot_result = await zero_shot_solver(\n",
      "                       ^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/Users/tcapelle/miniforge3/envs/weave/lib/python3.11/site-packages/weave/trace/op.py\", line 333, in wrapper\n",
      "    res, _ = await _execute_call(wrapper, call, *args, **kwargs)  # type: ignore\n",
      "             ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/Users/tcapelle/miniforge3/envs/weave/lib/python3.11/site-packages/weave/trace/op.py\", line 213, in _call_async\n",
      "    return handle_exception(e)\n",
      "           ^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/Users/tcapelle/miniforge3/envs/weave/lib/python3.11/site-packages/weave/trace/op.py\", line 211, in _call_async\n",
      "    res = await func(*args, **kwargs)\n",
      "          ^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/Users/tcapelle/work/hackercup_rag/agent.py\", line 330, in zero_shot_solver\n",
      "    solution = await draft_solution(\n",
      "               ^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/Users/tcapelle/miniforge3/envs/weave/lib/python3.11/site-packages/weave/trace/op.py\", line 333, in wrapper\n",
      "    res, _ = await _execute_call(wrapper, call, *args, **kwargs)  # type: ignore\n",
      "             ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/Users/tcapelle/miniforge3/envs/weave/lib/python3.11/site-packages/weave/trace/op.py\", line 213, in _call_async\n",
      "    return handle_exception(e)\n",
      "           ^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/Users/tcapelle/miniforge3/envs/weave/lib/python3.11/site-packages/weave/trace/op.py\", line 211, in _call_async\n",
      "    res = await func(*args, **kwargs)\n",
      "          ^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/Users/tcapelle/work/hackercup_rag/agent.py\", line 83, in draft_solution\n",
      "    response = await async_client.chat.completions.create(\n",
      "               ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/Users/tcapelle/miniforge3/envs/weave/lib/python3.11/site-packages/instructor/client.py\", line 300, in create\n",
      "    return await self.create_fn(\n",
      "           ^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/Users/tcapelle/miniforge3/envs/weave/lib/python3.11/site-packages/instructor/patch.py\", line 119, in new_create_async\n",
      "    response = await retry_async(\n",
      "               ^^^^^^^^^^^^^^^^^^\n",
      "  File \"/Users/tcapelle/miniforge3/envs/weave/lib/python3.11/site-packages/instructor/retry.py\", line 267, in retry_async\n",
      "    raise InstructorRetryException(\n",
      "instructor.exceptions.InstructorRetryException: RetryError[<Future at 0x33d3c08d0 state=finished raised RateLimitError>]\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\">Predict and score failed\n",
       "</pre>\n"
      ],
      "text/plain": [
       "Predict and score failed\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Traceback (most recent call last):\n",
      "  File \"/Users/tcapelle/miniforge3/envs/weave/lib/python3.11/site-packages/weave/flow/eval.py\", line 287, in eval_example\n",
      "    eval_row = await self.predict_and_score(model, example)\n",
      "               ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/Users/tcapelle/miniforge3/envs/weave/lib/python3.11/site-packages/weave/trace/op.py\", line 333, in wrapper\n",
      "    res, _ = await _execute_call(wrapper, call, *args, **kwargs)  # type: ignore\n",
      "             ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/Users/tcapelle/miniforge3/envs/weave/lib/python3.11/site-packages/weave/trace/op.py\", line 213, in _call_async\n",
      "    return handle_exception(e)\n",
      "           ^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/Users/tcapelle/miniforge3/envs/weave/lib/python3.11/site-packages/weave/trace/op.py\", line 211, in _call_async\n",
      "    res = await func(*args, **kwargs)\n",
      "          ^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/Users/tcapelle/miniforge3/envs/weave/lib/python3.11/site-packages/weave/flow/eval.py\", line 224, in predict_and_score\n",
      "    result = await async_call(score_fn, **score_args)\n",
      "             ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/Users/tcapelle/miniforge3/envs/weave/lib/python3.11/asyncio/threads.py\", line 25, in to_thread\n",
      "    return await loop.run_in_executor(None, func_call)\n",
      "           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/Users/tcapelle/miniforge3/envs/weave/lib/python3.11/asyncio/futures.py\", line 287, in __await__\n",
      "    yield self  # This tells Task to wait for completion.\n",
      "    ^^^^^^^^^^\n",
      "  File \"/Users/tcapelle/miniforge3/envs/weave/lib/python3.11/asyncio/tasks.py\", line 349, in __wakeup\n",
      "    future.result()\n",
      "  File \"/Users/tcapelle/miniforge3/envs/weave/lib/python3.11/asyncio/futures.py\", line 203, in result\n",
      "    raise self._exception.with_traceback(self._exception_tb)\n",
      "  File \"/Users/tcapelle/miniforge3/envs/weave/lib/python3.11/concurrent/futures/thread.py\", line 58, in run\n",
      "    result = self.fn(*self.args, **self.kwargs)\n",
      "             ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/Users/tcapelle/miniforge3/envs/weave/lib/python3.11/site-packages/weave/trace/op.py\", line 344, in wrapper\n",
      "    res, _ = _execute_call(wrapper, call, *args, **kwargs)  # type: ignore\n",
      "             ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/Users/tcapelle/miniforge3/envs/weave/lib/python3.11/site-packages/weave/trace/op.py\", line 224, in _execute_call\n",
      "    handle_exception(e)\n",
      "  File \"/Users/tcapelle/miniforge3/envs/weave/lib/python3.11/site-packages/weave/trace/op.py\", line 222, in _execute_call\n",
      "    res = func(*args, **kwargs)\n",
      "          ^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/var/folders/sf/tgv7vcv96x557p38bvvp1ms40000gn/T/ipykernel_6554/249536452.py\", line 9, in scorer\n",
      "    return {\"passed\": expected == model_output[\"test_report\"]}\n",
      "                                  ~~~~~~~~~~~~^^^^^^^^^^^^^^^\n",
      "TypeError: 'NoneType' object is not subscriptable\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\">Evaluated <span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">3</span> of <span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">5</span> examples\n",
       "</pre>\n"
      ],
      "text/plain": [
       "Evaluated \u001b[1;36m3\u001b[0m of \u001b[1;36m5\u001b[0m examples\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2024-09-05 23:05:36,079 : INFO : HTTP Request: POST http://195.242.24.252:8000/v1/chat/completions \"HTTP/1.1 429 Too Many Requests\"\n",
      "\u001b[92m23:05:36 - LiteLLM:INFO\u001b[0m: utils.py:2975 - \n",
      "LiteLLM completion() model= open-mistral-nemo-2407; provider = mistral\n",
      "2024-09-05 23:05:36,088 : INFO : \n",
      "LiteLLM completion() model= open-mistral-nemo-2407; provider = mistral\n",
      "2024-09-05 23:05:36,130 : INFO : HTTP Request: POST http://195.242.24.252:8000/v1/chat/completions \"HTTP/1.1 429 Too Many Requests\"\n",
      "2024-09-05 23:05:36,136 : ERROR : Failed after retries: <bound method Future.exception of <Future at 0x33d3d7590 state=finished raised RateLimitError>>\n",
      "Traceback (most recent call last):\n",
      "  File \"/Users/tcapelle/miniforge3/envs/weave/lib/python3.11/site-packages/litellm/llms/openai.py\", line 1089, in acompletion\n",
      "    headers, response = await self.make_openai_chat_completion_request(\n",
      "                        ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/Users/tcapelle/miniforge3/envs/weave/lib/python3.11/site-packages/litellm/llms/openai.py\", line 793, in make_openai_chat_completion_request\n",
      "    raise e\n",
      "  File \"/Users/tcapelle/miniforge3/envs/weave/lib/python3.11/site-packages/litellm/llms/openai.py\", line 781, in make_openai_chat_completion_request\n",
      "    await openai_aclient.chat.completions.with_raw_response.create(\n",
      "  File \"/Users/tcapelle/miniforge3/envs/weave/lib/python3.11/site-packages/openai/_legacy_response.py\", line 367, in wrapped\n",
      "    return cast(LegacyAPIResponse[R], await func(*args, **kwargs))\n",
      "                                      ^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/Users/tcapelle/miniforge3/envs/weave/lib/python3.11/site-packages/weave/trace/op.py\", line 333, in wrapper\n",
      "    res, _ = await _execute_call(wrapper, call, *args, **kwargs)  # type: ignore\n",
      "             ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/Users/tcapelle/miniforge3/envs/weave/lib/python3.11/site-packages/weave/trace/op.py\", line 213, in _call_async\n",
      "    return handle_exception(e)\n",
      "           ^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/Users/tcapelle/miniforge3/envs/weave/lib/python3.11/site-packages/weave/trace/op.py\", line 211, in _call_async\n",
      "    res = await func(*args, **kwargs)\n",
      "          ^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/Users/tcapelle/miniforge3/envs/weave/lib/python3.11/site-packages/weave/integrations/openai/openai_sdk.py\", line 288, in _wrapper\n",
      "    return await fn(*args, **kwargs)\n",
      "           ^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/Users/tcapelle/miniforge3/envs/weave/lib/python3.11/site-packages/openai/resources/chat/completions.py\", line 1339, in create\n",
      "    return await self._post(\n",
      "           ^^^^^^^^^^^^^^^^^\n",
      "  File \"/Users/tcapelle/miniforge3/envs/weave/lib/python3.11/site-packages/openai/_base_client.py\", line 1816, in post\n",
      "    return await self.request(cast_to, opts, stream=stream, stream_cls=stream_cls)\n",
      "           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/Users/tcapelle/miniforge3/envs/weave/lib/python3.11/site-packages/openai/_base_client.py\", line 1510, in request\n",
      "    return await self._request(\n",
      "           ^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/Users/tcapelle/miniforge3/envs/weave/lib/python3.11/site-packages/openai/_base_client.py\", line 1596, in _request\n",
      "    return await self._retry_request(\n",
      "           ^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/Users/tcapelle/miniforge3/envs/weave/lib/python3.11/site-packages/openai/_base_client.py\", line 1643, in _retry_request\n",
      "    return await self._request(\n",
      "           ^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/Users/tcapelle/miniforge3/envs/weave/lib/python3.11/site-packages/openai/_base_client.py\", line 1596, in _request\n",
      "    return await self._retry_request(\n",
      "           ^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/Users/tcapelle/miniforge3/envs/weave/lib/python3.11/site-packages/openai/_base_client.py\", line 1643, in _retry_request\n",
      "    return await self._request(\n",
      "           ^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/Users/tcapelle/miniforge3/envs/weave/lib/python3.11/site-packages/openai/_base_client.py\", line 1611, in _request\n",
      "    raise self._make_status_error_from_response(err.response) from None\n",
      "openai.RateLimitError: Error code: 429 - {'error': {'message': 'Rate limit exceeded. Please try again later.', 'type': 'rate_limit_error', 'param': None, 'code': 'rate_limit_exceeded'}}\n",
      "\n",
      "During handling of the above exception, another exception occurred:\n",
      "\n",
      "Traceback (most recent call last):\n",
      "  File \"/Users/tcapelle/miniforge3/envs/weave/lib/python3.11/site-packages/litellm/main.py\", line 427, in acompletion\n",
      "    response = await init_response\n",
      "               ^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/Users/tcapelle/miniforge3/envs/weave/lib/python3.11/site-packages/litellm/llms/openai.py\", line 1136, in acompletion\n",
      "    raise OpenAIError(\n",
      "litellm.llms.openai.OpenAIError: Error code: 429 - {'error': {'message': 'Rate limit exceeded. Please try again later.', 'type': 'rate_limit_error', 'param': None, 'code': 'rate_limit_exceeded'}}\n",
      "\n",
      "During handling of the above exception, another exception occurred:\n",
      "\n",
      "Traceback (most recent call last):\n",
      "  File \"/Users/tcapelle/miniforge3/envs/weave/lib/python3.11/site-packages/instructor/retry.py\", line 239, in retry_async\n",
      "    response: ChatCompletion = await func(*args, **kwargs)\n",
      "                               ^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/Users/tcapelle/miniforge3/envs/weave/lib/python3.11/site-packages/weave/trace/op.py\", line 333, in wrapper\n",
      "    res, _ = await _execute_call(wrapper, call, *args, **kwargs)  # type: ignore\n",
      "             ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/Users/tcapelle/miniforge3/envs/weave/lib/python3.11/site-packages/weave/trace/op.py\", line 213, in _call_async\n",
      "    return handle_exception(e)\n",
      "           ^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/Users/tcapelle/miniforge3/envs/weave/lib/python3.11/site-packages/weave/trace/op.py\", line 211, in _call_async\n",
      "    res = await func(*args, **kwargs)\n",
      "          ^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/Users/tcapelle/miniforge3/envs/weave/lib/python3.11/site-packages/litellm/utils.py\", line 1593, in wrapper_async\n",
      "    raise e\n",
      "  File \"/Users/tcapelle/miniforge3/envs/weave/lib/python3.11/site-packages/litellm/utils.py\", line 1413, in wrapper_async\n",
      "    result = await original_function(*args, **kwargs)\n",
      "             ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/Users/tcapelle/miniforge3/envs/weave/lib/python3.11/site-packages/litellm/main.py\", line 449, in acompletion\n",
      "    raise exception_type(\n",
      "          ^^^^^^^^^^^^^^^\n",
      "  File \"/Users/tcapelle/miniforge3/envs/weave/lib/python3.11/site-packages/litellm/utils.py\", line 8187, in exception_type\n",
      "    raise e\n",
      "  File \"/Users/tcapelle/miniforge3/envs/weave/lib/python3.11/site-packages/litellm/utils.py\", line 6555, in exception_type\n",
      "    raise RateLimitError(\n",
      "litellm.exceptions.RateLimitError: litellm.RateLimitError: RateLimitError: MistralException - Error code: 429 - {'error': {'message': 'Rate limit exceeded. Please try again later.', 'type': 'rate_limit_error', 'param': None, 'code': 'rate_limit_exceeded'}}\n",
      "\n",
      "The above exception was the direct cause of the following exception:\n",
      "\n",
      "Traceback (most recent call last):\n",
      "  File \"/Users/tcapelle/miniforge3/envs/weave/lib/python3.11/site-packages/instructor/retry.py\", line 235, in retry_async\n",
      "    async for attempt in max_retries:\n",
      "  File \"/Users/tcapelle/miniforge3/envs/weave/lib/python3.11/site-packages/tenacity/asyncio/__init__.py\", line 166, in __anext__\n",
      "    do = await self.iter(retry_state=self._retry_state)\n",
      "         ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/Users/tcapelle/miniforge3/envs/weave/lib/python3.11/site-packages/tenacity/asyncio/__init__.py\", line 153, in iter\n",
      "    result = await action(retry_state)\n",
      "             ^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/Users/tcapelle/miniforge3/envs/weave/lib/python3.11/site-packages/tenacity/_utils.py\", line 99, in inner\n",
      "    return call(*args, **kwargs)\n",
      "           ^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/Users/tcapelle/miniforge3/envs/weave/lib/python3.11/site-packages/tenacity/__init__.py\", line 419, in exc_check\n",
      "    raise retry_exc from fut.exception()\n",
      "tenacity.RetryError: RetryError[<Future at 0x33d3d7590 state=finished raised RateLimitError>]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "\u001b[1;31mGive Feedback / Get Help: https://github.com/BerriAI/litellm/issues/new\u001b[0m\n",
      "LiteLLM.Info: If you need to debug this error, use `litellm.set_verbose=True'.\n",
      "\n",
      "\n",
      "\u001b[1;31mGive Feedback / Get Help: https://github.com/BerriAI/litellm/issues/new\u001b[0m\n",
      "LiteLLM.Info: If you need to debug this error, use `litellm.set_verbose=True'.\n",
      "\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\">model_output failed\n",
       "</pre>\n"
      ],
      "text/plain": [
       "model_output failed\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Traceback (most recent call last):\n",
      "  File \"/Users/tcapelle/miniforge3/envs/weave/lib/python3.11/site-packages/litellm/llms/openai.py\", line 1089, in acompletion\n",
      "    headers, response = await self.make_openai_chat_completion_request(\n",
      "                        ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/Users/tcapelle/miniforge3/envs/weave/lib/python3.11/site-packages/litellm/llms/openai.py\", line 793, in make_openai_chat_completion_request\n",
      "    raise e\n",
      "  File \"/Users/tcapelle/miniforge3/envs/weave/lib/python3.11/site-packages/litellm/llms/openai.py\", line 781, in make_openai_chat_completion_request\n",
      "    await openai_aclient.chat.completions.with_raw_response.create(\n",
      "  File \"/Users/tcapelle/miniforge3/envs/weave/lib/python3.11/site-packages/openai/_legacy_response.py\", line 367, in wrapped\n",
      "    return cast(LegacyAPIResponse[R], await func(*args, **kwargs))\n",
      "                                      ^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/Users/tcapelle/miniforge3/envs/weave/lib/python3.11/site-packages/weave/trace/op.py\", line 333, in wrapper\n",
      "    res, _ = await _execute_call(wrapper, call, *args, **kwargs)  # type: ignore\n",
      "             ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/Users/tcapelle/miniforge3/envs/weave/lib/python3.11/site-packages/weave/trace/op.py\", line 213, in _call_async\n",
      "    return handle_exception(e)\n",
      "           ^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/Users/tcapelle/miniforge3/envs/weave/lib/python3.11/site-packages/weave/trace/op.py\", line 211, in _call_async\n",
      "    res = await func(*args, **kwargs)\n",
      "          ^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/Users/tcapelle/miniforge3/envs/weave/lib/python3.11/site-packages/weave/integrations/openai/openai_sdk.py\", line 288, in _wrapper\n",
      "    return await fn(*args, **kwargs)\n",
      "           ^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/Users/tcapelle/miniforge3/envs/weave/lib/python3.11/site-packages/openai/resources/chat/completions.py\", line 1339, in create\n",
      "    return await self._post(\n",
      "           ^^^^^^^^^^^^^^^^^\n",
      "  File \"/Users/tcapelle/miniforge3/envs/weave/lib/python3.11/site-packages/openai/_base_client.py\", line 1816, in post\n",
      "    return await self.request(cast_to, opts, stream=stream, stream_cls=stream_cls)\n",
      "           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/Users/tcapelle/miniforge3/envs/weave/lib/python3.11/site-packages/openai/_base_client.py\", line 1510, in request\n",
      "    return await self._request(\n",
      "           ^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/Users/tcapelle/miniforge3/envs/weave/lib/python3.11/site-packages/openai/_base_client.py\", line 1596, in _request\n",
      "    return await self._retry_request(\n",
      "           ^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/Users/tcapelle/miniforge3/envs/weave/lib/python3.11/site-packages/openai/_base_client.py\", line 1643, in _retry_request\n",
      "    return await self._request(\n",
      "           ^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/Users/tcapelle/miniforge3/envs/weave/lib/python3.11/site-packages/openai/_base_client.py\", line 1596, in _request\n",
      "    return await self._retry_request(\n",
      "           ^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/Users/tcapelle/miniforge3/envs/weave/lib/python3.11/site-packages/openai/_base_client.py\", line 1643, in _retry_request\n",
      "    return await self._request(\n",
      "           ^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/Users/tcapelle/miniforge3/envs/weave/lib/python3.11/site-packages/openai/_base_client.py\", line 1611, in _request\n",
      "    raise self._make_status_error_from_response(err.response) from None\n",
      "openai.RateLimitError: Error code: 429 - {'error': {'message': 'Rate limit exceeded. Please try again later.', 'type': 'rate_limit_error', 'param': None, 'code': 'rate_limit_exceeded'}}\n",
      "\n",
      "During handling of the above exception, another exception occurred:\n",
      "\n",
      "Traceback (most recent call last):\n",
      "  File \"/Users/tcapelle/miniforge3/envs/weave/lib/python3.11/site-packages/litellm/main.py\", line 427, in acompletion\n",
      "    response = await init_response\n",
      "               ^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/Users/tcapelle/miniforge3/envs/weave/lib/python3.11/site-packages/litellm/llms/openai.py\", line 1136, in acompletion\n",
      "    raise OpenAIError(\n",
      "litellm.llms.openai.OpenAIError: Error code: 429 - {'error': {'message': 'Rate limit exceeded. Please try again later.', 'type': 'rate_limit_error', 'param': None, 'code': 'rate_limit_exceeded'}}\n",
      "\n",
      "During handling of the above exception, another exception occurred:\n",
      "\n",
      "Traceback (most recent call last):\n",
      "  File \"/Users/tcapelle/miniforge3/envs/weave/lib/python3.11/site-packages/instructor/retry.py\", line 239, in retry_async\n",
      "    response: ChatCompletion = await func(*args, **kwargs)\n",
      "                               ^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/Users/tcapelle/miniforge3/envs/weave/lib/python3.11/site-packages/weave/trace/op.py\", line 333, in wrapper\n",
      "    res, _ = await _execute_call(wrapper, call, *args, **kwargs)  # type: ignore\n",
      "             ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/Users/tcapelle/miniforge3/envs/weave/lib/python3.11/site-packages/weave/trace/op.py\", line 213, in _call_async\n",
      "    return handle_exception(e)\n",
      "           ^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/Users/tcapelle/miniforge3/envs/weave/lib/python3.11/site-packages/weave/trace/op.py\", line 211, in _call_async\n",
      "    res = await func(*args, **kwargs)\n",
      "          ^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/Users/tcapelle/miniforge3/envs/weave/lib/python3.11/site-packages/litellm/utils.py\", line 1593, in wrapper_async\n",
      "    raise e\n",
      "  File \"/Users/tcapelle/miniforge3/envs/weave/lib/python3.11/site-packages/litellm/utils.py\", line 1413, in wrapper_async\n",
      "    result = await original_function(*args, **kwargs)\n",
      "             ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/Users/tcapelle/miniforge3/envs/weave/lib/python3.11/site-packages/litellm/main.py\", line 449, in acompletion\n",
      "    raise exception_type(\n",
      "          ^^^^^^^^^^^^^^^\n",
      "  File \"/Users/tcapelle/miniforge3/envs/weave/lib/python3.11/site-packages/litellm/utils.py\", line 8187, in exception_type\n",
      "    raise e\n",
      "  File \"/Users/tcapelle/miniforge3/envs/weave/lib/python3.11/site-packages/litellm/utils.py\", line 6555, in exception_type\n",
      "    raise RateLimitError(\n",
      "litellm.exceptions.RateLimitError: litellm.RateLimitError: RateLimitError: MistralException - Error code: 429 - {'error': {'message': 'Rate limit exceeded. Please try again later.', 'type': 'rate_limit_error', 'param': None, 'code': 'rate_limit_exceeded'}}\n",
      "\n",
      "The above exception was the direct cause of the following exception:\n",
      "\n",
      "Traceback (most recent call last):\n",
      "  File \"/Users/tcapelle/miniforge3/envs/weave/lib/python3.11/site-packages/instructor/retry.py\", line 235, in retry_async\n",
      "    async for attempt in max_retries:\n",
      "  File \"/Users/tcapelle/miniforge3/envs/weave/lib/python3.11/site-packages/tenacity/asyncio/__init__.py\", line 166, in __anext__\n",
      "    do = await self.iter(retry_state=self._retry_state)\n",
      "         ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/Users/tcapelle/miniforge3/envs/weave/lib/python3.11/site-packages/tenacity/asyncio/__init__.py\", line 153, in iter\n",
      "    result = await action(retry_state)\n",
      "             ^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/Users/tcapelle/miniforge3/envs/weave/lib/python3.11/site-packages/tenacity/_utils.py\", line 99, in inner\n",
      "    return call(*args, **kwargs)\n",
      "           ^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/Users/tcapelle/miniforge3/envs/weave/lib/python3.11/site-packages/tenacity/__init__.py\", line 419, in exc_check\n",
      "    raise retry_exc from fut.exception()\n",
      "tenacity.RetryError: RetryError[<Future at 0x33d3d7590 state=finished raised RateLimitError>]\n",
      "\n",
      "The above exception was the direct cause of the following exception:\n",
      "\n",
      "Traceback (most recent call last):\n",
      "  File \"/Users/tcapelle/miniforge3/envs/weave/lib/python3.11/site-packages/weave/flow/eval.py\", line 168, in predict_and_score\n",
      "    model_output = await async_call(model_predict, **model_predict_args)\n",
      "                   ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/Users/tcapelle/miniforge3/envs/weave/lib/python3.11/site-packages/weave/trace/op.py\", line 333, in wrapper\n",
      "    res, _ = await _execute_call(wrapper, call, *args, **kwargs)  # type: ignore\n",
      "             ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/Users/tcapelle/miniforge3/envs/weave/lib/python3.11/site-packages/weave/trace/op.py\", line 213, in _call_async\n",
      "    return handle_exception(e)\n",
      "           ^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/Users/tcapelle/miniforge3/envs/weave/lib/python3.11/site-packages/weave/trace/op.py\", line 211, in _call_async\n",
      "    res = await func(*args, **kwargs)\n",
      "          ^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/var/folders/sf/tgv7vcv96x557p38bvvp1ms40000gn/T/ipykernel_6554/970167077.py\", line 10, in predict\n",
      "    return await rag_solver_with_reflection(\n",
      "           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/Users/tcapelle/miniforge3/envs/weave/lib/python3.11/site-packages/weave/trace/op.py\", line 333, in wrapper\n",
      "    res, _ = await _execute_call(wrapper, call, *args, **kwargs)  # type: ignore\n",
      "             ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/Users/tcapelle/miniforge3/envs/weave/lib/python3.11/site-packages/weave/trace/op.py\", line 213, in _call_async\n",
      "    return handle_exception(e)\n",
      "           ^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/Users/tcapelle/miniforge3/envs/weave/lib/python3.11/site-packages/weave/trace/op.py\", line 211, in _call_async\n",
      "    res = await func(*args, **kwargs)\n",
      "          ^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/var/folders/sf/tgv7vcv96x557p38bvvp1ms40000gn/T/ipykernel_6554/4070537091.py\", line 14, in rag_solver_with_reflection\n",
      "    rag_result = await rag_solver(\n",
      "                 ^^^^^^^^^^^^^^^^^\n",
      "  File \"/Users/tcapelle/miniforge3/envs/weave/lib/python3.11/site-packages/weave/trace/op.py\", line 333, in wrapper\n",
      "    res, _ = await _execute_call(wrapper, call, *args, **kwargs)  # type: ignore\n",
      "             ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/Users/tcapelle/miniforge3/envs/weave/lib/python3.11/site-packages/weave/trace/op.py\", line 213, in _call_async\n",
      "    return handle_exception(e)\n",
      "           ^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/Users/tcapelle/miniforge3/envs/weave/lib/python3.11/site-packages/weave/trace/op.py\", line 211, in _call_async\n",
      "    res = await func(*args, **kwargs)\n",
      "          ^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/Users/tcapelle/work/hackercup_rag/agent.py\", line 350, in rag_solver\n",
      "    zero_shot_result = await zero_shot_solver(\n",
      "                       ^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/Users/tcapelle/miniforge3/envs/weave/lib/python3.11/site-packages/weave/trace/op.py\", line 333, in wrapper\n",
      "    res, _ = await _execute_call(wrapper, call, *args, **kwargs)  # type: ignore\n",
      "             ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/Users/tcapelle/miniforge3/envs/weave/lib/python3.11/site-packages/weave/trace/op.py\", line 213, in _call_async\n",
      "    return handle_exception(e)\n",
      "           ^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/Users/tcapelle/miniforge3/envs/weave/lib/python3.11/site-packages/weave/trace/op.py\", line 211, in _call_async\n",
      "    res = await func(*args, **kwargs)\n",
      "          ^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/Users/tcapelle/work/hackercup_rag/agent.py\", line 330, in zero_shot_solver\n",
      "    solution = await draft_solution(\n",
      "               ^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/Users/tcapelle/miniforge3/envs/weave/lib/python3.11/site-packages/weave/trace/op.py\", line 333, in wrapper\n",
      "    res, _ = await _execute_call(wrapper, call, *args, **kwargs)  # type: ignore\n",
      "             ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/Users/tcapelle/miniforge3/envs/weave/lib/python3.11/site-packages/weave/trace/op.py\", line 213, in _call_async\n",
      "    return handle_exception(e)\n",
      "           ^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/Users/tcapelle/miniforge3/envs/weave/lib/python3.11/site-packages/weave/trace/op.py\", line 211, in _call_async\n",
      "    res = await func(*args, **kwargs)\n",
      "          ^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/Users/tcapelle/work/hackercup_rag/agent.py\", line 83, in draft_solution\n",
      "    response = await async_client.chat.completions.create(\n",
      "               ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/Users/tcapelle/miniforge3/envs/weave/lib/python3.11/site-packages/instructor/client.py\", line 300, in create\n",
      "    return await self.create_fn(\n",
      "           ^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/Users/tcapelle/miniforge3/envs/weave/lib/python3.11/site-packages/instructor/patch.py\", line 119, in new_create_async\n",
      "    response = await retry_async(\n",
      "               ^^^^^^^^^^^^^^^^^^\n",
      "  File \"/Users/tcapelle/miniforge3/envs/weave/lib/python3.11/site-packages/instructor/retry.py\", line 267, in retry_async\n",
      "    raise InstructorRetryException(\n",
      "instructor.exceptions.InstructorRetryException: RetryError[<Future at 0x33d3d7590 state=finished raised RateLimitError>]\n",
      "2024-09-05 23:05:36,146 : INFO : HTTP Request: POST http://195.242.24.252:8000/v1/chat/completions \"HTTP/1.1 429 Too Many Requests\"\n",
      "2024-09-05 23:05:36,147 : INFO : Retrying request to /chat/completions in 0.976127 seconds\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\">Predict and score failed\n",
       "</pre>\n"
      ],
      "text/plain": [
       "Predict and score failed\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Traceback (most recent call last):\n",
      "  File \"/Users/tcapelle/miniforge3/envs/weave/lib/python3.11/site-packages/weave/flow/eval.py\", line 287, in eval_example\n",
      "    eval_row = await self.predict_and_score(model, example)\n",
      "               ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/Users/tcapelle/miniforge3/envs/weave/lib/python3.11/site-packages/weave/trace/op.py\", line 333, in wrapper\n",
      "    res, _ = await _execute_call(wrapper, call, *args, **kwargs)  # type: ignore\n",
      "             ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/Users/tcapelle/miniforge3/envs/weave/lib/python3.11/site-packages/weave/trace/op.py\", line 213, in _call_async\n",
      "    return handle_exception(e)\n",
      "           ^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/Users/tcapelle/miniforge3/envs/weave/lib/python3.11/site-packages/weave/trace/op.py\", line 211, in _call_async\n",
      "    res = await func(*args, **kwargs)\n",
      "          ^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/Users/tcapelle/miniforge3/envs/weave/lib/python3.11/site-packages/weave/flow/eval.py\", line 224, in predict_and_score\n",
      "    result = await async_call(score_fn, **score_args)\n",
      "             ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/Users/tcapelle/miniforge3/envs/weave/lib/python3.11/asyncio/threads.py\", line 25, in to_thread\n",
      "    return await loop.run_in_executor(None, func_call)\n",
      "           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/Users/tcapelle/miniforge3/envs/weave/lib/python3.11/asyncio/futures.py\", line 287, in __await__\n",
      "    yield self  # This tells Task to wait for completion.\n",
      "    ^^^^^^^^^^\n",
      "  File \"/Users/tcapelle/miniforge3/envs/weave/lib/python3.11/asyncio/tasks.py\", line 349, in __wakeup\n",
      "    future.result()\n",
      "  File \"/Users/tcapelle/miniforge3/envs/weave/lib/python3.11/asyncio/futures.py\", line 203, in result\n",
      "    raise self._exception.with_traceback(self._exception_tb)\n",
      "  File \"/Users/tcapelle/miniforge3/envs/weave/lib/python3.11/concurrent/futures/thread.py\", line 58, in run\n",
      "    result = self.fn(*self.args, **self.kwargs)\n",
      "             ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/Users/tcapelle/miniforge3/envs/weave/lib/python3.11/site-packages/weave/trace/op.py\", line 344, in wrapper\n",
      "    res, _ = _execute_call(wrapper, call, *args, **kwargs)  # type: ignore\n",
      "             ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/Users/tcapelle/miniforge3/envs/weave/lib/python3.11/site-packages/weave/trace/op.py\", line 224, in _execute_call\n",
      "    handle_exception(e)\n",
      "  File \"/Users/tcapelle/miniforge3/envs/weave/lib/python3.11/site-packages/weave/trace/op.py\", line 222, in _execute_call\n",
      "    res = func(*args, **kwargs)\n",
      "          ^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/var/folders/sf/tgv7vcv96x557p38bvvp1ms40000gn/T/ipykernel_6554/249536452.py\", line 9, in scorer\n",
      "    return {\"passed\": expected == model_output[\"test_report\"]}\n",
      "                                  ~~~~~~~~~~~~^^^^^^^^^^^^^^^\n",
      "TypeError: 'NoneType' object is not subscriptable\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\">Evaluated <span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">4</span> of <span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">5</span> examples\n",
       "</pre>\n"
      ],
      "text/plain": [
       "Evaluated \u001b[1;36m4\u001b[0m of \u001b[1;36m5\u001b[0m examples\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2024-09-05 23:05:36,288 : INFO : HTTP Request: POST http://195.242.24.252:8000/v1/chat/completions \"HTTP/1.1 429 Too Many Requests\"\n",
      "2024-09-05 23:05:36,296 : ERROR : Failed after retries: <bound method Future.exception of <Future at 0x33d3e2d90 state=finished raised RateLimitError>>\n",
      "Traceback (most recent call last):\n",
      "  File \"/Users/tcapelle/miniforge3/envs/weave/lib/python3.11/site-packages/litellm/llms/openai.py\", line 1089, in acompletion\n",
      "    headers, response = await self.make_openai_chat_completion_request(\n",
      "                        ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/Users/tcapelle/miniforge3/envs/weave/lib/python3.11/site-packages/litellm/llms/openai.py\", line 793, in make_openai_chat_completion_request\n",
      "    raise e\n",
      "  File \"/Users/tcapelle/miniforge3/envs/weave/lib/python3.11/site-packages/litellm/llms/openai.py\", line 781, in make_openai_chat_completion_request\n",
      "    await openai_aclient.chat.completions.with_raw_response.create(\n",
      "  File \"/Users/tcapelle/miniforge3/envs/weave/lib/python3.11/site-packages/openai/_legacy_response.py\", line 367, in wrapped\n",
      "    return cast(LegacyAPIResponse[R], await func(*args, **kwargs))\n",
      "                                      ^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/Users/tcapelle/miniforge3/envs/weave/lib/python3.11/site-packages/weave/trace/op.py\", line 333, in wrapper\n",
      "    res, _ = await _execute_call(wrapper, call, *args, **kwargs)  # type: ignore\n",
      "             ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/Users/tcapelle/miniforge3/envs/weave/lib/python3.11/site-packages/weave/trace/op.py\", line 213, in _call_async\n",
      "    return handle_exception(e)\n",
      "           ^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/Users/tcapelle/miniforge3/envs/weave/lib/python3.11/site-packages/weave/trace/op.py\", line 211, in _call_async\n",
      "    res = await func(*args, **kwargs)\n",
      "          ^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/Users/tcapelle/miniforge3/envs/weave/lib/python3.11/site-packages/weave/integrations/openai/openai_sdk.py\", line 288, in _wrapper\n",
      "    return await fn(*args, **kwargs)\n",
      "           ^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/Users/tcapelle/miniforge3/envs/weave/lib/python3.11/site-packages/openai/resources/chat/completions.py\", line 1339, in create\n",
      "    return await self._post(\n",
      "           ^^^^^^^^^^^^^^^^^\n",
      "  File \"/Users/tcapelle/miniforge3/envs/weave/lib/python3.11/site-packages/openai/_base_client.py\", line 1816, in post\n",
      "    return await self.request(cast_to, opts, stream=stream, stream_cls=stream_cls)\n",
      "           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/Users/tcapelle/miniforge3/envs/weave/lib/python3.11/site-packages/openai/_base_client.py\", line 1510, in request\n",
      "    return await self._request(\n",
      "           ^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/Users/tcapelle/miniforge3/envs/weave/lib/python3.11/site-packages/openai/_base_client.py\", line 1596, in _request\n",
      "    return await self._retry_request(\n",
      "           ^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/Users/tcapelle/miniforge3/envs/weave/lib/python3.11/site-packages/openai/_base_client.py\", line 1643, in _retry_request\n",
      "    return await self._request(\n",
      "           ^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/Users/tcapelle/miniforge3/envs/weave/lib/python3.11/site-packages/openai/_base_client.py\", line 1596, in _request\n",
      "    return await self._retry_request(\n",
      "           ^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/Users/tcapelle/miniforge3/envs/weave/lib/python3.11/site-packages/openai/_base_client.py\", line 1643, in _retry_request\n",
      "    return await self._request(\n",
      "           ^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/Users/tcapelle/miniforge3/envs/weave/lib/python3.11/site-packages/openai/_base_client.py\", line 1611, in _request\n",
      "    raise self._make_status_error_from_response(err.response) from None\n",
      "openai.RateLimitError: Error code: 429 - {'error': {'message': 'Rate limit exceeded. Please try again later.', 'type': 'rate_limit_error', 'param': None, 'code': 'rate_limit_exceeded'}}\n",
      "\n",
      "During handling of the above exception, another exception occurred:\n",
      "\n",
      "Traceback (most recent call last):\n",
      "  File \"/Users/tcapelle/miniforge3/envs/weave/lib/python3.11/site-packages/litellm/main.py\", line 427, in acompletion\n",
      "    response = await init_response\n",
      "               ^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/Users/tcapelle/miniforge3/envs/weave/lib/python3.11/site-packages/litellm/llms/openai.py\", line 1136, in acompletion\n",
      "    raise OpenAIError(\n",
      "litellm.llms.openai.OpenAIError: Error code: 429 - {'error': {'message': 'Rate limit exceeded. Please try again later.', 'type': 'rate_limit_error', 'param': None, 'code': 'rate_limit_exceeded'}}\n",
      "\n",
      "During handling of the above exception, another exception occurred:\n",
      "\n",
      "Traceback (most recent call last):\n",
      "  File \"/Users/tcapelle/miniforge3/envs/weave/lib/python3.11/site-packages/instructor/retry.py\", line 239, in retry_async\n",
      "    response: ChatCompletion = await func(*args, **kwargs)\n",
      "                               ^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/Users/tcapelle/miniforge3/envs/weave/lib/python3.11/site-packages/weave/trace/op.py\", line 333, in wrapper\n",
      "    res, _ = await _execute_call(wrapper, call, *args, **kwargs)  # type: ignore\n",
      "             ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/Users/tcapelle/miniforge3/envs/weave/lib/python3.11/site-packages/weave/trace/op.py\", line 213, in _call_async\n",
      "    return handle_exception(e)\n",
      "           ^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/Users/tcapelle/miniforge3/envs/weave/lib/python3.11/site-packages/weave/trace/op.py\", line 211, in _call_async\n",
      "    res = await func(*args, **kwargs)\n",
      "          ^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/Users/tcapelle/miniforge3/envs/weave/lib/python3.11/site-packages/litellm/utils.py\", line 1593, in wrapper_async\n",
      "    raise e\n",
      "  File \"/Users/tcapelle/miniforge3/envs/weave/lib/python3.11/site-packages/litellm/utils.py\", line 1413, in wrapper_async\n",
      "    result = await original_function(*args, **kwargs)\n",
      "             ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/Users/tcapelle/miniforge3/envs/weave/lib/python3.11/site-packages/litellm/main.py\", line 449, in acompletion\n",
      "    raise exception_type(\n",
      "          ^^^^^^^^^^^^^^^\n",
      "  File \"/Users/tcapelle/miniforge3/envs/weave/lib/python3.11/site-packages/litellm/utils.py\", line 8187, in exception_type\n",
      "    raise e\n",
      "  File \"/Users/tcapelle/miniforge3/envs/weave/lib/python3.11/site-packages/litellm/utils.py\", line 6555, in exception_type\n",
      "    raise RateLimitError(\n",
      "litellm.exceptions.RateLimitError: litellm.RateLimitError: RateLimitError: MistralException - Error code: 429 - {'error': {'message': 'Rate limit exceeded. Please try again later.', 'type': 'rate_limit_error', 'param': None, 'code': 'rate_limit_exceeded'}}\n",
      "\n",
      "The above exception was the direct cause of the following exception:\n",
      "\n",
      "Traceback (most recent call last):\n",
      "  File \"/Users/tcapelle/miniforge3/envs/weave/lib/python3.11/site-packages/instructor/retry.py\", line 235, in retry_async\n",
      "    async for attempt in max_retries:\n",
      "  File \"/Users/tcapelle/miniforge3/envs/weave/lib/python3.11/site-packages/tenacity/asyncio/__init__.py\", line 166, in __anext__\n",
      "    do = await self.iter(retry_state=self._retry_state)\n",
      "         ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/Users/tcapelle/miniforge3/envs/weave/lib/python3.11/site-packages/tenacity/asyncio/__init__.py\", line 153, in iter\n",
      "    result = await action(retry_state)\n",
      "             ^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/Users/tcapelle/miniforge3/envs/weave/lib/python3.11/site-packages/tenacity/_utils.py\", line 99, in inner\n",
      "    return call(*args, **kwargs)\n",
      "           ^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/Users/tcapelle/miniforge3/envs/weave/lib/python3.11/site-packages/tenacity/__init__.py\", line 419, in exc_check\n",
      "    raise retry_exc from fut.exception()\n",
      "tenacity.RetryError: RetryError[<Future at 0x33d3e2d90 state=finished raised RateLimitError>]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "\u001b[1;31mGive Feedback / Get Help: https://github.com/BerriAI/litellm/issues/new\u001b[0m\n",
      "LiteLLM.Info: If you need to debug this error, use `litellm.set_verbose=True'.\n",
      "\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\">model_output failed\n",
       "</pre>\n"
      ],
      "text/plain": [
       "model_output failed\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Traceback (most recent call last):\n",
      "  File \"/Users/tcapelle/miniforge3/envs/weave/lib/python3.11/site-packages/litellm/llms/openai.py\", line 1089, in acompletion\n",
      "    headers, response = await self.make_openai_chat_completion_request(\n",
      "                        ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/Users/tcapelle/miniforge3/envs/weave/lib/python3.11/site-packages/litellm/llms/openai.py\", line 793, in make_openai_chat_completion_request\n",
      "    raise e\n",
      "  File \"/Users/tcapelle/miniforge3/envs/weave/lib/python3.11/site-packages/litellm/llms/openai.py\", line 781, in make_openai_chat_completion_request\n",
      "    await openai_aclient.chat.completions.with_raw_response.create(\n",
      "  File \"/Users/tcapelle/miniforge3/envs/weave/lib/python3.11/site-packages/openai/_legacy_response.py\", line 367, in wrapped\n",
      "    return cast(LegacyAPIResponse[R], await func(*args, **kwargs))\n",
      "                                      ^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/Users/tcapelle/miniforge3/envs/weave/lib/python3.11/site-packages/weave/trace/op.py\", line 333, in wrapper\n",
      "    res, _ = await _execute_call(wrapper, call, *args, **kwargs)  # type: ignore\n",
      "             ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/Users/tcapelle/miniforge3/envs/weave/lib/python3.11/site-packages/weave/trace/op.py\", line 213, in _call_async\n",
      "    return handle_exception(e)\n",
      "           ^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/Users/tcapelle/miniforge3/envs/weave/lib/python3.11/site-packages/weave/trace/op.py\", line 211, in _call_async\n",
      "    res = await func(*args, **kwargs)\n",
      "          ^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/Users/tcapelle/miniforge3/envs/weave/lib/python3.11/site-packages/weave/integrations/openai/openai_sdk.py\", line 288, in _wrapper\n",
      "    return await fn(*args, **kwargs)\n",
      "           ^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/Users/tcapelle/miniforge3/envs/weave/lib/python3.11/site-packages/openai/resources/chat/completions.py\", line 1339, in create\n",
      "    return await self._post(\n",
      "           ^^^^^^^^^^^^^^^^^\n",
      "  File \"/Users/tcapelle/miniforge3/envs/weave/lib/python3.11/site-packages/openai/_base_client.py\", line 1816, in post\n",
      "    return await self.request(cast_to, opts, stream=stream, stream_cls=stream_cls)\n",
      "           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/Users/tcapelle/miniforge3/envs/weave/lib/python3.11/site-packages/openai/_base_client.py\", line 1510, in request\n",
      "    return await self._request(\n",
      "           ^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/Users/tcapelle/miniforge3/envs/weave/lib/python3.11/site-packages/openai/_base_client.py\", line 1596, in _request\n",
      "    return await self._retry_request(\n",
      "           ^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/Users/tcapelle/miniforge3/envs/weave/lib/python3.11/site-packages/openai/_base_client.py\", line 1643, in _retry_request\n",
      "    return await self._request(\n",
      "           ^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/Users/tcapelle/miniforge3/envs/weave/lib/python3.11/site-packages/openai/_base_client.py\", line 1596, in _request\n",
      "    return await self._retry_request(\n",
      "           ^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/Users/tcapelle/miniforge3/envs/weave/lib/python3.11/site-packages/openai/_base_client.py\", line 1643, in _retry_request\n",
      "    return await self._request(\n",
      "           ^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/Users/tcapelle/miniforge3/envs/weave/lib/python3.11/site-packages/openai/_base_client.py\", line 1611, in _request\n",
      "    raise self._make_status_error_from_response(err.response) from None\n",
      "openai.RateLimitError: Error code: 429 - {'error': {'message': 'Rate limit exceeded. Please try again later.', 'type': 'rate_limit_error', 'param': None, 'code': 'rate_limit_exceeded'}}\n",
      "\n",
      "During handling of the above exception, another exception occurred:\n",
      "\n",
      "Traceback (most recent call last):\n",
      "  File \"/Users/tcapelle/miniforge3/envs/weave/lib/python3.11/site-packages/litellm/main.py\", line 427, in acompletion\n",
      "    response = await init_response\n",
      "               ^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/Users/tcapelle/miniforge3/envs/weave/lib/python3.11/site-packages/litellm/llms/openai.py\", line 1136, in acompletion\n",
      "    raise OpenAIError(\n",
      "litellm.llms.openai.OpenAIError: Error code: 429 - {'error': {'message': 'Rate limit exceeded. Please try again later.', 'type': 'rate_limit_error', 'param': None, 'code': 'rate_limit_exceeded'}}\n",
      "\n",
      "During handling of the above exception, another exception occurred:\n",
      "\n",
      "Traceback (most recent call last):\n",
      "  File \"/Users/tcapelle/miniforge3/envs/weave/lib/python3.11/site-packages/instructor/retry.py\", line 239, in retry_async\n",
      "    response: ChatCompletion = await func(*args, **kwargs)\n",
      "                               ^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/Users/tcapelle/miniforge3/envs/weave/lib/python3.11/site-packages/weave/trace/op.py\", line 333, in wrapper\n",
      "    res, _ = await _execute_call(wrapper, call, *args, **kwargs)  # type: ignore\n",
      "             ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/Users/tcapelle/miniforge3/envs/weave/lib/python3.11/site-packages/weave/trace/op.py\", line 213, in _call_async\n",
      "    return handle_exception(e)\n",
      "           ^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/Users/tcapelle/miniforge3/envs/weave/lib/python3.11/site-packages/weave/trace/op.py\", line 211, in _call_async\n",
      "    res = await func(*args, **kwargs)\n",
      "          ^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/Users/tcapelle/miniforge3/envs/weave/lib/python3.11/site-packages/litellm/utils.py\", line 1593, in wrapper_async\n",
      "    raise e\n",
      "  File \"/Users/tcapelle/miniforge3/envs/weave/lib/python3.11/site-packages/litellm/utils.py\", line 1413, in wrapper_async\n",
      "    result = await original_function(*args, **kwargs)\n",
      "             ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/Users/tcapelle/miniforge3/envs/weave/lib/python3.11/site-packages/litellm/main.py\", line 449, in acompletion\n",
      "    raise exception_type(\n",
      "          ^^^^^^^^^^^^^^^\n",
      "  File \"/Users/tcapelle/miniforge3/envs/weave/lib/python3.11/site-packages/litellm/utils.py\", line 8187, in exception_type\n",
      "    raise e\n",
      "  File \"/Users/tcapelle/miniforge3/envs/weave/lib/python3.11/site-packages/litellm/utils.py\", line 6555, in exception_type\n",
      "    raise RateLimitError(\n",
      "litellm.exceptions.RateLimitError: litellm.RateLimitError: RateLimitError: MistralException - Error code: 429 - {'error': {'message': 'Rate limit exceeded. Please try again later.', 'type': 'rate_limit_error', 'param': None, 'code': 'rate_limit_exceeded'}}\n",
      "\n",
      "The above exception was the direct cause of the following exception:\n",
      "\n",
      "Traceback (most recent call last):\n",
      "  File \"/Users/tcapelle/miniforge3/envs/weave/lib/python3.11/site-packages/instructor/retry.py\", line 235, in retry_async\n",
      "    async for attempt in max_retries:\n",
      "  File \"/Users/tcapelle/miniforge3/envs/weave/lib/python3.11/site-packages/tenacity/asyncio/__init__.py\", line 166, in __anext__\n",
      "    do = await self.iter(retry_state=self._retry_state)\n",
      "         ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/Users/tcapelle/miniforge3/envs/weave/lib/python3.11/site-packages/tenacity/asyncio/__init__.py\", line 153, in iter\n",
      "    result = await action(retry_state)\n",
      "             ^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/Users/tcapelle/miniforge3/envs/weave/lib/python3.11/site-packages/tenacity/_utils.py\", line 99, in inner\n",
      "    return call(*args, **kwargs)\n",
      "           ^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/Users/tcapelle/miniforge3/envs/weave/lib/python3.11/site-packages/tenacity/__init__.py\", line 419, in exc_check\n",
      "    raise retry_exc from fut.exception()\n",
      "tenacity.RetryError: RetryError[<Future at 0x33d3e2d90 state=finished raised RateLimitError>]\n",
      "\n",
      "The above exception was the direct cause of the following exception:\n",
      "\n",
      "Traceback (most recent call last):\n",
      "  File \"/Users/tcapelle/miniforge3/envs/weave/lib/python3.11/site-packages/weave/flow/eval.py\", line 168, in predict_and_score\n",
      "    model_output = await async_call(model_predict, **model_predict_args)\n",
      "                   ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/Users/tcapelle/miniforge3/envs/weave/lib/python3.11/site-packages/weave/trace/op.py\", line 333, in wrapper\n",
      "    res, _ = await _execute_call(wrapper, call, *args, **kwargs)  # type: ignore\n",
      "             ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/Users/tcapelle/miniforge3/envs/weave/lib/python3.11/site-packages/weave/trace/op.py\", line 213, in _call_async\n",
      "    return handle_exception(e)\n",
      "           ^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/Users/tcapelle/miniforge3/envs/weave/lib/python3.11/site-packages/weave/trace/op.py\", line 211, in _call_async\n",
      "    res = await func(*args, **kwargs)\n",
      "          ^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/var/folders/sf/tgv7vcv96x557p38bvvp1ms40000gn/T/ipykernel_6554/970167077.py\", line 10, in predict\n",
      "    return await rag_solver_with_reflection(\n",
      "           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/Users/tcapelle/miniforge3/envs/weave/lib/python3.11/site-packages/weave/trace/op.py\", line 333, in wrapper\n",
      "    res, _ = await _execute_call(wrapper, call, *args, **kwargs)  # type: ignore\n",
      "             ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/Users/tcapelle/miniforge3/envs/weave/lib/python3.11/site-packages/weave/trace/op.py\", line 213, in _call_async\n",
      "    return handle_exception(e)\n",
      "           ^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/Users/tcapelle/miniforge3/envs/weave/lib/python3.11/site-packages/weave/trace/op.py\", line 211, in _call_async\n",
      "    res = await func(*args, **kwargs)\n",
      "          ^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/var/folders/sf/tgv7vcv96x557p38bvvp1ms40000gn/T/ipykernel_6554/4070537091.py\", line 14, in rag_solver_with_reflection\n",
      "    rag_result = await rag_solver(\n",
      "                 ^^^^^^^^^^^^^^^^^\n",
      "  File \"/Users/tcapelle/miniforge3/envs/weave/lib/python3.11/site-packages/weave/trace/op.py\", line 333, in wrapper\n",
      "    res, _ = await _execute_call(wrapper, call, *args, **kwargs)  # type: ignore\n",
      "             ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/Users/tcapelle/miniforge3/envs/weave/lib/python3.11/site-packages/weave/trace/op.py\", line 213, in _call_async\n",
      "    return handle_exception(e)\n",
      "           ^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/Users/tcapelle/miniforge3/envs/weave/lib/python3.11/site-packages/weave/trace/op.py\", line 211, in _call_async\n",
      "    res = await func(*args, **kwargs)\n",
      "          ^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/Users/tcapelle/work/hackercup_rag/agent.py\", line 350, in rag_solver\n",
      "    zero_shot_result = await zero_shot_solver(\n",
      "                       ^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/Users/tcapelle/miniforge3/envs/weave/lib/python3.11/site-packages/weave/trace/op.py\", line 333, in wrapper\n",
      "    res, _ = await _execute_call(wrapper, call, *args, **kwargs)  # type: ignore\n",
      "             ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/Users/tcapelle/miniforge3/envs/weave/lib/python3.11/site-packages/weave/trace/op.py\", line 213, in _call_async\n",
      "    return handle_exception(e)\n",
      "           ^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/Users/tcapelle/miniforge3/envs/weave/lib/python3.11/site-packages/weave/trace/op.py\", line 211, in _call_async\n",
      "    res = await func(*args, **kwargs)\n",
      "          ^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/Users/tcapelle/work/hackercup_rag/agent.py\", line 330, in zero_shot_solver\n",
      "    solution = await draft_solution(\n",
      "               ^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/Users/tcapelle/miniforge3/envs/weave/lib/python3.11/site-packages/weave/trace/op.py\", line 333, in wrapper\n",
      "    res, _ = await _execute_call(wrapper, call, *args, **kwargs)  # type: ignore\n",
      "             ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/Users/tcapelle/miniforge3/envs/weave/lib/python3.11/site-packages/weave/trace/op.py\", line 213, in _call_async\n",
      "    return handle_exception(e)\n",
      "           ^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/Users/tcapelle/miniforge3/envs/weave/lib/python3.11/site-packages/weave/trace/op.py\", line 211, in _call_async\n",
      "    res = await func(*args, **kwargs)\n",
      "          ^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/Users/tcapelle/work/hackercup_rag/agent.py\", line 83, in draft_solution\n",
      "    response = await async_client.chat.completions.create(\n",
      "               ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/Users/tcapelle/miniforge3/envs/weave/lib/python3.11/site-packages/instructor/client.py\", line 300, in create\n",
      "    return await self.create_fn(\n",
      "           ^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/Users/tcapelle/miniforge3/envs/weave/lib/python3.11/site-packages/instructor/patch.py\", line 119, in new_create_async\n",
      "    response = await retry_async(\n",
      "               ^^^^^^^^^^^^^^^^^^\n",
      "  File \"/Users/tcapelle/miniforge3/envs/weave/lib/python3.11/site-packages/instructor/retry.py\", line 267, in retry_async\n",
      "    raise InstructorRetryException(\n",
      "instructor.exceptions.InstructorRetryException: RetryError[<Future at 0x33d3e2d90 state=finished raised RateLimitError>]\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\">Predict and score failed\n",
       "</pre>\n"
      ],
      "text/plain": [
       "Predict and score failed\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Traceback (most recent call last):\n",
      "  File \"/Users/tcapelle/miniforge3/envs/weave/lib/python3.11/site-packages/weave/flow/eval.py\", line 287, in eval_example\n",
      "    eval_row = await self.predict_and_score(model, example)\n",
      "               ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/Users/tcapelle/miniforge3/envs/weave/lib/python3.11/site-packages/weave/trace/op.py\", line 333, in wrapper\n",
      "    res, _ = await _execute_call(wrapper, call, *args, **kwargs)  # type: ignore\n",
      "             ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/Users/tcapelle/miniforge3/envs/weave/lib/python3.11/site-packages/weave/trace/op.py\", line 213, in _call_async\n",
      "    return handle_exception(e)\n",
      "           ^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/Users/tcapelle/miniforge3/envs/weave/lib/python3.11/site-packages/weave/trace/op.py\", line 211, in _call_async\n",
      "    res = await func(*args, **kwargs)\n",
      "          ^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/Users/tcapelle/miniforge3/envs/weave/lib/python3.11/site-packages/weave/flow/eval.py\", line 224, in predict_and_score\n",
      "    result = await async_call(score_fn, **score_args)\n",
      "             ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/Users/tcapelle/miniforge3/envs/weave/lib/python3.11/asyncio/threads.py\", line 25, in to_thread\n",
      "    return await loop.run_in_executor(None, func_call)\n",
      "           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/Users/tcapelle/miniforge3/envs/weave/lib/python3.11/asyncio/futures.py\", line 287, in __await__\n",
      "    yield self  # This tells Task to wait for completion.\n",
      "    ^^^^^^^^^^\n",
      "  File \"/Users/tcapelle/miniforge3/envs/weave/lib/python3.11/asyncio/tasks.py\", line 349, in __wakeup\n",
      "    future.result()\n",
      "  File \"/Users/tcapelle/miniforge3/envs/weave/lib/python3.11/asyncio/futures.py\", line 203, in result\n",
      "    raise self._exception.with_traceback(self._exception_tb)\n",
      "  File \"/Users/tcapelle/miniforge3/envs/weave/lib/python3.11/concurrent/futures/thread.py\", line 58, in run\n",
      "    result = self.fn(*self.args, **self.kwargs)\n",
      "             ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/Users/tcapelle/miniforge3/envs/weave/lib/python3.11/site-packages/weave/trace/op.py\", line 344, in wrapper\n",
      "    res, _ = _execute_call(wrapper, call, *args, **kwargs)  # type: ignore\n",
      "             ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/Users/tcapelle/miniforge3/envs/weave/lib/python3.11/site-packages/weave/trace/op.py\", line 224, in _execute_call\n",
      "    handle_exception(e)\n",
      "  File \"/Users/tcapelle/miniforge3/envs/weave/lib/python3.11/site-packages/weave/trace/op.py\", line 222, in _execute_call\n",
      "    res = func(*args, **kwargs)\n",
      "          ^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/var/folders/sf/tgv7vcv96x557p38bvvp1ms40000gn/T/ipykernel_6554/249536452.py\", line 9, in scorer\n",
      "    return {\"passed\": expected == model_output[\"test_report\"]}\n",
      "                                  ~~~~~~~~~~~~^^^^^^^^^^^^^^^\n",
      "TypeError: 'NoneType' object is not subscriptable\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\">Evaluated <span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">5</span> of <span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">5</span> examples\n",
       "</pre>\n"
      ],
      "text/plain": [
       "Evaluated \u001b[1;36m5\u001b[0m of \u001b[1;36m5\u001b[0m examples\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\">Evaluation summary\n",
       "<span style=\"font-weight: bold\">{</span><span style=\"color: #008000; text-decoration-color: #008000\">'scorer'</span>: <span style=\"font-weight: bold\">{</span><span style=\"color: #008000; text-decoration-color: #008000\">'passed'</span>: <span style=\"font-weight: bold\">{</span><span style=\"color: #008000; text-decoration-color: #008000\">'true_count'</span>: <span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">1</span>, <span style=\"color: #008000; text-decoration-color: #008000\">'true_fraction'</span>: <span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">1.0</span><span style=\"font-weight: bold\">}}</span>, <span style=\"color: #008000; text-decoration-color: #008000\">'model_latency'</span>: <span style=\"font-weight: bold\">{</span><span style=\"color: #008000; text-decoration-color: #008000\">'mean'</span>: <span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">29.06909704208374</span><span style=\"font-weight: bold\">}}</span>\n",
       "</pre>\n"
      ],
      "text/plain": [
       "Evaluation summary\n",
       "\u001b[1m{\u001b[0m\u001b[32m'scorer'\u001b[0m: \u001b[1m{\u001b[0m\u001b[32m'passed'\u001b[0m: \u001b[1m{\u001b[0m\u001b[32m'true_count'\u001b[0m: \u001b[1;36m1\u001b[0m, \u001b[32m'true_fraction'\u001b[0m: \u001b[1;36m1.0\u001b[0m\u001b[1m}\u001b[0m\u001b[1m}\u001b[0m, \u001b[32m'model_latency'\u001b[0m: \u001b[1m{\u001b[0m\u001b[32m'mean'\u001b[0m: \u001b[1;36m29.06909704208374\u001b[0m\u001b[1m}\u001b[0m\u001b[1m}\u001b[0m\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2024-09-05 23:05:37,482 : INFO : HTTP Request: POST http://195.242.24.252:8000/v1/chat/completions \"HTTP/1.1 429 Too Many Requests\"\n",
      "2024-09-05 23:05:37,489 : ERROR : Failed after retries: <bound method Future.exception of <Future at 0x33d3ec6d0 state=finished raised RateLimitError>>\n",
      "Traceback (most recent call last):\n",
      "  File \"/Users/tcapelle/miniforge3/envs/weave/lib/python3.11/site-packages/litellm/llms/openai.py\", line 1089, in acompletion\n",
      "    headers, response = await self.make_openai_chat_completion_request(\n",
      "                        ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/Users/tcapelle/miniforge3/envs/weave/lib/python3.11/site-packages/litellm/llms/openai.py\", line 793, in make_openai_chat_completion_request\n",
      "    raise e\n",
      "  File \"/Users/tcapelle/miniforge3/envs/weave/lib/python3.11/site-packages/litellm/llms/openai.py\", line 781, in make_openai_chat_completion_request\n",
      "    await openai_aclient.chat.completions.with_raw_response.create(\n",
      "  File \"/Users/tcapelle/miniforge3/envs/weave/lib/python3.11/site-packages/openai/_legacy_response.py\", line 367, in wrapped\n",
      "    return cast(LegacyAPIResponse[R], await func(*args, **kwargs))\n",
      "                                      ^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/Users/tcapelle/miniforge3/envs/weave/lib/python3.11/site-packages/weave/trace/op.py\", line 333, in wrapper\n",
      "    res, _ = await _execute_call(wrapper, call, *args, **kwargs)  # type: ignore\n",
      "             ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/Users/tcapelle/miniforge3/envs/weave/lib/python3.11/site-packages/weave/trace/op.py\", line 213, in _call_async\n",
      "    return handle_exception(e)\n",
      "           ^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/Users/tcapelle/miniforge3/envs/weave/lib/python3.11/site-packages/weave/trace/op.py\", line 211, in _call_async\n",
      "    res = await func(*args, **kwargs)\n",
      "          ^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/Users/tcapelle/miniforge3/envs/weave/lib/python3.11/site-packages/weave/integrations/openai/openai_sdk.py\", line 288, in _wrapper\n",
      "    return await fn(*args, **kwargs)\n",
      "           ^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/Users/tcapelle/miniforge3/envs/weave/lib/python3.11/site-packages/openai/resources/chat/completions.py\", line 1339, in create\n",
      "    return await self._post(\n",
      "           ^^^^^^^^^^^^^^^^^\n",
      "  File \"/Users/tcapelle/miniforge3/envs/weave/lib/python3.11/site-packages/openai/_base_client.py\", line 1816, in post\n",
      "    return await self.request(cast_to, opts, stream=stream, stream_cls=stream_cls)\n",
      "           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/Users/tcapelle/miniforge3/envs/weave/lib/python3.11/site-packages/openai/_base_client.py\", line 1510, in request\n",
      "    return await self._request(\n",
      "           ^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/Users/tcapelle/miniforge3/envs/weave/lib/python3.11/site-packages/openai/_base_client.py\", line 1596, in _request\n",
      "    return await self._retry_request(\n",
      "           ^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/Users/tcapelle/miniforge3/envs/weave/lib/python3.11/site-packages/openai/_base_client.py\", line 1643, in _retry_request\n",
      "    return await self._request(\n",
      "           ^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/Users/tcapelle/miniforge3/envs/weave/lib/python3.11/site-packages/openai/_base_client.py\", line 1596, in _request\n",
      "    return await self._retry_request(\n",
      "           ^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/Users/tcapelle/miniforge3/envs/weave/lib/python3.11/site-packages/openai/_base_client.py\", line 1643, in _retry_request\n",
      "    return await self._request(\n",
      "           ^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/Users/tcapelle/miniforge3/envs/weave/lib/python3.11/site-packages/openai/_base_client.py\", line 1611, in _request\n",
      "    raise self._make_status_error_from_response(err.response) from None\n",
      "openai.RateLimitError: Error code: 429 - {'error': {'message': 'Rate limit exceeded. Please try again later.', 'type': 'rate_limit_error', 'param': None, 'code': 'rate_limit_exceeded'}}\n",
      "\n",
      "During handling of the above exception, another exception occurred:\n",
      "\n",
      "Traceback (most recent call last):\n",
      "  File \"/Users/tcapelle/miniforge3/envs/weave/lib/python3.11/site-packages/litellm/main.py\", line 427, in acompletion\n",
      "    response = await init_response\n",
      "               ^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/Users/tcapelle/miniforge3/envs/weave/lib/python3.11/site-packages/litellm/llms/openai.py\", line 1136, in acompletion\n",
      "    raise OpenAIError(\n",
      "litellm.llms.openai.OpenAIError: Error code: 429 - {'error': {'message': 'Rate limit exceeded. Please try again later.', 'type': 'rate_limit_error', 'param': None, 'code': 'rate_limit_exceeded'}}\n",
      "\n",
      "During handling of the above exception, another exception occurred:\n",
      "\n",
      "Traceback (most recent call last):\n",
      "  File \"/Users/tcapelle/miniforge3/envs/weave/lib/python3.11/site-packages/instructor/retry.py\", line 239, in retry_async\n",
      "    response: ChatCompletion = await func(*args, **kwargs)\n",
      "                               ^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/Users/tcapelle/miniforge3/envs/weave/lib/python3.11/site-packages/weave/trace/op.py\", line 333, in wrapper\n",
      "    res, _ = await _execute_call(wrapper, call, *args, **kwargs)  # type: ignore\n",
      "             ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/Users/tcapelle/miniforge3/envs/weave/lib/python3.11/site-packages/weave/trace/op.py\", line 213, in _call_async\n",
      "    return handle_exception(e)\n",
      "           ^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/Users/tcapelle/miniforge3/envs/weave/lib/python3.11/site-packages/weave/trace/op.py\", line 211, in _call_async\n",
      "    res = await func(*args, **kwargs)\n",
      "          ^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/Users/tcapelle/miniforge3/envs/weave/lib/python3.11/site-packages/litellm/utils.py\", line 1593, in wrapper_async\n",
      "    raise e\n",
      "  File \"/Users/tcapelle/miniforge3/envs/weave/lib/python3.11/site-packages/litellm/utils.py\", line 1413, in wrapper_async\n",
      "    result = await original_function(*args, **kwargs)\n",
      "             ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/Users/tcapelle/miniforge3/envs/weave/lib/python3.11/site-packages/litellm/main.py\", line 449, in acompletion\n",
      "    raise exception_type(\n",
      "          ^^^^^^^^^^^^^^^\n",
      "  File \"/Users/tcapelle/miniforge3/envs/weave/lib/python3.11/site-packages/litellm/utils.py\", line 8187, in exception_type\n",
      "    raise e\n",
      "  File \"/Users/tcapelle/miniforge3/envs/weave/lib/python3.11/site-packages/litellm/utils.py\", line 6555, in exception_type\n",
      "    raise RateLimitError(\n",
      "litellm.exceptions.RateLimitError: litellm.RateLimitError: RateLimitError: MistralException - Error code: 429 - {'error': {'message': 'Rate limit exceeded. Please try again later.', 'type': 'rate_limit_error', 'param': None, 'code': 'rate_limit_exceeded'}}\n",
      "\n",
      "The above exception was the direct cause of the following exception:\n",
      "\n",
      "Traceback (most recent call last):\n",
      "  File \"/Users/tcapelle/miniforge3/envs/weave/lib/python3.11/site-packages/instructor/retry.py\", line 235, in retry_async\n",
      "    async for attempt in max_retries:\n",
      "  File \"/Users/tcapelle/miniforge3/envs/weave/lib/python3.11/site-packages/tenacity/asyncio/__init__.py\", line 166, in __anext__\n",
      "    do = await self.iter(retry_state=self._retry_state)\n",
      "         ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/Users/tcapelle/miniforge3/envs/weave/lib/python3.11/site-packages/tenacity/asyncio/__init__.py\", line 153, in iter\n",
      "    result = await action(retry_state)\n",
      "             ^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/Users/tcapelle/miniforge3/envs/weave/lib/python3.11/site-packages/tenacity/_utils.py\", line 99, in inner\n",
      "    return call(*args, **kwargs)\n",
      "           ^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/Users/tcapelle/miniforge3/envs/weave/lib/python3.11/site-packages/tenacity/__init__.py\", line 419, in exc_check\n",
      "    raise retry_exc from fut.exception()\n",
      "tenacity.RetryError: RetryError[<Future at 0x33d3ec6d0 state=finished raised RateLimitError>]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "\u001b[1;31mGive Feedback / Get Help: https://github.com/BerriAI/litellm/issues/new\u001b[0m\n",
      "LiteLLM.Info: If you need to debug this error, use `litellm.set_verbose=True'.\n",
      "\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\">model_output failed\n",
       "</pre>\n"
      ],
      "text/plain": [
       "model_output failed\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Traceback (most recent call last):\n",
      "  File \"/Users/tcapelle/miniforge3/envs/weave/lib/python3.11/site-packages/litellm/llms/openai.py\", line 1089, in acompletion\n",
      "    headers, response = await self.make_openai_chat_completion_request(\n",
      "                        ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/Users/tcapelle/miniforge3/envs/weave/lib/python3.11/site-packages/litellm/llms/openai.py\", line 793, in make_openai_chat_completion_request\n",
      "    raise e\n",
      "  File \"/Users/tcapelle/miniforge3/envs/weave/lib/python3.11/site-packages/litellm/llms/openai.py\", line 781, in make_openai_chat_completion_request\n",
      "    await openai_aclient.chat.completions.with_raw_response.create(\n",
      "  File \"/Users/tcapelle/miniforge3/envs/weave/lib/python3.11/site-packages/openai/_legacy_response.py\", line 367, in wrapped\n",
      "    return cast(LegacyAPIResponse[R], await func(*args, **kwargs))\n",
      "                                      ^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/Users/tcapelle/miniforge3/envs/weave/lib/python3.11/site-packages/weave/trace/op.py\", line 333, in wrapper\n",
      "    res, _ = await _execute_call(wrapper, call, *args, **kwargs)  # type: ignore\n",
      "             ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/Users/tcapelle/miniforge3/envs/weave/lib/python3.11/site-packages/weave/trace/op.py\", line 213, in _call_async\n",
      "    return handle_exception(e)\n",
      "           ^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/Users/tcapelle/miniforge3/envs/weave/lib/python3.11/site-packages/weave/trace/op.py\", line 211, in _call_async\n",
      "    res = await func(*args, **kwargs)\n",
      "          ^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/Users/tcapelle/miniforge3/envs/weave/lib/python3.11/site-packages/weave/integrations/openai/openai_sdk.py\", line 288, in _wrapper\n",
      "    return await fn(*args, **kwargs)\n",
      "           ^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/Users/tcapelle/miniforge3/envs/weave/lib/python3.11/site-packages/openai/resources/chat/completions.py\", line 1339, in create\n",
      "    return await self._post(\n",
      "           ^^^^^^^^^^^^^^^^^\n",
      "  File \"/Users/tcapelle/miniforge3/envs/weave/lib/python3.11/site-packages/openai/_base_client.py\", line 1816, in post\n",
      "    return await self.request(cast_to, opts, stream=stream, stream_cls=stream_cls)\n",
      "           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/Users/tcapelle/miniforge3/envs/weave/lib/python3.11/site-packages/openai/_base_client.py\", line 1510, in request\n",
      "    return await self._request(\n",
      "           ^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/Users/tcapelle/miniforge3/envs/weave/lib/python3.11/site-packages/openai/_base_client.py\", line 1596, in _request\n",
      "    return await self._retry_request(\n",
      "           ^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/Users/tcapelle/miniforge3/envs/weave/lib/python3.11/site-packages/openai/_base_client.py\", line 1643, in _retry_request\n",
      "    return await self._request(\n",
      "           ^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/Users/tcapelle/miniforge3/envs/weave/lib/python3.11/site-packages/openai/_base_client.py\", line 1596, in _request\n",
      "    return await self._retry_request(\n",
      "           ^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/Users/tcapelle/miniforge3/envs/weave/lib/python3.11/site-packages/openai/_base_client.py\", line 1643, in _retry_request\n",
      "    return await self._request(\n",
      "           ^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/Users/tcapelle/miniforge3/envs/weave/lib/python3.11/site-packages/openai/_base_client.py\", line 1611, in _request\n",
      "    raise self._make_status_error_from_response(err.response) from None\n",
      "openai.RateLimitError: Error code: 429 - {'error': {'message': 'Rate limit exceeded. Please try again later.', 'type': 'rate_limit_error', 'param': None, 'code': 'rate_limit_exceeded'}}\n",
      "\n",
      "During handling of the above exception, another exception occurred:\n",
      "\n",
      "Traceback (most recent call last):\n",
      "  File \"/Users/tcapelle/miniforge3/envs/weave/lib/python3.11/site-packages/litellm/main.py\", line 427, in acompletion\n",
      "    response = await init_response\n",
      "               ^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/Users/tcapelle/miniforge3/envs/weave/lib/python3.11/site-packages/litellm/llms/openai.py\", line 1136, in acompletion\n",
      "    raise OpenAIError(\n",
      "litellm.llms.openai.OpenAIError: Error code: 429 - {'error': {'message': 'Rate limit exceeded. Please try again later.', 'type': 'rate_limit_error', 'param': None, 'code': 'rate_limit_exceeded'}}\n",
      "\n",
      "During handling of the above exception, another exception occurred:\n",
      "\n",
      "Traceback (most recent call last):\n",
      "  File \"/Users/tcapelle/miniforge3/envs/weave/lib/python3.11/site-packages/instructor/retry.py\", line 239, in retry_async\n",
      "    response: ChatCompletion = await func(*args, **kwargs)\n",
      "                               ^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/Users/tcapelle/miniforge3/envs/weave/lib/python3.11/site-packages/weave/trace/op.py\", line 333, in wrapper\n",
      "    res, _ = await _execute_call(wrapper, call, *args, **kwargs)  # type: ignore\n",
      "             ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/Users/tcapelle/miniforge3/envs/weave/lib/python3.11/site-packages/weave/trace/op.py\", line 213, in _call_async\n",
      "    return handle_exception(e)\n",
      "           ^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/Users/tcapelle/miniforge3/envs/weave/lib/python3.11/site-packages/weave/trace/op.py\", line 211, in _call_async\n",
      "    res = await func(*args, **kwargs)\n",
      "          ^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/Users/tcapelle/miniforge3/envs/weave/lib/python3.11/site-packages/litellm/utils.py\", line 1593, in wrapper_async\n",
      "    raise e\n",
      "  File \"/Users/tcapelle/miniforge3/envs/weave/lib/python3.11/site-packages/litellm/utils.py\", line 1413, in wrapper_async\n",
      "    result = await original_function(*args, **kwargs)\n",
      "             ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/Users/tcapelle/miniforge3/envs/weave/lib/python3.11/site-packages/litellm/main.py\", line 449, in acompletion\n",
      "    raise exception_type(\n",
      "          ^^^^^^^^^^^^^^^\n",
      "  File \"/Users/tcapelle/miniforge3/envs/weave/lib/python3.11/site-packages/litellm/utils.py\", line 8187, in exception_type\n",
      "    raise e\n",
      "  File \"/Users/tcapelle/miniforge3/envs/weave/lib/python3.11/site-packages/litellm/utils.py\", line 6555, in exception_type\n",
      "    raise RateLimitError(\n",
      "litellm.exceptions.RateLimitError: litellm.RateLimitError: RateLimitError: MistralException - Error code: 429 - {'error': {'message': 'Rate limit exceeded. Please try again later.', 'type': 'rate_limit_error', 'param': None, 'code': 'rate_limit_exceeded'}}\n",
      "\n",
      "The above exception was the direct cause of the following exception:\n",
      "\n",
      "Traceback (most recent call last):\n",
      "  File \"/Users/tcapelle/miniforge3/envs/weave/lib/python3.11/site-packages/instructor/retry.py\", line 235, in retry_async\n",
      "    async for attempt in max_retries:\n",
      "  File \"/Users/tcapelle/miniforge3/envs/weave/lib/python3.11/site-packages/tenacity/asyncio/__init__.py\", line 166, in __anext__\n",
      "    do = await self.iter(retry_state=self._retry_state)\n",
      "         ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/Users/tcapelle/miniforge3/envs/weave/lib/python3.11/site-packages/tenacity/asyncio/__init__.py\", line 153, in iter\n",
      "    result = await action(retry_state)\n",
      "             ^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/Users/tcapelle/miniforge3/envs/weave/lib/python3.11/site-packages/tenacity/_utils.py\", line 99, in inner\n",
      "    return call(*args, **kwargs)\n",
      "           ^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/Users/tcapelle/miniforge3/envs/weave/lib/python3.11/site-packages/tenacity/__init__.py\", line 419, in exc_check\n",
      "    raise retry_exc from fut.exception()\n",
      "tenacity.RetryError: RetryError[<Future at 0x33d3ec6d0 state=finished raised RateLimitError>]\n",
      "\n",
      "The above exception was the direct cause of the following exception:\n",
      "\n",
      "Traceback (most recent call last):\n",
      "  File \"/Users/tcapelle/miniforge3/envs/weave/lib/python3.11/site-packages/weave/flow/eval.py\", line 168, in predict_and_score\n",
      "    model_output = await async_call(model_predict, **model_predict_args)\n",
      "                   ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/Users/tcapelle/miniforge3/envs/weave/lib/python3.11/site-packages/weave/trace/op.py\", line 333, in wrapper\n",
      "    res, _ = await _execute_call(wrapper, call, *args, **kwargs)  # type: ignore\n",
      "             ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/Users/tcapelle/miniforge3/envs/weave/lib/python3.11/site-packages/weave/trace/op.py\", line 213, in _call_async\n",
      "    return handle_exception(e)\n",
      "           ^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/Users/tcapelle/miniforge3/envs/weave/lib/python3.11/site-packages/weave/trace/op.py\", line 211, in _call_async\n",
      "    res = await func(*args, **kwargs)\n",
      "          ^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/var/folders/sf/tgv7vcv96x557p38bvvp1ms40000gn/T/ipykernel_6554/970167077.py\", line 10, in predict\n",
      "    return await rag_solver_with_reflection(\n",
      "           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/Users/tcapelle/miniforge3/envs/weave/lib/python3.11/site-packages/weave/trace/op.py\", line 333, in wrapper\n",
      "    res, _ = await _execute_call(wrapper, call, *args, **kwargs)  # type: ignore\n",
      "             ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/Users/tcapelle/miniforge3/envs/weave/lib/python3.11/site-packages/weave/trace/op.py\", line 213, in _call_async\n",
      "    return handle_exception(e)\n",
      "           ^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/Users/tcapelle/miniforge3/envs/weave/lib/python3.11/site-packages/weave/trace/op.py\", line 211, in _call_async\n",
      "    res = await func(*args, **kwargs)\n",
      "          ^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/var/folders/sf/tgv7vcv96x557p38bvvp1ms40000gn/T/ipykernel_6554/4070537091.py\", line 14, in rag_solver_with_reflection\n",
      "    rag_result = await rag_solver(\n",
      "                 ^^^^^^^^^^^^^^^^^\n",
      "  File \"/Users/tcapelle/miniforge3/envs/weave/lib/python3.11/site-packages/weave/trace/op.py\", line 333, in wrapper\n",
      "    res, _ = await _execute_call(wrapper, call, *args, **kwargs)  # type: ignore\n",
      "             ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/Users/tcapelle/miniforge3/envs/weave/lib/python3.11/site-packages/weave/trace/op.py\", line 213, in _call_async\n",
      "    return handle_exception(e)\n",
      "           ^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/Users/tcapelle/miniforge3/envs/weave/lib/python3.11/site-packages/weave/trace/op.py\", line 211, in _call_async\n",
      "    res = await func(*args, **kwargs)\n",
      "          ^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/Users/tcapelle/work/hackercup_rag/agent.py\", line 350, in rag_solver\n",
      "    zero_shot_result = await zero_shot_solver(\n",
      "                       ^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/Users/tcapelle/miniforge3/envs/weave/lib/python3.11/site-packages/weave/trace/op.py\", line 333, in wrapper\n",
      "    res, _ = await _execute_call(wrapper, call, *args, **kwargs)  # type: ignore\n",
      "             ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/Users/tcapelle/miniforge3/envs/weave/lib/python3.11/site-packages/weave/trace/op.py\", line 213, in _call_async\n",
      "    return handle_exception(e)\n",
      "           ^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/Users/tcapelle/miniforge3/envs/weave/lib/python3.11/site-packages/weave/trace/op.py\", line 211, in _call_async\n",
      "    res = await func(*args, **kwargs)\n",
      "          ^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/Users/tcapelle/work/hackercup_rag/agent.py\", line 330, in zero_shot_solver\n",
      "    solution = await draft_solution(\n",
      "               ^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/Users/tcapelle/miniforge3/envs/weave/lib/python3.11/site-packages/weave/trace/op.py\", line 333, in wrapper\n",
      "    res, _ = await _execute_call(wrapper, call, *args, **kwargs)  # type: ignore\n",
      "             ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/Users/tcapelle/miniforge3/envs/weave/lib/python3.11/site-packages/weave/trace/op.py\", line 213, in _call_async\n",
      "    return handle_exception(e)\n",
      "           ^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/Users/tcapelle/miniforge3/envs/weave/lib/python3.11/site-packages/weave/trace/op.py\", line 211, in _call_async\n",
      "    res = await func(*args, **kwargs)\n",
      "          ^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/Users/tcapelle/work/hackercup_rag/agent.py\", line 83, in draft_solution\n",
      "    response = await async_client.chat.completions.create(\n",
      "               ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/Users/tcapelle/miniforge3/envs/weave/lib/python3.11/site-packages/instructor/client.py\", line 300, in create\n",
      "    return await self.create_fn(\n",
      "           ^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/Users/tcapelle/miniforge3/envs/weave/lib/python3.11/site-packages/instructor/patch.py\", line 119, in new_create_async\n",
      "    response = await retry_async(\n",
      "               ^^^^^^^^^^^^^^^^^^\n",
      "  File \"/Users/tcapelle/miniforge3/envs/weave/lib/python3.11/site-packages/instructor/retry.py\", line 267, in retry_async\n",
      "    raise InstructorRetryException(\n",
      "instructor.exceptions.InstructorRetryException: RetryError[<Future at 0x33d3ec6d0 state=finished raised RateLimitError>]\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\">Predict and score failed\n",
       "</pre>\n"
      ],
      "text/plain": [
       "Predict and score failed\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Traceback (most recent call last):\n",
      "  File \"/Users/tcapelle/miniforge3/envs/weave/lib/python3.11/site-packages/weave/flow/eval.py\", line 287, in eval_example\n",
      "    eval_row = await self.predict_and_score(model, example)\n",
      "               ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/Users/tcapelle/miniforge3/envs/weave/lib/python3.11/site-packages/weave/trace/op.py\", line 333, in wrapper\n",
      "    res, _ = await _execute_call(wrapper, call, *args, **kwargs)  # type: ignore\n",
      "             ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/Users/tcapelle/miniforge3/envs/weave/lib/python3.11/site-packages/weave/trace/op.py\", line 213, in _call_async\n",
      "    return handle_exception(e)\n",
      "           ^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/Users/tcapelle/miniforge3/envs/weave/lib/python3.11/site-packages/weave/trace/op.py\", line 211, in _call_async\n",
      "    res = await func(*args, **kwargs)\n",
      "          ^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/Users/tcapelle/miniforge3/envs/weave/lib/python3.11/site-packages/weave/flow/eval.py\", line 224, in predict_and_score\n",
      "    result = await async_call(score_fn, **score_args)\n",
      "             ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/Users/tcapelle/miniforge3/envs/weave/lib/python3.11/asyncio/threads.py\", line 25, in to_thread\n",
      "    return await loop.run_in_executor(None, func_call)\n",
      "           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/Users/tcapelle/miniforge3/envs/weave/lib/python3.11/asyncio/futures.py\", line 287, in __await__\n",
      "    yield self  # This tells Task to wait for completion.\n",
      "    ^^^^^^^^^^\n",
      "  File \"/Users/tcapelle/miniforge3/envs/weave/lib/python3.11/asyncio/tasks.py\", line 349, in __wakeup\n",
      "    future.result()\n",
      "  File \"/Users/tcapelle/miniforge3/envs/weave/lib/python3.11/asyncio/futures.py\", line 203, in result\n",
      "    raise self._exception.with_traceback(self._exception_tb)\n",
      "  File \"/Users/tcapelle/miniforge3/envs/weave/lib/python3.11/concurrent/futures/thread.py\", line 58, in run\n",
      "    result = self.fn(*self.args, **self.kwargs)\n",
      "             ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/Users/tcapelle/miniforge3/envs/weave/lib/python3.11/site-packages/weave/trace/op.py\", line 344, in wrapper\n",
      "    res, _ = _execute_call(wrapper, call, *args, **kwargs)  # type: ignore\n",
      "             ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/Users/tcapelle/miniforge3/envs/weave/lib/python3.11/site-packages/weave/trace/op.py\", line 224, in _execute_call\n",
      "    handle_exception(e)\n",
      "  File \"/Users/tcapelle/miniforge3/envs/weave/lib/python3.11/site-packages/weave/trace/op.py\", line 222, in _execute_call\n",
      "    res = func(*args, **kwargs)\n",
      "          ^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/var/folders/sf/tgv7vcv96x557p38bvvp1ms40000gn/T/ipykernel_6554/249536452.py\", line 9, in scorer\n",
      "    return {\"passed\": expected == model_output[\"test_report\"]}\n",
      "                                  ~~~~~~~~~~~~^^^^^^^^^^^^^^^\n",
      "TypeError: 'NoneType' object is not subscriptable\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\">Evaluated <span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">4</span> of <span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">5</span> examples\n",
       "</pre>\n"
      ],
      "text/plain": [
       "Evaluated \u001b[1;36m4\u001b[0m of \u001b[1;36m5\u001b[0m examples\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2024-09-05 23:05:37,556 : INFO : HTTP Request: POST http://195.242.24.252:8000/v1/chat/completions \"HTTP/1.1 429 Too Many Requests\"\n",
      "2024-09-05 23:05:37,557 : INFO : Retrying request to /chat/completions in 1.739371 seconds\n",
      "2024-09-05 23:05:39,367 : INFO : HTTP Request: POST http://195.242.24.252:8000/v1/chat/completions \"HTTP/1.1 429 Too Many Requests\"\n",
      "\u001b[92m23:05:39 - LiteLLM:INFO\u001b[0m: utils.py:2975 - \n",
      "LiteLLM completion() model= open-mistral-nemo-2407; provider = mistral\n",
      "2024-09-05 23:05:39,382 : INFO : \n",
      "LiteLLM completion() model= open-mistral-nemo-2407; provider = mistral\n",
      "2024-09-05 23:05:39,444 : INFO : HTTP Request: POST http://195.242.24.252:8000/v1/chat/completions \"HTTP/1.1 429 Too Many Requests\"\n",
      "2024-09-05 23:05:39,445 : INFO : Retrying request to /chat/completions in 0.754823 seconds\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "\u001b[1;31mGive Feedback / Get Help: https://github.com/BerriAI/litellm/issues/new\u001b[0m\n",
      "LiteLLM.Info: If you need to debug this error, use `litellm.set_verbose=True'.\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2024-09-05 23:05:40,260 : INFO : HTTP Request: POST http://195.242.24.252:8000/v1/chat/completions \"HTTP/1.1 429 Too Many Requests\"\n",
      "2024-09-05 23:05:40,261 : INFO : Retrying request to /chat/completions in 1.981091 seconds\n",
      "2024-09-05 23:05:42,304 : INFO : HTTP Request: POST http://195.242.24.252:8000/v1/chat/completions \"HTTP/1.1 429 Too Many Requests\"\n",
      "2024-09-05 23:05:42,314 : ERROR : Failed after retries: <bound method Future.exception of <Future at 0x33d245890 state=finished raised RateLimitError>>\n",
      "Traceback (most recent call last):\n",
      "  File \"/Users/tcapelle/miniforge3/envs/weave/lib/python3.11/site-packages/litellm/llms/openai.py\", line 1089, in acompletion\n",
      "    headers, response = await self.make_openai_chat_completion_request(\n",
      "                        ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/Users/tcapelle/miniforge3/envs/weave/lib/python3.11/site-packages/litellm/llms/openai.py\", line 793, in make_openai_chat_completion_request\n",
      "    raise e\n",
      "  File \"/Users/tcapelle/miniforge3/envs/weave/lib/python3.11/site-packages/litellm/llms/openai.py\", line 781, in make_openai_chat_completion_request\n",
      "    await openai_aclient.chat.completions.with_raw_response.create(\n",
      "  File \"/Users/tcapelle/miniforge3/envs/weave/lib/python3.11/site-packages/openai/_legacy_response.py\", line 367, in wrapped\n",
      "    return cast(LegacyAPIResponse[R], await func(*args, **kwargs))\n",
      "                                      ^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/Users/tcapelle/miniforge3/envs/weave/lib/python3.11/site-packages/weave/trace/op.py\", line 333, in wrapper\n",
      "    res, _ = await _execute_call(wrapper, call, *args, **kwargs)  # type: ignore\n",
      "             ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/Users/tcapelle/miniforge3/envs/weave/lib/python3.11/site-packages/weave/trace/op.py\", line 213, in _call_async\n",
      "    return handle_exception(e)\n",
      "           ^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/Users/tcapelle/miniforge3/envs/weave/lib/python3.11/site-packages/weave/trace/op.py\", line 211, in _call_async\n",
      "    res = await func(*args, **kwargs)\n",
      "          ^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/Users/tcapelle/miniforge3/envs/weave/lib/python3.11/site-packages/weave/integrations/openai/openai_sdk.py\", line 288, in _wrapper\n",
      "    return await fn(*args, **kwargs)\n",
      "           ^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/Users/tcapelle/miniforge3/envs/weave/lib/python3.11/site-packages/openai/resources/chat/completions.py\", line 1339, in create\n",
      "    return await self._post(\n",
      "           ^^^^^^^^^^^^^^^^^\n",
      "  File \"/Users/tcapelle/miniforge3/envs/weave/lib/python3.11/site-packages/openai/_base_client.py\", line 1816, in post\n",
      "    return await self.request(cast_to, opts, stream=stream, stream_cls=stream_cls)\n",
      "           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/Users/tcapelle/miniforge3/envs/weave/lib/python3.11/site-packages/openai/_base_client.py\", line 1510, in request\n",
      "    return await self._request(\n",
      "           ^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/Users/tcapelle/miniforge3/envs/weave/lib/python3.11/site-packages/openai/_base_client.py\", line 1596, in _request\n",
      "    return await self._retry_request(\n",
      "           ^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/Users/tcapelle/miniforge3/envs/weave/lib/python3.11/site-packages/openai/_base_client.py\", line 1643, in _retry_request\n",
      "    return await self._request(\n",
      "           ^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/Users/tcapelle/miniforge3/envs/weave/lib/python3.11/site-packages/openai/_base_client.py\", line 1596, in _request\n",
      "    return await self._retry_request(\n",
      "           ^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/Users/tcapelle/miniforge3/envs/weave/lib/python3.11/site-packages/openai/_base_client.py\", line 1643, in _retry_request\n",
      "    return await self._request(\n",
      "           ^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/Users/tcapelle/miniforge3/envs/weave/lib/python3.11/site-packages/openai/_base_client.py\", line 1611, in _request\n",
      "    raise self._make_status_error_from_response(err.response) from None\n",
      "openai.RateLimitError: Error code: 429 - {'error': {'message': 'Rate limit exceeded. Please try again later.', 'type': 'rate_limit_error', 'param': None, 'code': 'rate_limit_exceeded'}}\n",
      "\n",
      "During handling of the above exception, another exception occurred:\n",
      "\n",
      "Traceback (most recent call last):\n",
      "  File \"/Users/tcapelle/miniforge3/envs/weave/lib/python3.11/site-packages/litellm/main.py\", line 427, in acompletion\n",
      "    response = await init_response\n",
      "               ^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/Users/tcapelle/miniforge3/envs/weave/lib/python3.11/site-packages/litellm/llms/openai.py\", line 1136, in acompletion\n",
      "    raise OpenAIError(\n",
      "litellm.llms.openai.OpenAIError: Error code: 429 - {'error': {'message': 'Rate limit exceeded. Please try again later.', 'type': 'rate_limit_error', 'param': None, 'code': 'rate_limit_exceeded'}}\n",
      "\n",
      "During handling of the above exception, another exception occurred:\n",
      "\n",
      "Traceback (most recent call last):\n",
      "  File \"/Users/tcapelle/miniforge3/envs/weave/lib/python3.11/site-packages/instructor/retry.py\", line 239, in retry_async\n",
      "    response: ChatCompletion = await func(*args, **kwargs)\n",
      "                               ^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/Users/tcapelle/miniforge3/envs/weave/lib/python3.11/site-packages/weave/trace/op.py\", line 333, in wrapper\n",
      "    res, _ = await _execute_call(wrapper, call, *args, **kwargs)  # type: ignore\n",
      "             ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/Users/tcapelle/miniforge3/envs/weave/lib/python3.11/site-packages/weave/trace/op.py\", line 213, in _call_async\n",
      "    return handle_exception(e)\n",
      "           ^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/Users/tcapelle/miniforge3/envs/weave/lib/python3.11/site-packages/weave/trace/op.py\", line 211, in _call_async\n",
      "    res = await func(*args, **kwargs)\n",
      "          ^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/Users/tcapelle/miniforge3/envs/weave/lib/python3.11/site-packages/litellm/utils.py\", line 1593, in wrapper_async\n",
      "    raise e\n",
      "  File \"/Users/tcapelle/miniforge3/envs/weave/lib/python3.11/site-packages/litellm/utils.py\", line 1413, in wrapper_async\n",
      "    result = await original_function(*args, **kwargs)\n",
      "             ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/Users/tcapelle/miniforge3/envs/weave/lib/python3.11/site-packages/litellm/main.py\", line 449, in acompletion\n",
      "    raise exception_type(\n",
      "          ^^^^^^^^^^^^^^^\n",
      "  File \"/Users/tcapelle/miniforge3/envs/weave/lib/python3.11/site-packages/litellm/utils.py\", line 8187, in exception_type\n",
      "    raise e\n",
      "  File \"/Users/tcapelle/miniforge3/envs/weave/lib/python3.11/site-packages/litellm/utils.py\", line 6555, in exception_type\n",
      "    raise RateLimitError(\n",
      "litellm.exceptions.RateLimitError: litellm.RateLimitError: RateLimitError: MistralException - Error code: 429 - {'error': {'message': 'Rate limit exceeded. Please try again later.', 'type': 'rate_limit_error', 'param': None, 'code': 'rate_limit_exceeded'}}\n",
      "\n",
      "The above exception was the direct cause of the following exception:\n",
      "\n",
      "Traceback (most recent call last):\n",
      "  File \"/Users/tcapelle/miniforge3/envs/weave/lib/python3.11/site-packages/instructor/retry.py\", line 235, in retry_async\n",
      "    async for attempt in max_retries:\n",
      "  File \"/Users/tcapelle/miniforge3/envs/weave/lib/python3.11/site-packages/tenacity/asyncio/__init__.py\", line 166, in __anext__\n",
      "    do = await self.iter(retry_state=self._retry_state)\n",
      "         ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/Users/tcapelle/miniforge3/envs/weave/lib/python3.11/site-packages/tenacity/asyncio/__init__.py\", line 153, in iter\n",
      "    result = await action(retry_state)\n",
      "             ^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/Users/tcapelle/miniforge3/envs/weave/lib/python3.11/site-packages/tenacity/_utils.py\", line 99, in inner\n",
      "    return call(*args, **kwargs)\n",
      "           ^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/Users/tcapelle/miniforge3/envs/weave/lib/python3.11/site-packages/tenacity/__init__.py\", line 419, in exc_check\n",
      "    raise retry_exc from fut.exception()\n",
      "tenacity.RetryError: RetryError[<Future at 0x33d245890 state=finished raised RateLimitError>]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "\u001b[1;31mGive Feedback / Get Help: https://github.com/BerriAI/litellm/issues/new\u001b[0m\n",
      "LiteLLM.Info: If you need to debug this error, use `litellm.set_verbose=True'.\n",
      "\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\">model_output failed\n",
       "</pre>\n"
      ],
      "text/plain": [
       "model_output failed\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Traceback (most recent call last):\n",
      "  File \"/Users/tcapelle/miniforge3/envs/weave/lib/python3.11/site-packages/litellm/llms/openai.py\", line 1089, in acompletion\n",
      "    headers, response = await self.make_openai_chat_completion_request(\n",
      "                        ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/Users/tcapelle/miniforge3/envs/weave/lib/python3.11/site-packages/litellm/llms/openai.py\", line 793, in make_openai_chat_completion_request\n",
      "    raise e\n",
      "  File \"/Users/tcapelle/miniforge3/envs/weave/lib/python3.11/site-packages/litellm/llms/openai.py\", line 781, in make_openai_chat_completion_request\n",
      "    await openai_aclient.chat.completions.with_raw_response.create(\n",
      "  File \"/Users/tcapelle/miniforge3/envs/weave/lib/python3.11/site-packages/openai/_legacy_response.py\", line 367, in wrapped\n",
      "    return cast(LegacyAPIResponse[R], await func(*args, **kwargs))\n",
      "                                      ^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/Users/tcapelle/miniforge3/envs/weave/lib/python3.11/site-packages/weave/trace/op.py\", line 333, in wrapper\n",
      "    res, _ = await _execute_call(wrapper, call, *args, **kwargs)  # type: ignore\n",
      "             ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/Users/tcapelle/miniforge3/envs/weave/lib/python3.11/site-packages/weave/trace/op.py\", line 213, in _call_async\n",
      "    return handle_exception(e)\n",
      "           ^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/Users/tcapelle/miniforge3/envs/weave/lib/python3.11/site-packages/weave/trace/op.py\", line 211, in _call_async\n",
      "    res = await func(*args, **kwargs)\n",
      "          ^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/Users/tcapelle/miniforge3/envs/weave/lib/python3.11/site-packages/weave/integrations/openai/openai_sdk.py\", line 288, in _wrapper\n",
      "    return await fn(*args, **kwargs)\n",
      "           ^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/Users/tcapelle/miniforge3/envs/weave/lib/python3.11/site-packages/openai/resources/chat/completions.py\", line 1339, in create\n",
      "    return await self._post(\n",
      "           ^^^^^^^^^^^^^^^^^\n",
      "  File \"/Users/tcapelle/miniforge3/envs/weave/lib/python3.11/site-packages/openai/_base_client.py\", line 1816, in post\n",
      "    return await self.request(cast_to, opts, stream=stream, stream_cls=stream_cls)\n",
      "           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/Users/tcapelle/miniforge3/envs/weave/lib/python3.11/site-packages/openai/_base_client.py\", line 1510, in request\n",
      "    return await self._request(\n",
      "           ^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/Users/tcapelle/miniforge3/envs/weave/lib/python3.11/site-packages/openai/_base_client.py\", line 1596, in _request\n",
      "    return await self._retry_request(\n",
      "           ^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/Users/tcapelle/miniforge3/envs/weave/lib/python3.11/site-packages/openai/_base_client.py\", line 1643, in _retry_request\n",
      "    return await self._request(\n",
      "           ^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/Users/tcapelle/miniforge3/envs/weave/lib/python3.11/site-packages/openai/_base_client.py\", line 1596, in _request\n",
      "    return await self._retry_request(\n",
      "           ^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/Users/tcapelle/miniforge3/envs/weave/lib/python3.11/site-packages/openai/_base_client.py\", line 1643, in _retry_request\n",
      "    return await self._request(\n",
      "           ^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/Users/tcapelle/miniforge3/envs/weave/lib/python3.11/site-packages/openai/_base_client.py\", line 1611, in _request\n",
      "    raise self._make_status_error_from_response(err.response) from None\n",
      "openai.RateLimitError: Error code: 429 - {'error': {'message': 'Rate limit exceeded. Please try again later.', 'type': 'rate_limit_error', 'param': None, 'code': 'rate_limit_exceeded'}}\n",
      "\n",
      "During handling of the above exception, another exception occurred:\n",
      "\n",
      "Traceback (most recent call last):\n",
      "  File \"/Users/tcapelle/miniforge3/envs/weave/lib/python3.11/site-packages/litellm/main.py\", line 427, in acompletion\n",
      "    response = await init_response\n",
      "               ^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/Users/tcapelle/miniforge3/envs/weave/lib/python3.11/site-packages/litellm/llms/openai.py\", line 1136, in acompletion\n",
      "    raise OpenAIError(\n",
      "litellm.llms.openai.OpenAIError: Error code: 429 - {'error': {'message': 'Rate limit exceeded. Please try again later.', 'type': 'rate_limit_error', 'param': None, 'code': 'rate_limit_exceeded'}}\n",
      "\n",
      "During handling of the above exception, another exception occurred:\n",
      "\n",
      "Traceback (most recent call last):\n",
      "  File \"/Users/tcapelle/miniforge3/envs/weave/lib/python3.11/site-packages/instructor/retry.py\", line 239, in retry_async\n",
      "    response: ChatCompletion = await func(*args, **kwargs)\n",
      "                               ^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/Users/tcapelle/miniforge3/envs/weave/lib/python3.11/site-packages/weave/trace/op.py\", line 333, in wrapper\n",
      "    res, _ = await _execute_call(wrapper, call, *args, **kwargs)  # type: ignore\n",
      "             ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/Users/tcapelle/miniforge3/envs/weave/lib/python3.11/site-packages/weave/trace/op.py\", line 213, in _call_async\n",
      "    return handle_exception(e)\n",
      "           ^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/Users/tcapelle/miniforge3/envs/weave/lib/python3.11/site-packages/weave/trace/op.py\", line 211, in _call_async\n",
      "    res = await func(*args, **kwargs)\n",
      "          ^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/Users/tcapelle/miniforge3/envs/weave/lib/python3.11/site-packages/litellm/utils.py\", line 1593, in wrapper_async\n",
      "    raise e\n",
      "  File \"/Users/tcapelle/miniforge3/envs/weave/lib/python3.11/site-packages/litellm/utils.py\", line 1413, in wrapper_async\n",
      "    result = await original_function(*args, **kwargs)\n",
      "             ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/Users/tcapelle/miniforge3/envs/weave/lib/python3.11/site-packages/litellm/main.py\", line 449, in acompletion\n",
      "    raise exception_type(\n",
      "          ^^^^^^^^^^^^^^^\n",
      "  File \"/Users/tcapelle/miniforge3/envs/weave/lib/python3.11/site-packages/litellm/utils.py\", line 8187, in exception_type\n",
      "    raise e\n",
      "  File \"/Users/tcapelle/miniforge3/envs/weave/lib/python3.11/site-packages/litellm/utils.py\", line 6555, in exception_type\n",
      "    raise RateLimitError(\n",
      "litellm.exceptions.RateLimitError: litellm.RateLimitError: RateLimitError: MistralException - Error code: 429 - {'error': {'message': 'Rate limit exceeded. Please try again later.', 'type': 'rate_limit_error', 'param': None, 'code': 'rate_limit_exceeded'}}\n",
      "\n",
      "The above exception was the direct cause of the following exception:\n",
      "\n",
      "Traceback (most recent call last):\n",
      "  File \"/Users/tcapelle/miniforge3/envs/weave/lib/python3.11/site-packages/instructor/retry.py\", line 235, in retry_async\n",
      "    async for attempt in max_retries:\n",
      "  File \"/Users/tcapelle/miniforge3/envs/weave/lib/python3.11/site-packages/tenacity/asyncio/__init__.py\", line 166, in __anext__\n",
      "    do = await self.iter(retry_state=self._retry_state)\n",
      "         ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/Users/tcapelle/miniforge3/envs/weave/lib/python3.11/site-packages/tenacity/asyncio/__init__.py\", line 153, in iter\n",
      "    result = await action(retry_state)\n",
      "             ^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/Users/tcapelle/miniforge3/envs/weave/lib/python3.11/site-packages/tenacity/_utils.py\", line 99, in inner\n",
      "    return call(*args, **kwargs)\n",
      "           ^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/Users/tcapelle/miniforge3/envs/weave/lib/python3.11/site-packages/tenacity/__init__.py\", line 419, in exc_check\n",
      "    raise retry_exc from fut.exception()\n",
      "tenacity.RetryError: RetryError[<Future at 0x33d245890 state=finished raised RateLimitError>]\n",
      "\n",
      "The above exception was the direct cause of the following exception:\n",
      "\n",
      "Traceback (most recent call last):\n",
      "  File \"/Users/tcapelle/miniforge3/envs/weave/lib/python3.11/site-packages/weave/flow/eval.py\", line 168, in predict_and_score\n",
      "    model_output = await async_call(model_predict, **model_predict_args)\n",
      "                   ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/Users/tcapelle/miniforge3/envs/weave/lib/python3.11/site-packages/weave/trace/op.py\", line 333, in wrapper\n",
      "    res, _ = await _execute_call(wrapper, call, *args, **kwargs)  # type: ignore\n",
      "             ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/Users/tcapelle/miniforge3/envs/weave/lib/python3.11/site-packages/weave/trace/op.py\", line 213, in _call_async\n",
      "    return handle_exception(e)\n",
      "           ^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/Users/tcapelle/miniforge3/envs/weave/lib/python3.11/site-packages/weave/trace/op.py\", line 211, in _call_async\n",
      "    res = await func(*args, **kwargs)\n",
      "          ^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/var/folders/sf/tgv7vcv96x557p38bvvp1ms40000gn/T/ipykernel_6554/970167077.py\", line 10, in predict\n",
      "    return await rag_solver_with_reflection(\n",
      "           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/Users/tcapelle/miniforge3/envs/weave/lib/python3.11/site-packages/weave/trace/op.py\", line 333, in wrapper\n",
      "    res, _ = await _execute_call(wrapper, call, *args, **kwargs)  # type: ignore\n",
      "             ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/Users/tcapelle/miniforge3/envs/weave/lib/python3.11/site-packages/weave/trace/op.py\", line 213, in _call_async\n",
      "    return handle_exception(e)\n",
      "           ^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/Users/tcapelle/miniforge3/envs/weave/lib/python3.11/site-packages/weave/trace/op.py\", line 211, in _call_async\n",
      "    res = await func(*args, **kwargs)\n",
      "          ^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/var/folders/sf/tgv7vcv96x557p38bvvp1ms40000gn/T/ipykernel_6554/4070537091.py\", line 14, in rag_solver_with_reflection\n",
      "    rag_result = await rag_solver(\n",
      "                 ^^^^^^^^^^^^^^^^^\n",
      "  File \"/Users/tcapelle/miniforge3/envs/weave/lib/python3.11/site-packages/weave/trace/op.py\", line 333, in wrapper\n",
      "    res, _ = await _execute_call(wrapper, call, *args, **kwargs)  # type: ignore\n",
      "             ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/Users/tcapelle/miniforge3/envs/weave/lib/python3.11/site-packages/weave/trace/op.py\", line 213, in _call_async\n",
      "    return handle_exception(e)\n",
      "           ^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/Users/tcapelle/miniforge3/envs/weave/lib/python3.11/site-packages/weave/trace/op.py\", line 211, in _call_async\n",
      "    res = await func(*args, **kwargs)\n",
      "          ^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/Users/tcapelle/work/hackercup_rag/agent.py\", line 350, in rag_solver\n",
      "    zero_shot_result = await zero_shot_solver(\n",
      "                       ^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/Users/tcapelle/miniforge3/envs/weave/lib/python3.11/site-packages/weave/trace/op.py\", line 333, in wrapper\n",
      "    res, _ = await _execute_call(wrapper, call, *args, **kwargs)  # type: ignore\n",
      "             ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/Users/tcapelle/miniforge3/envs/weave/lib/python3.11/site-packages/weave/trace/op.py\", line 213, in _call_async\n",
      "    return handle_exception(e)\n",
      "           ^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/Users/tcapelle/miniforge3/envs/weave/lib/python3.11/site-packages/weave/trace/op.py\", line 211, in _call_async\n",
      "    res = await func(*args, **kwargs)\n",
      "          ^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/Users/tcapelle/work/hackercup_rag/agent.py\", line 330, in zero_shot_solver\n",
      "    solution = await draft_solution(\n",
      "               ^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/Users/tcapelle/miniforge3/envs/weave/lib/python3.11/site-packages/weave/trace/op.py\", line 333, in wrapper\n",
      "    res, _ = await _execute_call(wrapper, call, *args, **kwargs)  # type: ignore\n",
      "             ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/Users/tcapelle/miniforge3/envs/weave/lib/python3.11/site-packages/weave/trace/op.py\", line 213, in _call_async\n",
      "    return handle_exception(e)\n",
      "           ^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/Users/tcapelle/miniforge3/envs/weave/lib/python3.11/site-packages/weave/trace/op.py\", line 211, in _call_async\n",
      "    res = await func(*args, **kwargs)\n",
      "          ^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/Users/tcapelle/work/hackercup_rag/agent.py\", line 94, in draft_solution\n",
      "    formatted_response = await format_response(\n",
      "                         ^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/Users/tcapelle/miniforge3/envs/weave/lib/python3.11/site-packages/weave/trace/op.py\", line 333, in wrapper\n",
      "    res, _ = await _execute_call(wrapper, call, *args, **kwargs)  # type: ignore\n",
      "             ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/Users/tcapelle/miniforge3/envs/weave/lib/python3.11/site-packages/weave/trace/op.py\", line 213, in _call_async\n",
      "    return handle_exception(e)\n",
      "           ^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/Users/tcapelle/miniforge3/envs/weave/lib/python3.11/site-packages/weave/trace/op.py\", line 211, in _call_async\n",
      "    res = await func(*args, **kwargs)\n",
      "          ^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/Users/tcapelle/work/hackercup_rag/utils.py\", line 170, in format_response\n",
      "    formatted_response = await async_client.chat.completions.create(\n",
      "                         ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/Users/tcapelle/miniforge3/envs/weave/lib/python3.11/site-packages/instructor/client.py\", line 300, in create\n",
      "    return await self.create_fn(\n",
      "           ^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/Users/tcapelle/miniforge3/envs/weave/lib/python3.11/site-packages/instructor/patch.py\", line 119, in new_create_async\n",
      "    response = await retry_async(\n",
      "               ^^^^^^^^^^^^^^^^^^\n",
      "  File \"/Users/tcapelle/miniforge3/envs/weave/lib/python3.11/site-packages/instructor/retry.py\", line 267, in retry_async\n",
      "    raise InstructorRetryException(\n",
      "instructor.exceptions.InstructorRetryException: RetryError[<Future at 0x33d245890 state=finished raised RateLimitError>]\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\">Predict and score failed\n",
       "</pre>\n"
      ],
      "text/plain": [
       "Predict and score failed\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Traceback (most recent call last):\n",
      "  File \"/Users/tcapelle/miniforge3/envs/weave/lib/python3.11/site-packages/weave/flow/eval.py\", line 287, in eval_example\n",
      "    eval_row = await self.predict_and_score(model, example)\n",
      "               ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/Users/tcapelle/miniforge3/envs/weave/lib/python3.11/site-packages/weave/trace/op.py\", line 333, in wrapper\n",
      "    res, _ = await _execute_call(wrapper, call, *args, **kwargs)  # type: ignore\n",
      "             ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/Users/tcapelle/miniforge3/envs/weave/lib/python3.11/site-packages/weave/trace/op.py\", line 213, in _call_async\n",
      "    return handle_exception(e)\n",
      "           ^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/Users/tcapelle/miniforge3/envs/weave/lib/python3.11/site-packages/weave/trace/op.py\", line 211, in _call_async\n",
      "    res = await func(*args, **kwargs)\n",
      "          ^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/Users/tcapelle/miniforge3/envs/weave/lib/python3.11/site-packages/weave/flow/eval.py\", line 224, in predict_and_score\n",
      "    result = await async_call(score_fn, **score_args)\n",
      "             ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/Users/tcapelle/miniforge3/envs/weave/lib/python3.11/asyncio/threads.py\", line 25, in to_thread\n",
      "    return await loop.run_in_executor(None, func_call)\n",
      "           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/Users/tcapelle/miniforge3/envs/weave/lib/python3.11/asyncio/futures.py\", line 287, in __await__\n",
      "    yield self  # This tells Task to wait for completion.\n",
      "    ^^^^^^^^^^\n",
      "  File \"/Users/tcapelle/miniforge3/envs/weave/lib/python3.11/asyncio/tasks.py\", line 349, in __wakeup\n",
      "    future.result()\n",
      "  File \"/Users/tcapelle/miniforge3/envs/weave/lib/python3.11/asyncio/futures.py\", line 203, in result\n",
      "    raise self._exception.with_traceback(self._exception_tb)\n",
      "  File \"/Users/tcapelle/miniforge3/envs/weave/lib/python3.11/concurrent/futures/thread.py\", line 58, in run\n",
      "    result = self.fn(*self.args, **self.kwargs)\n",
      "             ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/Users/tcapelle/miniforge3/envs/weave/lib/python3.11/site-packages/weave/trace/op.py\", line 344, in wrapper\n",
      "    res, _ = _execute_call(wrapper, call, *args, **kwargs)  # type: ignore\n",
      "             ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/Users/tcapelle/miniforge3/envs/weave/lib/python3.11/site-packages/weave/trace/op.py\", line 224, in _execute_call\n",
      "    handle_exception(e)\n",
      "  File \"/Users/tcapelle/miniforge3/envs/weave/lib/python3.11/site-packages/weave/trace/op.py\", line 222, in _execute_call\n",
      "    res = func(*args, **kwargs)\n",
      "          ^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/var/folders/sf/tgv7vcv96x557p38bvvp1ms40000gn/T/ipykernel_6554/249536452.py\", line 9, in scorer\n",
      "    return {\"passed\": expected == model_output[\"test_report\"]}\n",
      "                                  ~~~~~~~~~~~~^^^^^^^^^^^^^^^\n",
      "TypeError: 'NoneType' object is not subscriptable\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\">Evaluated <span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">5</span> of <span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">5</span> examples\n",
       "</pre>\n"
      ],
      "text/plain": [
       "Evaluated \u001b[1;36m5\u001b[0m of \u001b[1;36m5\u001b[0m examples\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\">Evaluation summary\n",
       "<span style=\"font-weight: bold\">{</span><span style=\"color: #008000; text-decoration-color: #008000\">'scorer'</span>: <span style=\"color: #800080; text-decoration-color: #800080; font-style: italic\">None</span><span style=\"font-weight: bold\">}</span>\n",
       "</pre>\n"
      ],
      "text/plain": [
       "Evaluation summary\n",
       "\u001b[1m{\u001b[0m\u001b[32m'scorer'\u001b[0m: \u001b[3;35mNone\u001b[0m\u001b[1m}\u001b[0m\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# Evaluate the RAG reflection agent for all the models and temperatures\n",
    "tasks = []\n",
    "\n",
    "LLM = STRONG_LLM\n",
    "eval_temperatures = [0.0, 0.5]\n",
    "\n",
    "\n",
    "for temperature in eval_temperatures:\n",
    "        rag_reflection_agent = RAGReflectionAgent(\n",
    "            retriever=retriever, model=LLM, temperature=temperature, timeout=30\n",
    "        )\n",
    "        rag_reflection_results = evaluation.evaluate(rag_reflection_agent)\n",
    "        tasks.append(rag_reflection_results)\n",
    "rag_reflection_results = await asyncio.gather(*tasks)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Okay, that completes the demo!\n",
    "\n",
    "Key takeaways from this demo:\n",
    "1. We tried to solve some challenging competitive programming problems using LLM agents.\n",
    "2. We tried three different agents:\n",
    "    - Zero-shot agent\n",
    "    - RAG agent\n",
    "    - RAG reflection agent\n",
    "3. We used Weave to evaluate the agents and compare their performance.\n",
    "\n",
    "We hope you found this demo useful and interesting and that it gave you some ideas on how to use LLM agents to solve challenging problems.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  }
 ],
 "metadata": {
  "accelerator": "GPU",
  "colab": {
   "include_colab_link": true,
   "provenance": [],
   "toc_visible": true
  },
  "kernelspec": {
   "display_name": "Python 3",
   "name": "python3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
