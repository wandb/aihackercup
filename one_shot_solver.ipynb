{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a href=\"https://colab.research.google.com/github/wandb/aihackercup/blob/main/one_shot_solver.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# A simple one-shot solver for the AI Hacker Cup 2024 Qualification Round"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Setup "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Note: You need to run this cell only once**\n",
    "We will clone the starter-kits repo\n",
    "Set the rag folder as our working directory\n",
    "and install the dependencies for the project.\n",
    "\n",
    "**You can comment out the cell after you have run it once.**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# # Clone the starter-kits repo\n",
    "# !git clone https://github.com/wandb/aihackercup\n",
    "# # Change directory to the rag folder. Running the next line twice in the same session will raise an error.\n",
    "# %cd aihackercup\n",
    "# # Install dependencies\n",
    "# !pip install -r requirements.txt -qq"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "To run this colab, create a [free Weights & Biases (W&B) account here](https://wandb.ai/site?utm_source=colab&utm_medium=code&utm_campaign=lightning-ai-hacker-cup) and then copy your API key from https://wandb.ai/authorize into the input box below when requested."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import weave\n",
    "\n",
    "WEAVE_PROJECT = \"ai-hacker-cup-benchmark\"\n",
    "weave_client = weave.init(WEAVE_PROJECT)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "os.environ[\"FAST_LLM\"] =\"gpt-4o-2024-08-06\"\n",
    "os.environ[\"STRONG_LLM\"] = \"gpt-4o-2024-08-06\"\n",
    "\n",
    "\n",
    "# URL for the MistralAI api we'll be using\n",
    "# os.environ[\"BASE_URL\"] = \"http://195.242.25.198:8000/v1\"\n",
    "# os.environ[\"API_KEY\"] = \"dummy_key\"\n",
    "\n",
    "# Set the max tokens for the models and how many parallel requests to make in Weave Evaluations\n",
    "# os.environ[\"MAX_TOKENS\"] = \"4096\"\n",
    "os.environ[\"WEAVE_PARALLELISM\"] = \"5\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 27,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import asyncio\n",
    "import logging\n",
    "\n",
    "# Start of workout\n",
    "from utils import Problem, async_client, oai_client, format_response, check_correctness\n",
    "\n",
    "from dotenv import load_dotenv\n",
    "load_dotenv()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [],
   "source": [
    "# get dataset\n",
    "practice_dataset_uri = \"weave:///parambharat/hackercup/object/practice_dataset:R35fXf9N3FE2IOesg7bRPaPAxiE9YbpirhXO9HcHs8w\"\n",
    "problems_dataset = weave.ref(practice_dataset_uri).get().rows[:]\n",
    "problems = list(map(lambda x: Problem(**x), problems_dataset))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pydantic import BaseModel, Field\n",
    "\n",
    "class Solution(BaseModel):\n",
    "    core_question: str = Field(..., description=\"Core question of the problem\")\n",
    "    problem_solving_info: str = Field(..., description=\"Problem-solving information related to the core question\")\n",
    "    plan: str = Field(..., description=\"Step by step plan to solve the problem\")\n",
    "    pseudocode: str = Field(..., description=\"Pseudocode to solve the problem\")\n",
    "    source_code: str = Field(..., description=\"Valid Python3 sourcecode to solve the problem.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [],
   "source": [
    "system_prompt = \"\"\"\n",
    "You are a world-class competitive programmer tasked with solving a programming problem. \n",
    "You will be provided with a problem statement, and you need to create a Python3 solution for it. \n",
    "Your task it to develop a winning solution to the problem in Python3 programming language.\n",
    "You will do this in a step-by-step manner.\n",
    "\n",
    "Step 1: Extract the core question and the problem-solving information from the problem statement.\n",
    "Step 2: Generate a step by step plan to solve the problem.\n",
    "Step 3: Generate the pseudocode to solve the problem.\n",
    "Step 4: Write the final solution in Python3 programming language to solve the problem.\n",
    "\n",
    "Competition Guidelines:\n",
    "    a. Do not use any external libraries; stick to Python 3 standard library\n",
    "    b. Handle input and output using standard input/output (stdin/stdout)\n",
    "    c. Use helper functions to improve readability of the code.\n",
    "    c. Use the `input()` function to take input from stdin and print the output to stdout.\n",
    "    d. Do not add extra print statements otherwise it will fail the test cases.\n",
    "    e. Make sure your code passes all potential test cases, including edge cases\n",
    "    f. Follow the input/output format specified in the problem statement and the sample test cases.\"\"\"\n",
    "\n",
    "prompt_template = \"\"\"\n",
    "Let's think step by step to solve the problem:\n",
    "\n",
    "Problem: \n",
    "{problem_description}\n",
    "\n",
    "Input: \n",
    "{sample_input}\n",
    "\n",
    "Output: \n",
    "{sample_output}\n",
    "\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [],
   "source": [
    "import openai"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "@weave.op\n",
    "async def one_shot_solver(\n",
    "    problem: Problem, \n",
    "    llm_model: str,\n",
    "    system_prompt: str, \n",
    "    prompt_template: str,\n",
    "    # temperature: float = 0.7,\n",
    "    timeout: int = 10\n",
    ") -> str:\n",
    "    logging.info(f\"Solving problem: {problem.problem_name}\")\n",
    "\n",
    "    # call model one first time to get the code\n",
    "    logging.info(\"Calling model to solve the problem\")\n",
    "    if \"o1\" in llm_model:\n",
    "        model_output = await oai_client.chat.completions.create(\n",
    "            model=llm_model,\n",
    "            messages=[\n",
    "                {\"role\": \"user\", \n",
    "                \"content\": [\n",
    "                    {\"type\": \"text\", \"text\": prompt_template.format(\n",
    "                    problem_description=problem.problem_description,\n",
    "                    sample_input=problem.sample_input,\n",
    "                    sample_output=problem.sample_output)}\n",
    "                    ]\n",
    "                }\n",
    "            ],\n",
    "            # temperature=temperature,\n",
    "        )\n",
    "    else:\n",
    "        model_output = await oai_client.chat.completions.create(\n",
    "            model=llm_model,\n",
    "            messages=[\n",
    "                {\"role\": \"system\", \n",
    "                \"content\": [\n",
    "                    {\"type\": \"text\", \"text\": system_prompt}\n",
    "                ]\n",
    "                },\n",
    "                {\"role\": \"user\", \n",
    "                \"content\": [\n",
    "                    {\"type\": \"text\", \"text\": prompt_template.format(\n",
    "                    problem_description=problem.problem_description,\n",
    "                    sample_input=problem.sample_input,\n",
    "                    sample_output=problem.sample_output)}\n",
    "                    ]\n",
    "                }\n",
    "            ],\n",
    "            # temperature=temperature,\n",
    "        )\n",
    "\n",
    "    out = model_output.choices[0].message.content\n",
    "\n",
    "    # extract code from the response\n",
    "    logging.info(\"Formatting the response\")\n",
    "    solution = await format_response(out, Solution)\n",
    "\n",
    "    # check if the code is correct\n",
    "    logging.info(\"Checking if the code is correct\")\n",
    "    test_report = await check_correctness(\n",
    "        solution.source_code,\n",
    "        problem.sample_input,\n",
    "        problem.sample_output,\n",
    "        timeout=timeout,\n",
    "    )\n",
    "\n",
    "    return {\"solution\": solution, \"test_report\": test_report}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Evaluation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [],
   "source": [
    "# STRONG_LLM = \"o1-preview\"\n",
    "# STRONG_LLM = \"gpt-4o-2024-08-06\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [],
   "source": [
    "class OneShotSolver(weave.Model):\n",
    "    code_execution_timeout: int = 30\n",
    "    llm_model: str = \"\"\n",
    "    system_prompt: str = system_prompt\n",
    "    prompt_template: str = prompt_template\n",
    "    # temperature: float = None\n",
    "\n",
    "    @weave.op\n",
    "    async def predict(self, problem: dict):\n",
    "        return await one_shot_solver(\n",
    "            problem=Problem(**problem), \n",
    "            llm_model=self.llm_model,\n",
    "            system_prompt=self.system_prompt, \n",
    "            prompt_template=self.prompt_template, \n",
    "            timeout=self.code_execution_timeout,\n",
    "            # temperature=self.temperature\n",
    "        )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [],
   "source": [
    "evals_dataset = [{\"problem\": problem.model_dump(), \"expected_result\": \"passed\"} for problem in problems]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [],
   "source": [
    "@weave.op\n",
    "def scorer(expected_result: str, model_output: dict) -> dict:\n",
    "    if model_output is None or model_output[\"test_report\"].status is None:\n",
    "        return {\"solution_passed\": False}\n",
    "    return {\"solution_passed\": expected_result == model_output[\"test_report\"].status} # check if the test_report status == passed"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = OneShotSolver(llm_model=\"o1-preview\", system_prompt=\"\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\">Evaluated <span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">1</span> of <span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">25</span> examples\n",
       "</pre>\n"
      ],
      "text/plain": [
       "Evaluated \u001b[1;36m1\u001b[0m of \u001b[1;36m25\u001b[0m examples\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\">Evaluated <span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">2</span> of <span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">25</span> examples\n",
       "</pre>\n"
      ],
      "text/plain": [
       "Evaluated \u001b[1;36m2\u001b[0m of \u001b[1;36m25\u001b[0m examples\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\">Evaluated <span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">3</span> of <span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">25</span> examples\n",
       "</pre>\n"
      ],
      "text/plain": [
       "Evaluated \u001b[1;36m3\u001b[0m of \u001b[1;36m25\u001b[0m examples\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\">Evaluated <span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">4</span> of <span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">25</span> examples\n",
       "</pre>\n"
      ],
      "text/plain": [
       "Evaluated \u001b[1;36m4\u001b[0m of \u001b[1;36m25\u001b[0m examples\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\">Evaluated <span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">5</span> of <span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">25</span> examples\n",
       "</pre>\n"
      ],
      "text/plain": [
       "Evaluated \u001b[1;36m5\u001b[0m of \u001b[1;36m25\u001b[0m examples\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "evaluator = weave.Evaluation(dataset=evals_dataset, scorers=[scorer], trials=5)\n",
    "\n",
    "results = await evaluator.evaluate(model)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "accelerator": "GPU",
  "colab": {
   "include_colab_link": true,
   "provenance": [],
   "toc_visible": true
  },
  "kernelspec": {
   "display_name": "Python 3",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
